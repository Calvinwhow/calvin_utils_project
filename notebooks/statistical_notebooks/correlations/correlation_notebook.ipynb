{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Correlation-Based Analyses\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: July 6, 2023\n",
    "\n",
    "Use this to assess if a correlation between a dependent variable and an independent variable is statistically significant using permutation analysis. \n",
    "\n",
    "Further, follow this up with a contrast analysis which sees which categorical variables have significantly different correlations from each other. \n",
    "\n",
    "Notes:\n",
    "- To best use this notebook, you should be familar with mixed effects models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/cognition_2023/metadata/master_list_proper_subjects.xlsx'\n",
    "sheet= 'master_list_proper_subjects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/3_cohort_delta_r/andy_similarity_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T_Redone</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>...</th>\n",
       "      <th>DECLINE</th>\n",
       "      <th>Cognitive_Improve</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>21.150595</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>19.702349</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>23.231614</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>22.172312</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>21.546222</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>211</td>\n",
       "      <td>58.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.415745</td>\n",
       "      <td>-0.189000</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>152</td>\n",
       "      <td>69.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.701419</td>\n",
       "      <td>-0.455000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>208</td>\n",
       "      <td>79.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.929958</td>\n",
       "      <td>-0.669000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>223</td>\n",
       "      <td>71.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.829972</td>\n",
       "      <td>-0.575000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>214</td>\n",
       "      <td>76.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.244199</td>\n",
       "      <td>-0.958000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0        101  62.0                                 -0.392857   \n",
       "1        102  77.0                                 -0.666667   \n",
       "2        103  76.0                                 -1.447368   \n",
       "3        104  65.0                                 -2.372549   \n",
       "4        105  50.0                                 -0.192982   \n",
       "..       ...   ...                                       ...   \n",
       "194      211  58.7                                       NaN   \n",
       "195      152  69.4                                       NaN   \n",
       "196      208  79.2                                       NaN   \n",
       "197      223  71.1                                       NaN   \n",
       "198      214  76.6                                       NaN   \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                             0.314066        \n",
       "1                                             0.013999        \n",
       "2                                            -0.841572        \n",
       "3                                            -1.855477        \n",
       "4                                             0.533109        \n",
       "..                                                 ...        \n",
       "194                                                NaN        \n",
       "195                                                NaN        \n",
       "196                                                NaN        \n",
       "197                                                NaN        \n",
       "198                                                NaN        \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                  0.314066                     -21.428571   \n",
       "1                                  0.013999                     -36.363636   \n",
       "2                                 -0.841572                     -78.947368   \n",
       "3                                 -1.855477                    -129.411765   \n",
       "4                                  0.533109                     -10.526316   \n",
       "..                                      ...                            ...   \n",
       "194                                     NaN                            NaN   \n",
       "195                                     NaN                            NaN   \n",
       "196                                     NaN                            NaN   \n",
       "197                                     NaN                            NaN   \n",
       "198                                     NaN                            NaN   \n",
       "\n",
       "     Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                                -1.282630                          -1.282630   \n",
       "1                                -1.760917                          -1.760917   \n",
       "2                                -0.595369                          -0.595369   \n",
       "3                                -0.945206                          -0.945206   \n",
       "4                                -1.151973                          -1.151973   \n",
       "..                                     ...                                ...   \n",
       "194                              -0.415745                          -0.189000   \n",
       "195                              -0.701419                          -0.455000   \n",
       "196                              -0.929958                          -0.669000   \n",
       "197                              -0.829972                          -0.575000   \n",
       "198                              -1.244199                          -0.958000   \n",
       "\n",
       "     Subiculum_Connectivity_T_Redone  Subiculum_Connectivity_T  ...  DECLINE  \\\n",
       "0                          21.150595                 56.864683  ...      1.0   \n",
       "1                          19.702349                 52.970984  ...      1.0   \n",
       "2                          23.231614                 62.459631  ...      1.0   \n",
       "3                          22.172312                 59.611631  ...      1.0   \n",
       "4                          21.546222                 57.928350  ...      0.0   \n",
       "..                               ...                       ...  ...      ...   \n",
       "194                        19.900000                 19.900000  ...      NaN   \n",
       "195                        17.900000                 17.900000  ...      NaN   \n",
       "196                        16.300000                 16.300000  ...      NaN   \n",
       "197                        17.000000                 17.000000  ...      NaN   \n",
       "198                        14.100000                 14.100000  ...      NaN   \n",
       "\n",
       "     Cognitive_Improve  Z_Scored_Cognitive_Baseline  \\\n",
       "0                   No                     1.518764   \n",
       "1                   No                     0.465551   \n",
       "2                   No                    -0.061056   \n",
       "3                   No                    -0.412127   \n",
       "4                   No                    -0.061056   \n",
       "..                 ...                          ...   \n",
       "194                Yes                          NaN   \n",
       "195                Yes                          NaN   \n",
       "196                Yes                          NaN   \n",
       "197                Yes                          NaN   \n",
       "198                Yes                          NaN   \n",
       "\n",
       "     Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                        -1.518764   \n",
       "1                                        -0.465551   \n",
       "2                                         0.061056   \n",
       "3                                         0.412127   \n",
       "4                                         0.061056   \n",
       "..                                             ...   \n",
       "194                                            NaN   \n",
       "195                                            NaN   \n",
       "196                                            NaN   \n",
       "197                                            NaN   \n",
       "198                                            NaN   \n",
       "\n",
       "     Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                           0.72                                 0.28   \n",
       "1                           0.48                                 0.52   \n",
       "2                           0.36                                 0.64   \n",
       "3                           0.28                                 0.72   \n",
       "4                           0.36                                 0.64   \n",
       "..                           ...                                  ...   \n",
       "194                          NaN                                  NaN   \n",
       "195                          NaN                                  NaN   \n",
       "196                          NaN                                  NaN   \n",
       "197                          NaN                                  NaN   \n",
       "198                          NaN                                  NaN   \n",
       "\n",
       "     ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0         12.222658      14.493929            -1.714513            -1.227368  \n",
       "1         14.020048      15.257338            -1.155843            -1.022243  \n",
       "2         15.118727      17.376384            -0.814348            -0.452865  \n",
       "3         13.112424      15.287916            -1.437954            -1.014027  \n",
       "4         15.086568      12.951426            -0.824344            -1.641831  \n",
       "..              ...            ...                  ...                  ...  \n",
       "194             NaN            NaN                  NaN                  NaN  \n",
       "195             NaN            NaN                  NaN                  NaN  \n",
       "196             NaN            NaN                  NaN                  NaN  \n",
       "197             NaN            NaN                  NaN                  NaN  \n",
       "198             NaN            NaN                  NaN                  NaN  \n",
       "\n",
       "[199 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'Age', 'Normalized_Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group',\n",
       "       'Z_Scored_Percent_Cognitive_Improvement',\n",
       "       'Percent_Cognitive_Improvement',\n",
       "       'Z_Scored_Subiculum_T_By_Origin_Group_',\n",
       "       'Z_Scored_Subiculum_Connectivity_T', 'Subiculum_Connectivity_T_Redone',\n",
       "       'Subiculum_Connectivity_T', 'Amnesia_Lesion_T_Map', 'Memory_Network_T',\n",
       "       'Z_Scored_Memory_Network_R', 'Memory_Network_R',\n",
       "       'Subiculum_Grey_Matter', 'Subiculum_White_Matter', 'Subiculum_CSF',\n",
       "       'Subiculum_Total', 'Standardized_Age',\n",
       "       'Standardized_Percent_Improvement',\n",
       "       'Standardized_Subiculum_Connectivity',\n",
       "       'Standardized_Subiculum_Grey_Matter',\n",
       "       'Standardized_Subiculum_White_Matter', 'Standardized_Subiculum_CSF',\n",
       "       'Standardized_Subiculum_Total', 'Disease', 'Cohort', 'City',\n",
       "       'Inclusion_Cohort', 'Categorical_Age_Group', 'Age_Group',\n",
       "       'Age_And_Disease', 'Age_Disease_and_Cohort', 'Age_Disease_Cohort_Stim',\n",
       "       'Age_And_Stim', 'Subiculum_Group_By_Z_Score_Sign',\n",
       "       'Subiculum_Group_By_Inflection_Point', 'Subiculum_Group_By_24',\n",
       "       'Cognitive_Outcome', 'StimMatch', 'StimMatch24', 'Cognitive_Baseline',\n",
       "       'Cognitive_Score_1_Yr', 'MinMaxNorm_Cog_Score_1_Yr',\n",
       "       'Abs_Cognitive_Improve', 'IMPROVE_OR_STABLE', 'DECLINE_OR_STABLE',\n",
       "       'IMPROVE', 'DECLINE', 'Cognitive_Improve',\n",
       "       'Z_Scored_Cognitive_Baseline',\n",
       "       'Z_Scored_Cognitive_Baseline__Lower_is_Better_',\n",
       "       'Min_Max_Normalized_Baseline', 'MinMaxNormBaseline_Higher_is_Better',\n",
       "       'ROI_to_Alz_Max', 'ROI_to_PD_Max', 'Standardzied_AD_Max',\n",
       "       'Standardized_PD_Max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Z_Scored_Percent_Cognitive_Improvement', 'Age_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T_Redone</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>...</th>\n",
       "      <th>DECLINE</th>\n",
       "      <th>Cognitive_Improve</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>21.150595</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>19.702349</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>23.231614</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>22.172312</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>21.546222</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.64</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>86</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087220</td>\n",
       "      <td>-0.621000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598397</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>88</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.854050</td>\n",
       "      <td>2.637141</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>0.269872</td>\n",
       "      <td>-0.207000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>89</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598787</td>\n",
       "      <td>-0.099428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.158639</td>\n",
       "      <td>-0.694000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>91</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.818759</td>\n",
       "      <td>0.535847</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.469844</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0        101  62.0                                 -0.392857   \n",
       "1        102  77.0                                 -0.666667   \n",
       "2        103  76.0                                 -1.447368   \n",
       "3        104  65.0                                 -2.372549   \n",
       "4        105  50.0                                 -0.192982   \n",
       "..       ...   ...                                       ...   \n",
       "160       86  57.0                                       NaN   \n",
       "161       87  65.0                                       NaN   \n",
       "162       88  65.0                                       NaN   \n",
       "163       89  67.0                                       NaN   \n",
       "164       91  45.0                                       NaN   \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                             0.314066        \n",
       "1                                             0.013999        \n",
       "2                                            -0.841572        \n",
       "3                                            -1.855477        \n",
       "4                                             0.533109        \n",
       "..                                                 ...        \n",
       "160                                           0.598787        \n",
       "161                                           0.598787        \n",
       "162                                           5.854050        \n",
       "163                                           0.598787        \n",
       "164                                           1.818759        \n",
       "\n",
       "     Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                  0.314066                     -21.428571   \n",
       "1                                  0.013999                     -36.363636   \n",
       "2                                 -0.841572                     -78.947368   \n",
       "3                                 -1.855477                    -129.411765   \n",
       "4                                  0.533109                     -10.526316   \n",
       "..                                      ...                            ...   \n",
       "160                               -0.099428                       0.000000   \n",
       "161                               -0.099428                       0.000000   \n",
       "162                                2.637141                      15.384615   \n",
       "163                               -0.099428                       0.000000   \n",
       "164                                0.535847                       3.571429   \n",
       "\n",
       "     Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                                -1.282630                          -1.282630   \n",
       "1                                -1.760917                          -1.760917   \n",
       "2                                -0.595369                          -0.595369   \n",
       "3                                -0.945206                          -0.945206   \n",
       "4                                -1.151973                          -1.151973   \n",
       "..                                     ...                                ...   \n",
       "160                              -0.087220                          -0.621000   \n",
       "161                               0.598397                           0.173000   \n",
       "162                               0.269872                          -0.207000   \n",
       "163                              -0.158639                          -0.694000   \n",
       "164                               0.469844                           0.019400   \n",
       "\n",
       "     Subiculum_Connectivity_T_Redone  Subiculum_Connectivity_T  ...  DECLINE  \\\n",
       "0                          21.150595                 56.864683  ...      1.0   \n",
       "1                          19.702349                 52.970984  ...      1.0   \n",
       "2                          23.231614                 62.459631  ...      1.0   \n",
       "3                          22.172312                 59.611631  ...      1.0   \n",
       "4                          21.546222                 57.928350  ...      0.0   \n",
       "..                               ...                       ...  ...      ...   \n",
       "160                        22.200000                 22.200000  ...      NaN   \n",
       "161                        27.000000                 27.000000  ...      NaN   \n",
       "162                        24.700000                 24.700000  ...      NaN   \n",
       "163                        21.700000                 21.700000  ...      NaN   \n",
       "164                        26.100000                 26.100000  ...      NaN   \n",
       "\n",
       "     Cognitive_Improve  Z_Scored_Cognitive_Baseline  \\\n",
       "0                   No                     1.518764   \n",
       "1                   No                     0.465551   \n",
       "2                   No                    -0.061056   \n",
       "3                   No                    -0.412127   \n",
       "4                   No                    -0.061056   \n",
       "..                 ...                          ...   \n",
       "160                Yes                          NaN   \n",
       "161                Yes                          NaN   \n",
       "162                Yes                          NaN   \n",
       "163                Yes                          NaN   \n",
       "164                Yes                          NaN   \n",
       "\n",
       "     Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                        -1.518764   \n",
       "1                                        -0.465551   \n",
       "2                                         0.061056   \n",
       "3                                         0.412127   \n",
       "4                                         0.061056   \n",
       "..                                             ...   \n",
       "160                                            NaN   \n",
       "161                                            NaN   \n",
       "162                                            NaN   \n",
       "163                                            NaN   \n",
       "164                                            NaN   \n",
       "\n",
       "     Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                           0.72                                 0.28   \n",
       "1                           0.48                                 0.52   \n",
       "2                           0.36                                 0.64   \n",
       "3                           0.28                                 0.72   \n",
       "4                           0.36                                 0.64   \n",
       "..                           ...                                  ...   \n",
       "160                          NaN                                  NaN   \n",
       "161                          NaN                                  NaN   \n",
       "162                          NaN                                  NaN   \n",
       "163                          NaN                                  NaN   \n",
       "164                          NaN                                  NaN   \n",
       "\n",
       "     ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0         12.222658      14.493929            -1.714513            -1.227368  \n",
       "1         14.020048      15.257338            -1.155843            -1.022243  \n",
       "2         15.118727      17.376384            -0.814348            -0.452865  \n",
       "3         13.112424      15.287916            -1.437954            -1.014027  \n",
       "4         15.086568      12.951426            -0.824344            -1.641831  \n",
       "..              ...            ...                  ...                  ...  \n",
       "160             NaN            NaN                  NaN                  NaN  \n",
       "161             NaN            NaN                  NaN                  NaN  \n",
       "162             NaN            NaN                  NaN                  NaN  \n",
       "163             NaN            NaN                  NaN                  NaN  \n",
       "164             NaN            NaN                  NaN                  NaN  \n",
       "\n",
       "[148 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'City'  # The column you'd like to evaluate\n",
    "condition = 'equal'  # Thecondition to check ('equal', 'above', 'below', 'not')\n",
    "value = 'Queensland' # The value to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Age</th>\n",
       "      <th>Normalized_Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group</th>\n",
       "      <th>Z_Scored_Percent_Cognitive_Improvement</th>\n",
       "      <th>Percent_Cognitive_Improvement</th>\n",
       "      <th>Z_Scored_Subiculum_T_By_Origin_Group_</th>\n",
       "      <th>Z_Scored_Subiculum_Connectivity_T</th>\n",
       "      <th>Subiculum_Connectivity_T_Redone</th>\n",
       "      <th>Subiculum_Connectivity_T</th>\n",
       "      <th>...</th>\n",
       "      <th>DECLINE</th>\n",
       "      <th>Cognitive_Improve</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline</th>\n",
       "      <th>Z_Scored_Cognitive_Baseline__Lower_is_Better_</th>\n",
       "      <th>Min_Max_Normalized_Baseline</th>\n",
       "      <th>MinMaxNormBaseline_Higher_is_Better</th>\n",
       "      <th>ROI_to_Alz_Max</th>\n",
       "      <th>ROI_to_PD_Max</th>\n",
       "      <th>Standardzied_AD_Max</th>\n",
       "      <th>Standardized_PD_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>0.314066</td>\n",
       "      <td>-21.428571</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>-1.282630</td>\n",
       "      <td>21.150595</td>\n",
       "      <td>56.864683</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1.518764</td>\n",
       "      <td>-1.518764</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.280</td>\n",
       "      <td>12.222658</td>\n",
       "      <td>14.493929</td>\n",
       "      <td>-1.714513</td>\n",
       "      <td>-1.227368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-36.363636</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>-1.760917</td>\n",
       "      <td>19.702349</td>\n",
       "      <td>52.970984</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.465551</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.520</td>\n",
       "      <td>14.020048</td>\n",
       "      <td>15.257338</td>\n",
       "      <td>-1.155843</td>\n",
       "      <td>-1.022243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-1.447368</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-0.841572</td>\n",
       "      <td>-78.947368</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>-0.595369</td>\n",
       "      <td>23.231614</td>\n",
       "      <td>62.459631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.640</td>\n",
       "      <td>15.118727</td>\n",
       "      <td>17.376384</td>\n",
       "      <td>-0.814348</td>\n",
       "      <td>-0.452865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-2.372549</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-1.855477</td>\n",
       "      <td>-129.411765</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>-0.945206</td>\n",
       "      <td>22.172312</td>\n",
       "      <td>59.611631</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.720</td>\n",
       "      <td>13.112424</td>\n",
       "      <td>15.287916</td>\n",
       "      <td>-1.437954</td>\n",
       "      <td>-1.014027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.192982</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>0.533109</td>\n",
       "      <td>-10.526316</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>-1.151973</td>\n",
       "      <td>21.546222</td>\n",
       "      <td>57.928350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.061056</td>\n",
       "      <td>0.061056</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.640</td>\n",
       "      <td>15.086568</td>\n",
       "      <td>12.951426</td>\n",
       "      <td>-0.824344</td>\n",
       "      <td>-1.641831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>30</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-0.638889</td>\n",
       "      <td>0.106772</td>\n",
       "      <td>0.106772</td>\n",
       "      <td>-1.388889</td>\n",
       "      <td>-0.590767</td>\n",
       "      <td>-0.590767</td>\n",
       "      <td>18.674670</td>\n",
       "      <td>18.674670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1.114522</td>\n",
       "      <td>1.114522</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>13.873339</td>\n",
       "      <td>15.685024</td>\n",
       "      <td>-0.347959</td>\n",
       "      <td>-0.150172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>31</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.086636</td>\n",
       "      <td>1.086636</td>\n",
       "      <td>1.449275</td>\n",
       "      <td>-1.065220</td>\n",
       "      <td>-1.065220</td>\n",
       "      <td>15.353030</td>\n",
       "      <td>15.353030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-1.345113</td>\n",
       "      <td>-1.345113</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>10.930371</td>\n",
       "      <td>11.599140</td>\n",
       "      <td>-0.855211</td>\n",
       "      <td>-0.712879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>33</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-0.643357</td>\n",
       "      <td>0.103418</td>\n",
       "      <td>0.103418</td>\n",
       "      <td>-1.398601</td>\n",
       "      <td>-1.108473</td>\n",
       "      <td>-1.108473</td>\n",
       "      <td>15.050219</td>\n",
       "      <td>15.050219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.704583</td>\n",
       "      <td>0.704583</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>6.780175</td>\n",
       "      <td>8.774895</td>\n",
       "      <td>-1.570542</td>\n",
       "      <td>-1.101833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>36</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-1.286713</td>\n",
       "      <td>-0.379443</td>\n",
       "      <td>-0.379443</td>\n",
       "      <td>-2.797203</td>\n",
       "      <td>-0.775406</td>\n",
       "      <td>-0.775406</td>\n",
       "      <td>17.382020</td>\n",
       "      <td>17.382020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.704583</td>\n",
       "      <td>0.704583</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>9.841726</td>\n",
       "      <td>16.308768</td>\n",
       "      <td>-1.042851</td>\n",
       "      <td>-0.064270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>37</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.992806</td>\n",
       "      <td>-0.158855</td>\n",
       "      <td>-0.158855</td>\n",
       "      <td>-2.158273</td>\n",
       "      <td>-0.690244</td>\n",
       "      <td>-0.690244</td>\n",
       "      <td>17.978233</td>\n",
       "      <td>17.978233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.935174</td>\n",
       "      <td>-0.935174</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>11.101963</td>\n",
       "      <td>10.543942</td>\n",
       "      <td>-0.825635</td>\n",
       "      <td>-0.858200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject   Age  Normalized_Percent_Cognitive_Improvement  \\\n",
       "0       101  62.0                                 -0.392857   \n",
       "1       102  77.0                                 -0.666667   \n",
       "2       103  76.0                                 -1.447368   \n",
       "3       104  65.0                                 -2.372549   \n",
       "4       105  50.0                                 -0.192982   \n",
       "..      ...   ...                                       ...   \n",
       "68       30  58.0                                 -0.638889   \n",
       "69       31  64.0                                  0.666667   \n",
       "70       33  60.0                                 -0.643357   \n",
       "72       36  52.0                                 -1.286713   \n",
       "73       37  64.0                                 -0.992806   \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group  \\\n",
       "0                                            0.314066        \n",
       "1                                            0.013999        \n",
       "2                                           -0.841572        \n",
       "3                                           -1.855477        \n",
       "4                                            0.533109        \n",
       "..                                                ...        \n",
       "68                                           0.106772        \n",
       "69                                           1.086636        \n",
       "70                                           0.103418        \n",
       "72                                          -0.379443        \n",
       "73                                          -0.158855        \n",
       "\n",
       "    Z_Scored_Percent_Cognitive_Improvement  Percent_Cognitive_Improvement  \\\n",
       "0                                 0.314066                     -21.428571   \n",
       "1                                 0.013999                     -36.363636   \n",
       "2                                -0.841572                     -78.947368   \n",
       "3                                -1.855477                    -129.411765   \n",
       "4                                 0.533109                     -10.526316   \n",
       "..                                     ...                            ...   \n",
       "68                                0.106772                      -1.388889   \n",
       "69                                1.086636                       1.449275   \n",
       "70                                0.103418                      -1.398601   \n",
       "72                               -0.379443                      -2.797203   \n",
       "73                               -0.158855                      -2.158273   \n",
       "\n",
       "    Z_Scored_Subiculum_T_By_Origin_Group_  Z_Scored_Subiculum_Connectivity_T  \\\n",
       "0                               -1.282630                          -1.282630   \n",
       "1                               -1.760917                          -1.760917   \n",
       "2                               -0.595369                          -0.595369   \n",
       "3                               -0.945206                          -0.945206   \n",
       "4                               -1.151973                          -1.151973   \n",
       "..                                    ...                                ...   \n",
       "68                              -0.590767                          -0.590767   \n",
       "69                              -1.065220                          -1.065220   \n",
       "70                              -1.108473                          -1.108473   \n",
       "72                              -0.775406                          -0.775406   \n",
       "73                              -0.690244                          -0.690244   \n",
       "\n",
       "    Subiculum_Connectivity_T_Redone  Subiculum_Connectivity_T  ...  DECLINE  \\\n",
       "0                         21.150595                 56.864683  ...      1.0   \n",
       "1                         19.702349                 52.970984  ...      1.0   \n",
       "2                         23.231614                 62.459631  ...      1.0   \n",
       "3                         22.172312                 59.611631  ...      1.0   \n",
       "4                         21.546222                 57.928350  ...      0.0   \n",
       "..                              ...                       ...  ...      ...   \n",
       "68                        18.674670                 18.674670  ...      0.0   \n",
       "69                        15.353030                 15.353030  ...      0.0   \n",
       "70                        15.050219                 15.050219  ...      0.0   \n",
       "72                        17.382020                 17.382020  ...      0.0   \n",
       "73                        17.978233                 17.978233  ...      0.0   \n",
       "\n",
       "    Cognitive_Improve  Z_Scored_Cognitive_Baseline  \\\n",
       "0                  No                     1.518764   \n",
       "1                  No                     0.465551   \n",
       "2                  No                    -0.061056   \n",
       "3                  No                    -0.412127   \n",
       "4                  No                    -0.061056   \n",
       "..                ...                          ...   \n",
       "68                 No                     1.114522   \n",
       "69                Yes                    -1.345113   \n",
       "70                 No                     0.704583   \n",
       "72                 No                     0.704583   \n",
       "73                 No                    -0.935174   \n",
       "\n",
       "    Z_Scored_Cognitive_Baseline__Lower_is_Better_  \\\n",
       "0                                       -1.518764   \n",
       "1                                       -0.465551   \n",
       "2                                        0.061056   \n",
       "3                                        0.412127   \n",
       "4                                        0.061056   \n",
       "..                                            ...   \n",
       "68                                       1.114522   \n",
       "69                                      -1.345113   \n",
       "70                                       0.704583   \n",
       "72                                       0.704583   \n",
       "73                                      -0.935174   \n",
       "\n",
       "    Min_Max_Normalized_Baseline  MinMaxNormBaseline_Higher_is_Better  \\\n",
       "0                         0.720                                0.280   \n",
       "1                         0.480                                0.520   \n",
       "2                         0.360                                0.640   \n",
       "3                         0.280                                0.720   \n",
       "4                         0.360                                0.640   \n",
       "..                          ...                                  ...   \n",
       "68                        1.000                                1.000   \n",
       "69                        0.250                                0.250   \n",
       "70                        0.875                                0.875   \n",
       "72                        0.875                                0.875   \n",
       "73                        0.375                                0.375   \n",
       "\n",
       "    ROI_to_Alz_Max  ROI_to_PD_Max  Standardzied_AD_Max  Standardized_PD_Max  \n",
       "0        12.222658      14.493929            -1.714513            -1.227368  \n",
       "1        14.020048      15.257338            -1.155843            -1.022243  \n",
       "2        15.118727      17.376384            -0.814348            -0.452865  \n",
       "3        13.112424      15.287916            -1.437954            -1.014027  \n",
       "4        15.086568      12.951426            -0.824344            -1.641831  \n",
       "..             ...            ...                  ...                  ...  \n",
       "68       13.873339      15.685024            -0.347959            -0.150172  \n",
       "69       10.930371      11.599140            -0.855211            -0.712879  \n",
       "70        6.780175       8.774895            -1.570542            -1.101833  \n",
       "72        9.841726      16.308768            -1.042851            -0.064270  \n",
       "73       11.101963      10.543942            -0.825635            -0.858200  \n",
       "\n",
       "[72 rows x 58 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regress out a Covariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = []\n",
    "for col in data_df.columns:\n",
    "    if 'surface' in col.lower():\n",
    "        lis.append(col)\n",
    "print(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.regression_utils import RegressOutCovariates\n",
    "# use this code block to regress out covariates. Generally better to just include as covariates in a model..\n",
    "dependent_variable_list = lis\n",
    "regressors = ['Age', 'Sex']\n",
    "\n",
    "data_df, adjusted_dep_vars_list = RegressOutCovariates.run(df=data_df, dependent_variable_list=dependent_variable_list, covariates_list=regressors)\n",
    "print(adjusted_dep_vars_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Age',  'Subiculum_Connectivity_T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Perform Basic Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables to Correlate\n",
    "- dependent_variable = 'Z_Scored_Percent_Cognitive_Improvement'\n",
    "- independent_variable_list = ['Z_Scored_Cognitive_Baseline']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns = [col for col in data_df.columns if 'occipital' in col.lower()]\n",
    "print(filtered_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_variables_list = ['Z_Scored_Percent_Cognitive_Improvement']\n",
    "x_variable_list = ['Subiculum_Connectivity_T_Redone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter a column for categories (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Correlation Method\n",
    "- Options: 'spearman', 'pearson', 'kendall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = 'pearson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define plot Labels\n",
    "- These are the axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axis Labels\n",
    "x_label = 'Brain Region'\n",
    "y_label = 'Correlation to Clinician'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.scatterplot import ScatterplotGenerator \n",
    "for y_var in y_variables_list:\n",
    "    generator = ScatterplotGenerator(dataframe=data_df, data_dict={y_var: x_variable_list}, \n",
    "                                    x_label=x_label, y_label=y_label, correlation=correlation, \n",
    "                                    palette='tab10',\n",
    "                                    rows_per_fig=1, cols_per_fig=2,\n",
    "                                    ylim=None,\n",
    "                                    out_dir=out_dir,\n",
    "                                    category_col=cat_col)\n",
    "    generator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Run ANCOVA-Style Analysis Using Correlation\n",
    "- AKA a 'Delta-R Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ANCOVACorrelation:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        similarity: Bool\n",
    "            - This determins if the delta-r analysis will be performed to assess if the correlations are MORE SIMILAR THAN EXPECTED.\n",
    "            - Defaults to False, which evalutes if they are MORE DIFFERENCE THAN EXPECTED. \n",
    "        two_tail: Bool\n",
    "            - This determines if a two-tailed p-value derivation will be performed. \n",
    "            - Set to false to generate a one-tail. \n",
    "    \"\"\"\n",
    "    def __init__(self, df, dependent_variable, independent_variable, categorical_variable, spearman=False, out_dir=None, n_permutations=10000, similarity=False, two_tail=True):\n",
    "        self.df = df\n",
    "        self.dependent_variable = dependent_variable\n",
    "        self.independent_variable = independent_variable\n",
    "        self.categorical_variable = categorical_variable\n",
    "        self.out_dir = out_dir\n",
    "        self.similarity = similarity\n",
    "        self.two_tail = two_tail\n",
    "        # Initialize a dictionary to store dataframes for each category\n",
    "        self.category_dataframes = {}\n",
    "        \n",
    "        # Initialize a dictionary to store observed correlations\n",
    "        self.observed_correlations = {}\n",
    "        \n",
    "        # Call the segregator function to split the data\n",
    "        self.segregate_data()\n",
    "        self.spearman=spearman\n",
    "        \n",
    "        self.n_permutations=n_permutations\n",
    "    \n",
    "    def segregate_data(self):\n",
    "        # Group the data by unique values of the categorical variable\n",
    "        unique_categories = self.df[self.categorical_variable].unique()\n",
    "        \n",
    "        # Create separate dataframes for each category\n",
    "        for category in unique_categories:\n",
    "            self.category_dataframes[category] = self.df[self.df[self.categorical_variable] == category]\n",
    "    \n",
    "    def calculate_correlation(self, category):\n",
    "        # Calculate the correlation between independent and dependent variables for a given category\n",
    "        category_df = self.category_dataframes.get(category)\n",
    "        if category_df is not None:\n",
    "            if self.spearman:\n",
    "                correlation = category_df[self.independent_variable].corr(category_df[self.dependent_variable], method='spearman')\n",
    "            else:\n",
    "                correlation = category_df[self.independent_variable].corr(category_df[self.dependent_variable])\n",
    "            self.observed_correlations[category] = correlation\n",
    "        else:\n",
    "            print(f\"Category '{category}' not found in the data.\")\n",
    "\n",
    "            \n",
    "    def calculate_observed_r_values(self):\n",
    "        # Calculate observed r values for each category and store them\n",
    "        self.observed_correlations = {}\n",
    "        for category in self.category_dataframes.keys():\n",
    "            correlation = self.calculate_correlation(category)\n",
    "            if correlation is not None:\n",
    "                self.observed_correlations[category] = correlation\n",
    "\n",
    "    def permute_and_calculate_correlations(self):\n",
    "        # Initialize a dictionary to store permuted correlations for each category\n",
    "        self.permuted_correlations = {category: [] for category in self.category_dataframes.keys()}\n",
    "\n",
    "        for _ in tqdm(range(self.n_permutations), desc=\"Permutations\"):\n",
    "            # Create a copy of the original data to permute\n",
    "            permuted_data = self.df.copy()\n",
    "\n",
    "            # Loop through each category's dataframe\n",
    "            for category, category_df in self.category_dataframes.items():\n",
    "                # Permute the outcomes (dependent variable) within the category's dataframe\n",
    "                category_outcomes = category_df[self.dependent_variable].values\n",
    "                random.shuffle(category_outcomes)\n",
    "                permuted_data.loc[category_df.index, self.dependent_variable] = category_outcomes\n",
    "\n",
    "                # Calculate and store the correlation with the independent variable\n",
    "                correlation = category_df[self.independent_variable].corr(permuted_data.loc[category_df.index, self.dependent_variable])\n",
    "                self.permuted_correlations[category].append(correlation)\n",
    "                \n",
    "    def calculate_p_values(self):\n",
    "        '''\n",
    "        This calculate a two-tailed p-value.\n",
    "        '''\n",
    "        # Initialize a dictionary to store p-values for each category\n",
    "        self.p_values = {category: None for category in self.category_dataframes.keys()}\n",
    "\n",
    "        for category in self.category_dataframes.keys():\n",
    "            observed_val = self.observed_correlations[category]\n",
    "            permuted_dist = self.permuted_correlations[category]\n",
    "\n",
    "            # Calculate the p-value\n",
    "            p_value = np.mean(np.array(np.abs(permuted_dist)) > np.abs(observed_val))\n",
    "\n",
    "            self.p_values[category] = p_value\n",
    "            \n",
    "    def calculate_delta_r(self):\n",
    "        # Initialize a dictionary to store Delta-R values and their significance for each category combination\n",
    "        self.delta_r_values = {}\n",
    "        \n",
    "        # Get all unique combinations of categories\n",
    "        category_combinations = list(itertools.combinations(self.category_dataframes.keys(), 2))\n",
    "\n",
    "        for category1, category2 in category_combinations:\n",
    "            observed_val1 = self.observed_correlations[category1]\n",
    "            observed_val2 = self.observed_correlations[category2]\n",
    "\n",
    "            # Get the permuted distributions for both categories\n",
    "            permuted_dist1 = self.permuted_correlations[category1]\n",
    "            permuted_dist2 = self.permuted_correlations[category2]\n",
    "\n",
    "            delta_r = observed_val1 - observed_val2\n",
    "            delta_r_permuted = np.array(permuted_dist1) - np.array(permuted_dist2)\n",
    "\n",
    "            # Calculate the significance using a two-tailed test\n",
    "            if self.two_tail:\n",
    "                delta_r = np.abs(delta_r)\n",
    "                delta_r_permuted = np.abs(delta_r_permuted)\n",
    "            if self.similarity:\n",
    "                p_value = np.mean(delta_r_permuted < delta_r)\n",
    "            else:\n",
    "                p_value = np.mean(delta_r_permuted > delta_r)\n",
    "            # Store the Delta-R value and its significance\n",
    "            self.delta_r_values[(category1, category2)] = {\n",
    "                'delta_r': delta_r,\n",
    "                'p_value': p_value\n",
    "            }\n",
    "\n",
    "    def plot_correlations(self):\n",
    "        # Convert observed correlations data to a DataFrame\n",
    "        observed_data = pd.DataFrame({\n",
    "            'Category': self.observed_correlations.keys(),\n",
    "            'Correlation': self.observed_correlations.values()\n",
    "        })\n",
    "\n",
    "        # Set style and increase font size\n",
    "        sns.set_style(\"white\")\n",
    "        # sns.set(font_scale=1)\n",
    "\n",
    "        # Create the bar plot using the observed data\n",
    "        sns.barplot(x='Category', y='Correlation', data=observed_data, palette='tab10')\n",
    "        sns.despine()\n",
    "        # # Add p-value annotations using Annotator\n",
    "        p_values = [self.delta_r_values[comb]['p_value'] for comb in self.delta_r_values]\n",
    "        combinations = [f'{comb[0]} vs {comb[1]}' for comb in self.delta_r_values]\n",
    "        data = pd.DataFrame({'Combination': combinations, 'p-value': p_values})\n",
    "        # annotator = Annotator(ax=ax, data=data, x='Combination', y='p-value', loc='outside', fontsize=12)\n",
    "\n",
    "        # Save the figure if out_dir is provided\n",
    "        if self.out_dir:\n",
    "            plt.savefig(f\"{self.out_dir}/delta_correlation_plot.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{self.out_dir}/delta_correlation_plot.svg\", bbox_inches='tight')\n",
    "            print(f'Saved to {self.out_dir}/delta_correlation_plot.svg')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        return observed_data, data\n",
    "        \n",
    "    def run(self):\n",
    "        self.segregate_data()\n",
    "        self.calculate_observed_r_values()\n",
    "        self.permute_and_calculate_correlations()\n",
    "        self.calculate_p_values()\n",
    "        self.calculate_delta_r()\n",
    "        single_data, delta_data = self.plot_correlations()\n",
    "        return single_data, delta_data\n",
    "\n",
    "    def create_combined_plot(self, group_variable, dependent_variable):\n",
    "        sns.set()\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=len(self.df[group_variable].unique()), sharex=True)\n",
    "        ax[-1].set_xlabel(dependent_variable)\n",
    "\n",
    "        for i, group_val in enumerate(self.df[group_variable].unique()):\n",
    "            group_data = self.df[self.df[group_variable] == group_val][dependent_variable]\n",
    "            jittered_group_data = group_data + 0.1 * (2 * random.random() - 1)  # Add small vertical jitter\n",
    "            ax[i].scatter(jittered_group_data, [0] * len(group_data))\n",
    "            sns.kdeplot(group_data, ax=ax[i], shade=False, legend=False)\n",
    "            ax[i].set_yticks([])\n",
    "            ax[i].set_ylim(-0.01)\n",
    "            ax[i].set_ylabel(f'{group_variable} ' + str(group_val))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_correlations_with_lmplot(self, palette='Tab10'):\n",
    "        # Using seaborn's lmplot to plot linear regression lines for each category\n",
    "        if self.spearman:\n",
    "            print('Have rank-transformed data for visualization of Spearman correlation.')\n",
    "            self.df = self.df.rank()\n",
    "        lm = sns.lmplot(x=self.independent_variable, y=self.dependent_variable, \n",
    "                        hue=self.categorical_variable, data=self.df, \n",
    "                        aspect=1.5, height=5, palette=palette, legend=False, ci=0)\n",
    "\n",
    "        # Enhancements for better readability\n",
    "        lm.set_xlabels(f\"{self.independent_variable}\")\n",
    "        lm.set_ylabels(f\"{self.dependent_variable}\")\n",
    "        plt.title(\"Correlation Split by Category\")\n",
    "        plt.legend()\n",
    "        # Save the figure if out_dir is provided\n",
    "        if self.out_dir:\n",
    "            plt.savefig(f\"{self.out_dir}/delta_correlation_plot.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{self.out_dir}/delta_correlation_plot.svg\", bbox_inches='tight')\n",
    "            print(f'Saved to {self.out_dir}/delta_correlation_plot.svg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter Variables\n",
    "- dependent_variable: the name of the dependent variable\n",
    "- independent_variable: the name of the independent variable\n",
    "- categorical_variable: the column containing categorical information. This may be strings or numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable = 'Z_Scored_Percent_Cognitive_Improvement'\n",
    "independent_variable_list = ['Z_Scored_Subiculum_Connectivity_T', 'Age_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Reshaping the DataFrame\n",
    "melted_df = pd.melt(data_df, id_vars=[dependent_variable], value_vars=independent_variable_list, \n",
    "                    var_name='Category', value_name='Value')\n",
    "melted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable = 'Percent_Cognitive_Improvement'\n",
    "independent_variable = 'Subiculum_Connectivity_T_Redone'\n",
    "categorical_variable = 'Age_Group'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do You Want to Run A Spearman Correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = True\n",
    "two_tail = True\n",
    "similarity = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Run the Correlational ANCOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_r = ANCOVACorrelation(df=data_df, dependent_variable=dependent_variable, independent_variable=independent_variable, categorical_variable=categorical_variable, out_dir=out_dir, spearman=spearman,\n",
    "                            n_permutations=10000, similarity=similarity, two_tail=two_tail)\n",
    "single_data, delta_data = delta_r.run()\n",
    "delta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.join(out_dir, f'{dependent_variable}_delta_rho.csv'))\n",
    "delta_data.to_csv(os.path.join(out_dir, '{dependent_variable}_delta_rho.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Correlational ANCOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data_prep and category_dataframes are already defined\n",
    "dependent_variable = 'Z_Scored_Percent_Cognitive_Improvement'\n",
    "independent_variable = 'Z_Scored_Subiculum_Connectivity_T'\n",
    "categorical_variable = 'Age_Group'\n",
    "cohort_variable = 'City'\n",
    "correlation_method = 'pearson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataPreparation:\n",
    "    \"\"\"\n",
    "    A class used to segregate data by categorical variables and optional cohorts.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input dataframe containing the data.\n",
    "    dependent_variable : str\n",
    "        The name of the dependent variable column.\n",
    "    independent_variable : str\n",
    "        The name of the independent variable column.\n",
    "    categorical_variable : str\n",
    "        The name of the categorical variable column used for splitting.\n",
    "    cohort_variable : str, optional\n",
    "        The name of the cohort variable column used for additional splitting. Default is None.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    segregate_data():\n",
    "        Splits the dataframe into nested dictionaries by cohort and category.\n",
    "\n",
    "    Example Usage:\n",
    "    -------------\n",
    "    dependent_variable = 'Z_Scored_Percent_Cognitive_Improvement'\n",
    "    independent_variable = 'Subiculum_Connectivity_T_Redone'\n",
    "    categorical_variable = 'Age_Group'\n",
    "    cohort_variable = 'City'  # Optional, can be None if not needed\n",
    "\n",
    "    data_prep = DataPreparation(data_df, dependent_variable, independent_variable, categorical_variable, cohort_variable)\n",
    "    category_dataframes = data_prep.category_dataframes\n",
    "\n",
    "    for cohort, categories in category_dataframes.items():\n",
    "        print(f\"Cohort: {cohort}\")\n",
    "        for category, cat_df in categories.items():\n",
    "            print(f\"  Category: {category}\")\n",
    "            print(cat_df.head())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, dependent_variable, independent_variable, categorical_variable, cohort_variable=None):\n",
    "        self.dependent_variable = dependent_variable\n",
    "        self.independent_variable = independent_variable\n",
    "        self.categorical_variable = categorical_variable\n",
    "        self.cohort_variable = cohort_variable\n",
    "        self.df = self.prepare_dataframe(df)\n",
    "        self.category_dataframes = self.segregate_data()\n",
    "        self.category_dataframes = self.reverse_dictionary()\n",
    "        \n",
    "    def prepare_dataframe(self, df):\n",
    "        if self.cohort_variable is None:\n",
    "            df = df.loc[:, [self.dependent_variable, self.independent_variable, self.categorical_variable]]\n",
    "        else:\n",
    "            df = df.loc[:, [self.dependent_variable, self.independent_variable, self.categorical_variable, self.cohort_variable]]\n",
    "        return df\n",
    "    \n",
    "    def segregate_data(self):\n",
    "        \"\"\"\n",
    "        Splits the dataframe into nested dictionaries by cohort and category.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A nested dictionary where data is first split by cohort (if provided) and then by category.\n",
    "            Each entry corresponds to a subset of the data for a specific cohort and category.\n",
    "        \"\"\"\n",
    "        # Initialize dictionary to hold dataframes for each category\n",
    "        category_dataframes = {}\n",
    "        unique_categories = self.df[self.categorical_variable].unique()\n",
    "        # We do not need to split by cohort, as we can iterate over each cohort during plotting. \n",
    "        for category in unique_categories:\n",
    "            category_dataframes[category] = self.df[self.df[self.categorical_variable] == category]\n",
    "        category_dataframes['all_categories'] = self.df\n",
    "        return category_dataframes\n",
    "    \n",
    "    def reverse_dictionary(self):\n",
    "        reversed_dict = {k: self.category_dataframes[k] for k in reversed(self.category_dataframes)}\n",
    "        return reversed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ScatterPlot:\n",
    "    \"\"\"\n",
    "    A class to create scatter plots with linear regression lines for each category and cohort.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    data_prep : DataPreparation\n",
    "        An instance of the DataPreparation class containing the processed data.\n",
    "    colors : list\n",
    "        List of colors for each category.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    get_colors(colors_list):\n",
    "        Generates a list of colors for plotting.\n",
    "    get_scatterplots():\n",
    "        Creates scatter plots with linear regression lines for each category and cohort.\n",
    "    plot():\n",
    "        Plots the scatter plots.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_prep, colors_list=None, confidence_intervals=False):\n",
    "        self.data_prep = data_prep\n",
    "        self.colors = self.get_colors(colors_list)\n",
    "        self.scatterplots_dict = self.get_scatterplots(confidence_intervals)\n",
    "        \n",
    "    def get_colors(self, colors_list):\n",
    "        if colors_list is not None:\n",
    "            return sns.color_palette(colors_list, len(self.data_prep.df[self.data_prep.cohort_variable].unique()))\n",
    "        else:\n",
    "            return sns.color_palette(\"tab10\", len(self.data_prep.df[self.data_prep.cohort_variable].unique()))\n",
    "\n",
    "    def get_scatterplots(self,confidence_intervals):\n",
    "        scatterplots_dict = {}\n",
    "        for name, dataframe in self.data_prep.category_dataframes.items():\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            \n",
    "            if self.data_prep.cohort_variable is not None: \n",
    "                for i, cohort in enumerate(dataframe[self.data_prep.cohort_variable].unique()):\n",
    "                    cohort_df = dataframe[dataframe[self.data_prep.cohort_variable] == cohort]\n",
    "                    sns.regplot(x=self.data_prep.independent_variable, \n",
    "                                y=self.data_prep.dependent_variable, \n",
    "                                data=cohort_df, ax=ax, label=cohort, color=self.colors[i],\n",
    "                                ci=confidence_intervals)\n",
    "            else:\n",
    "                sns.regplot(x=self.data_prep.independent_variable, \n",
    "                            y=self.data_prep.dependent_variable, \n",
    "                            data=dataframe, ax=ax, label=name, color=self.colors[0],\n",
    "                            ci=confidence_intervals)\n",
    "                \n",
    "            ax.legend()\n",
    "            plt.title(name)\n",
    "            plt.tight_layout()\n",
    "            scatterplots_dict[name] = fig\n",
    "            plt.close()\n",
    "\n",
    "        return scatterplots_dict\n",
    "\n",
    "    def plot(self):\n",
    "        import warnings\n",
    "        for name, fig in self.scatterplots_dict.items():\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"error\", UserWarning)\n",
    "                    fig.show()\n",
    "            except UserWarning:\n",
    "                from IPython.display import display\n",
    "                display(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "class BarPlot:\n",
    "    \"\"\"\n",
    "    A class to create bar plots summarizing the correlation strengths for each category and cohort.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    data_prep : DataPreparation\n",
    "        An instance of the DataPreparation class containing the processed data.\n",
    "    colors : list\n",
    "        List of colors for each cohort.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    get_colors(colors_list):\n",
    "        Generates a list of colors for plotting.\n",
    "    get_barplots(method):\n",
    "        Creates bar plots summarizing the correlation strengths for each category and cohort.\n",
    "    plot():\n",
    "        Plots the bar plots.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_prep, colors_list=None, method='pearson'):\n",
    "        self.data_prep = data_prep\n",
    "        self.colors = self.get_colors(colors_list)\n",
    "        self.method = method\n",
    "        self.barplots_dict = self.get_barplots()\n",
    "        self.set_shared_y_limits()\n",
    "        \n",
    "    def get_colors(self, colors_list):\n",
    "        if colors_list is not None:\n",
    "            return sns.color_palette(colors_list, len(self.data_prep.df[self.data_prep.cohort_variable].unique()))\n",
    "        else:\n",
    "            return sns.color_palette(\"tab10\", len(self.data_prep.df[self.data_prep.cohort_variable].unique()))\n",
    "\n",
    "    def get_barplots(self):\n",
    "        barplots_dict = {}\n",
    "        all_correlations = []\n",
    "\n",
    "        for name, dataframe in self.data_prep.category_dataframes.items():\n",
    "            correlations = {}\n",
    "            p_values = {}\n",
    "\n",
    "            if self.data_prep.cohort_variable is not None:\n",
    "                for i, cohort in enumerate(dataframe[self.data_prep.cohort_variable].unique()):\n",
    "                    cohort_df = dataframe[dataframe[self.data_prep.cohort_variable] == cohort]\n",
    "                    if self.method == 'pearson':\n",
    "                        correlation, p_value = pearsonr(cohort_df[self.data_prep.independent_variable], cohort_df[self.data_prep.dependent_variable])\n",
    "                    else:  # spearman\n",
    "                        correlation, p_value = spearmanr(cohort_df[self.data_prep.independent_variable], cohort_df[self.data_prep.dependent_variable])\n",
    "                    correlations[cohort] = correlation\n",
    "                    p_values[cohort] = p_value\n",
    "                    all_correlations.append(correlation)\n",
    "            else:\n",
    "                if self.method == 'pearson':\n",
    "                    correlation, p_value = pearsonr(dataframe[self.data_prep.independent_variable], dataframe[self.data_prep.dependent_variable])\n",
    "                else:  # spearman\n",
    "                    correlation, p_value = spearmanr(dataframe[self.data_prep.independent_variable], dataframe[self.data_prep.dependent_variable])\n",
    "                correlations[name] = correlation\n",
    "                p_values[name] = p_value\n",
    "                all_correlations.append(correlation)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            bars = sns.barplot(y=list(correlations.keys()), x=list(correlations.values()), ax=ax, palette=self.colors[:len(correlations)])\n",
    "            ax.set_xlabel('Correlation Strength')\n",
    "            ax.set_title(name)\n",
    "\n",
    "            # Outline bars with p-value < 0.05\n",
    "            for j, bar in enumerate(bars.patches):\n",
    "                cohort_name = list(correlations.keys())[j]\n",
    "                if p_values[cohort_name] < 0.05:\n",
    "                    bar.set_edgecolor('black')\n",
    "                    bar.set_linewidth(1.5)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            barplots_dict[name] = fig\n",
    "            plt.close()\n",
    "\n",
    "        self.y_min = min(all_correlations)\n",
    "        self.y_max = max(all_correlations)\n",
    "\n",
    "        return barplots_dict\n",
    "\n",
    "    def set_shared_y_limits(self):\n",
    "        for name, fig in self.barplots_dict.items():\n",
    "            ax = fig.get_axes()[0]\n",
    "            ax.set_xlim(self.y_min, self.y_max)\n",
    "\n",
    "    def plot(self):\n",
    "        import warnings\n",
    "        for name, fig in self.barplots_dict.items():\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"error\", UserWarning)\n",
    "                    fig.show()\n",
    "            except UserWarning:\n",
    "                from IPython.display import display\n",
    "                display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "class CombinedPlotV1:\n",
    "    \"\"\"\n",
    "    A class to create a combined plot of scatter plots and bar plots.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    scatter_plot : ScatterPlot\n",
    "        An instance of the ScatterPlot class containing the scatter plots.\n",
    "    bar_plot : BarPlot\n",
    "        An instance of the BarPlot class containing the bar plots.\n",
    "    out_dir : str\n",
    "        Directory to save the output plot.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    create_figure():\n",
    "        Creates a combined figure with scatter plots and bar plots.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scatter_plot, bar_plot, out_dir=None):\n",
    "        self.scatter_plot = scatter_plot\n",
    "        self.bar_plot = bar_plot\n",
    "        self.out_dir = out_dir\n",
    "        self.fig, self.axes = self.create_figure()\n",
    "\n",
    "    def create_figure(self):\n",
    "        \"\"\"\n",
    "        Creates a combined figure with scatter plots and bar plots.\n",
    "        \"\"\"\n",
    "        categories = list(self.scatter_plot.data_prep.category_dataframes.keys())\n",
    "        num_categories = len(categories)\n",
    "\n",
    "        fig, axes = plt.subplots(num_categories, 2, figsize=(15, 6 * num_categories))\n",
    "\n",
    "        for i, category in enumerate(categories):\n",
    "            # Scatter Plot\n",
    "            scatter_ax = axes[i, 0]\n",
    "            scatter_fig = self.scatter_plot.scatterplots_dict[category]\n",
    "            scatter_canvas = scatter_fig.canvas\n",
    "            scatter_canvas.draw()\n",
    "            scatter_image = scatter_canvas.buffer_rgba()\n",
    "            scatter_ax.imshow(scatter_image)\n",
    "            scatter_ax.axis('off')\n",
    "\n",
    "            # Bar Plot\n",
    "            bar_ax = axes[i, 1]\n",
    "            bar_fig = self.bar_plot.barplots_dict[category]\n",
    "            bar_canvas = bar_fig.canvas\n",
    "            bar_canvas.draw()\n",
    "            bar_image = bar_canvas.buffer_rgba()\n",
    "            bar_ax.imshow(bar_image)\n",
    "            bar_ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if self.out_dir:\n",
    "            plt.savefig(f\"{self.out_dir}/combined_plot.svg\", format='svg')\n",
    "        plt.close()\n",
    "        return fig, axes\n",
    "    \n",
    "    def plot(self):\n",
    "        import warnings\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"error\", UserWarning)\n",
    "                self.fig.show()\n",
    "        except UserWarning:\n",
    "            from IPython.display import display\n",
    "            display(self.fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contrast cohorts within categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PermutationAnalysis:\n",
    "    def __init__(self, data_prep, onetail=True, correlation_method='pearson'):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        onetail (bool): whether to perform 1-tail testing or not. \n",
    "            1-tail testing compares if the correlation is larger than the raw value of all other correlations. \n",
    "            2-tail testing compares the absolute value of the correlations.\n",
    "        correlation_method (str): what correlation to use. pearson | spearman | kendall \n",
    "        Notes:\n",
    "        The similarity by chance is 1-p. The difference by chance is the p value outputted. \n",
    "        \"\"\"\n",
    "        self.data_prep = data_prep\n",
    "        self.observed_correlations = {}\n",
    "        self.permuted_differences = {}\n",
    "        self.p_values = {}\n",
    "        self.onetail = onetail\n",
    "        self.correlation_method = correlation_method\n",
    "\n",
    "    def calculate_observed_differences(self, df):\n",
    "        \"\"\"Calculate the observed differences for each cohort within a dataframe.\"\"\"\n",
    "        observed_correlations = {}\n",
    "        for cohort in df[self.data_prep.cohort_variable].unique():\n",
    "            cohort_df = df[df[self.data_prep.cohort_variable] == cohort]\n",
    "            correlation = cohort_df[self.data_prep.independent_variable].corr(cohort_df[self.data_prep.dependent_variable], method=self.correlation_method)\n",
    "            observed_correlations[cohort] = correlation\n",
    "\n",
    "        observed_differences = {}\n",
    "        cohorts = list(observed_correlations.keys())\n",
    "        for i in range(len(cohorts)):\n",
    "            for j in range(i + 1, len(cohorts)):\n",
    "                cohort1, cohort2 = cohorts[i], cohorts[j]\n",
    "                diff = observed_correlations[cohort1] - observed_correlations[cohort2]\n",
    "                \n",
    "                observed_differences[(cohort1, cohort2)] = diff if self.onetail else np.abs(diff)\n",
    "\n",
    "        return observed_differences\n",
    "\n",
    "    def calculate_permuted_differences(self, df, n_permutations=1000):\n",
    "        \"\"\"Permute the data and calculate the permuted differences for each cohort within a dataframe.\"\"\"\n",
    "        permuted_differences = []\n",
    "\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_df = df.copy()\n",
    "            permuted_df[self.data_prep.dependent_variable] = np.random.permutation(permuted_df[self.data_prep.dependent_variable].values)\n",
    "            correlations = {}\n",
    "            for cohort in df[self.data_prep.cohort_variable].unique():\n",
    "                cohort_df = permuted_df[permuted_df[self.data_prep.cohort_variable] == cohort]\n",
    "                correlation = cohort_df[self.data_prep.independent_variable].corr(cohort_df[self.data_prep.dependent_variable], method=self.correlation_method)\n",
    "                correlations[cohort] = correlation\n",
    "\n",
    "            permuted_diffs = {}\n",
    "            cohorts = list(correlations.keys())\n",
    "            for i in range(len(cohorts)):\n",
    "                for j in range(i + 1, len(cohorts)):\n",
    "                    cohort1, cohort2 = cohorts[i], cohorts[j]\n",
    "                    diff = correlations[cohort1] - correlations[cohort2]\n",
    "                    permuted_diffs[(cohort1, cohort2)] = diff if self.onetail else np.abs(diff)\n",
    "\n",
    "            permuted_differences.append(permuted_diffs)\n",
    "\n",
    "        return permuted_differences\n",
    "\n",
    "    def calculate_p_values(self, observed_differences, permuted_differences):\n",
    "        \"\"\"Calculate p-values based on the observed and permuted differences.\"\"\"\n",
    "        p_values = {}\n",
    "        for (cohort1, cohort2), observed_diff in observed_differences.items():\n",
    "            permuted_diffs = [permuted[(cohort1, cohort2)] for permuted in permuted_differences]\n",
    "            p_value = np.mean([diff >= observed_diff for diff in permuted_diffs])\n",
    "            p_values[(cohort1, cohort2, f'delta = {observed_diff}')] = p_value\n",
    "\n",
    "        return p_values\n",
    "\n",
    "    def run_analysis(self, n_permutations=1000):\n",
    "        \"\"\"Run the entire permutation analysis for each dataframe in the data_prep object.\"\"\"\n",
    "        for category, df in self.data_prep.category_dataframes.items():\n",
    "            observed_differences = self.calculate_observed_differences(df)\n",
    "            permuted_differences = self.calculate_permuted_differences(df, n_permutations)\n",
    "            p_values = self.calculate_p_values(observed_differences, permuted_differences)\n",
    "            self.p_values[category] = p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contrast across categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ContrastAnalysis:\n",
    "    def __init__(self, perm_analysis, onetail=True, correlation_method='pearson'):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        onetail (bool): whether to perform 1-tail testing or not. \n",
    "            1-tail testing compares if the correlation is larger than the raw value of all other correlations. \n",
    "            2-tail testing compares the absolute value of the correlations. \n",
    "        Notes:\n",
    "        The similarity by chance is 1-p. The difference by chance is the p value outputted. \n",
    "        \"\"\"\n",
    "        self.perm_analysis = perm_analysis\n",
    "        self.summed_correlations = {}\n",
    "        self.cross_dataframe_differences = {}\n",
    "        self.cross_dataframe_p_values = {}\n",
    "        self.onetail = onetail\n",
    "        self.correlation_method = correlation_method\n",
    "\n",
    "    def summate_correlations(self):\n",
    "        \"\"\"Summate the correlations within each dataframe.\"\"\"\n",
    "        for category, df in self.perm_analysis.data_prep.category_dataframes.items():\n",
    "            summed_correlation = 0\n",
    "            for cohort in df[self.perm_analysis.data_prep.cohort_variable].unique():\n",
    "                cohort_df = df[df[self.perm_analysis.data_prep.cohort_variable] == cohort]\n",
    "                correlation = cohort_df[self.perm_analysis.data_prep.independent_variable].corr(cohort_df[self.perm_analysis.data_prep.dependent_variable], method=self.correlation_method)\n",
    "                summed_correlation += correlation\n",
    "            self.summed_correlations[category] = summed_correlation\n",
    "\n",
    "    def calculate_cross_dataframe_differences(self, n_permutations=1000):\n",
    "        \"\"\"Calculate the difference between summed correlations from different dataframes and calculate p-values.\"\"\"\n",
    "        categories = list(self.summed_correlations.keys())\n",
    "        for i in range(len(categories)):\n",
    "            for j in range(i + 1, len(categories)):\n",
    "                category1, category2 = categories[i], categories[j]\n",
    "                observed_diff = self.summed_correlations[category1] - self.summed_correlations[category2]\n",
    "                self.cross_dataframe_differences[(category1, category2)] = observed_diff if self.onetail else np.abs(observed_diff)\n",
    "\n",
    "                permuted_diffs = []\n",
    "                for _ in range(n_permutations):\n",
    "                    permuted_df1 = self.perm_analysis.data_prep.category_dataframes[category1].copy()\n",
    "                    permuted_df2 = self.perm_analysis.data_prep.category_dataframes[category2].copy()\n",
    "\n",
    "                    permuted_df1[self.perm_analysis.data_prep.dependent_variable] = np.random.permutation(permuted_df1[self.perm_analysis.data_prep.dependent_variable].values)\n",
    "                    permuted_df2[self.perm_analysis.data_prep.dependent_variable] = np.random.permutation(permuted_df2[self.perm_analysis.data_prep.dependent_variable].values)\n",
    "\n",
    "                    permuted_sum1 = 0\n",
    "                    for cohort in permuted_df1[self.perm_analysis.data_prep.cohort_variable].unique():\n",
    "                        cohort_df = permuted_df1[permuted_df1[self.perm_analysis.data_prep.cohort_variable] == cohort]\n",
    "                        correlation = cohort_df[self.perm_analysis.data_prep.independent_variable].corr(cohort_df[self.perm_analysis.data_prep.dependent_variable], method=self.correlation_method)\n",
    "                        permuted_sum1 += correlation\n",
    "\n",
    "                    permuted_sum2 = 0\n",
    "                    for cohort in permuted_df2[self.perm_analysis.data_prep.cohort_variable].unique():\n",
    "                        cohort_df = permuted_df2[permuted_df2[self.perm_analysis.data_prep.cohort_variable] == cohort]\n",
    "                        correlation = cohort_df[self.perm_analysis.data_prep.independent_variable].corr(cohort_df[self.perm_analysis.data_prep.dependent_variable], method=self.correlation_method)\n",
    "                        permuted_sum2 += correlation\n",
    "\n",
    "                    permuted_diff = permuted_sum1 - permuted_sum2\n",
    "                    permuted_diffs.append(permuted_diff if self.onetail else np.abs(permuted_diff))\n",
    "\n",
    "                p_value = np.mean([diff >= observed_diff for diff in permuted_diffs])\n",
    "                self.cross_dataframe_p_values[(category1, category2, f'delta = {observed_diff}')] = p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the difference across categories and cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "class CrossCategoryPermutationAnalysis:\n",
    "    def __init__(self, perm_analysis, onetail=True, correlation_method='pearson'):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        onetail (bool): whether to perform 1-tail testing or not. \n",
    "            1-tail testing compares if the correlation is larger than the raw value of all other correlations. \n",
    "            2-tail testing compares the absolute value of the correlations. \n",
    "        Notes:\n",
    "        The similarity by chance is 1-p. The difference by chance is the p value outputted. \n",
    "        \"\"\"\n",
    "        self.perm_analysis = perm_analysis\n",
    "        self.observed_correlations = {}\n",
    "        self.permuted_correlations = []\n",
    "        self.observed_differences = {}\n",
    "        self.permuted_differences = []\n",
    "        self.p_values = {}\n",
    "        self.onetail = onetail\n",
    "        self.correlation_method = correlation_method\n",
    "\n",
    "    def calculate_observed_correlations(self):\n",
    "        \"\"\"Calculate the observed correlations for each cohort within each category dataframe.\"\"\"\n",
    "        for category, df in self.perm_analysis.data_prep.category_dataframes.items():\n",
    "            self.observed_correlations[category] = {}\n",
    "            for cohort in df[self.perm_analysis.data_prep.cohort_variable].unique():\n",
    "                cohort_df = df[df[self.perm_analysis.data_prep.cohort_variable] == cohort]\n",
    "                correlation = cohort_df[self.perm_analysis.data_prep.independent_variable].corr(cohort_df[self.perm_analysis.data_prep.dependent_variable], method=self.correlation_method)\n",
    "                self.observed_correlations[category][cohort] = correlation\n",
    "\n",
    "    def calculate_observed_differences(self):\n",
    "        \"\"\"Calculate the differences between correlations of every cohort in one category dataframe against every cohort in other category dataframes.\"\"\"\n",
    "        categories = list(self.observed_correlations.keys())\n",
    "        for i in range(len(categories)):\n",
    "            for j in range(i + 1, len(categories)):\n",
    "                category1, category2 = categories[i], categories[j]\n",
    "                self.observed_differences[(category1, category2)] = {}\n",
    "                for cohort1, corr1 in self.observed_correlations[category1].items():\n",
    "                    for cohort2, corr2 in self.observed_correlations[category2].items():\n",
    "                        diff = corr1 - corr2\n",
    "                        self.observed_differences[(category1, category2)][(cohort1, cohort2)] = diff if self.onetail else np.abs(diff)\n",
    "\n",
    "\n",
    "    def calculate_permuted_differences(self, n_permutations=1000):\n",
    "        \"\"\"Permute the data and calculate the permuted differences for each permutation.\"\"\"\n",
    "        categories = list(self.perm_analysis.data_prep.category_dataframes.keys())\n",
    "        all_data = pd.concat(self.perm_analysis.data_prep.category_dataframes.values())\n",
    "\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_data = all_data.copy()\n",
    "            permuted_data[self.perm_analysis.data_prep.dependent_variable] = np.random.permutation(permuted_data[self.perm_analysis.data_prep.dependent_variable].values)\n",
    "\n",
    "            permuted_correlations = {}\n",
    "            for category in categories:\n",
    "                permuted_correlations[category] = {}\n",
    "                df = permuted_data[permuted_data[self.perm_analysis.data_prep.categorical_variable] == category]\n",
    "                for cohort in df[self.perm_analysis.data_prep.cohort_variable].unique():\n",
    "                    cohort_df = df[df[self.perm_analysis.data_prep.cohort_variable] == cohort]\n",
    "                    correlation = cohort_df[self.perm_analysis.data_prep.independent_variable].corr(cohort_df[self.perm_analysis.data_prep.dependent_variable], method=self.correlation_method)\n",
    "                    permuted_correlations[category][cohort] = correlation\n",
    "\n",
    "            permuted_diffs = {}\n",
    "            for i in range(len(categories)):\n",
    "                for j in range(i + 1, len(categories)):\n",
    "                    category1, category2 = categories[i], categories[j]\n",
    "                    permuted_diffs[(category1, category2)] = {}\n",
    "                    for cohort1, corr1 in permuted_correlations[category1].items():\n",
    "                        for cohort2, corr2 in permuted_correlations[category2].items():\n",
    "                            diff = corr1 - corr2\n",
    "                            permuted_diffs[(category1, category2)][(cohort1, cohort2)] = diff if self.onetail else np.abs(diff)\n",
    "\n",
    "            self.permuted_differences.append(permuted_diffs)\n",
    "\n",
    "    def calculate_p_values(self):\n",
    "        \"\"\"Calculate p-values based on the observed and permuted differences.\"\"\"\n",
    "        categories = list(self.observed_correlations.keys())\n",
    "        for i in range(len(categories)):\n",
    "            for j in range(i + 1, len(categories)):\n",
    "                category1, category2 = categories[i], categories[j]\n",
    "                self.p_values[(category1, category2)] = {}\n",
    "                for (cohort1, cohort2), observed_diff in self.observed_differences[(category1, category2)].items():\n",
    "                    permuted_diffs = [permuted[(category1, category2)][(cohort1, cohort2)] for permuted in self.permuted_differences if (category1, category2) in permuted and (cohort1, cohort2) in permuted[(category1, category2)]]\n",
    "                    p_value = np.mean([diff >= observed_diff for diff in permuted_diffs])\n",
    "                    self.p_values[(category1, category2)][(cohort1, cohort2, f'delta = {observed_diff}')] = p_value\n",
    "\n",
    "    def run_cross_category_permutation_analysis(self, n_permutations=1000):\n",
    "        \"\"\"Run the entire cross-category permutation analysis.\"\"\"\n",
    "        self.calculate_observed_correlations()\n",
    "        self.calculate_observed_differences()\n",
    "        self.calculate_permuted_differences(n_permutations)\n",
    "        self.calculate_p_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run It All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "class CombinedPlot:\n",
    "    \"\"\"\n",
    "    A class to create a combined plot of scatter plots and bar plots.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    scatter_plot : ScatterPlot\n",
    "        An instance of the ScatterPlot class containing the scatter plots.\n",
    "    bar_plot : BarPlot\n",
    "        An instance of the BarPlot class containing the bar plots.\n",
    "    out_dir : str\n",
    "        Directory to save the output plot.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    create_figure():\n",
    "        Creates a combined figure with scatter plots and bar plots.\n",
    "    plot():\n",
    "        Displays the combined figure.\n",
    "    save_figure():\n",
    "        Saves the combined figure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scatter_plot, bar_plot, out_dir=None):\n",
    "        self.scatter_plot = scatter_plot\n",
    "        self.bar_plot = bar_plot\n",
    "        self.out_dir = out_dir\n",
    "        self.fig, self.axes = self.create_figure()\n",
    "        self.set_shared_x_limits()\n",
    "\n",
    "    def set_shared_x_limits(self):\n",
    "        \"\"\"\n",
    "        Sets the x-limits for scatter plots and bar plots to be shared separately.\n",
    "        \"\"\"\n",
    "        scatter_x_limits = [self.axes[i, 0].get_xlim() for i in range(self.axes.shape[0])]\n",
    "        scatter_x_min = min([lim[0] for lim in scatter_x_limits])\n",
    "        scatter_x_max = max([lim[1] for lim in scatter_x_limits])\n",
    "\n",
    "        bar_x_limits = [self.axes[i, 1].get_xlim() for i in range(self.axes.shape[0])]\n",
    "        bar_x_min = min([lim[0] for lim in bar_x_limits])\n",
    "        bar_x_max = max([lim[1] for lim in bar_x_limits])\n",
    "\n",
    "        for i in range(self.axes.shape[0]):\n",
    "            self.axes[i, 0].set_xlim(scatter_x_min, scatter_x_max)\n",
    "            self.axes[i, 1].set_xlim(bar_x_min, bar_x_max)\n",
    "\n",
    "    def create_figure(self):\n",
    "        \"\"\"\n",
    "        Creates a combined figure with scatter plots and bar plots.\n",
    "        \"\"\"\n",
    "        categories = list(self.scatter_plot.data_prep.category_dataframes.keys())\n",
    "        num_categories = len(categories)\n",
    "\n",
    "        fig, axes = plt.subplots(num_categories, 2, figsize=(15, 6 * num_categories))\n",
    "\n",
    "        for i, category in enumerate(categories):\n",
    "            # Scatter Plot\n",
    "            scatter_ax = axes[i, 0]\n",
    "            scatter_data = self.scatter_plot.data_prep.category_dataframes[category]\n",
    "            if self.scatter_plot.data_prep.cohort_variable is not None:\n",
    "                for j, cohort in enumerate(scatter_data[self.scatter_plot.data_prep.cohort_variable].unique()):\n",
    "                    cohort_df = scatter_data[scatter_data[self.scatter_plot.data_prep.cohort_variable] == cohort]\n",
    "                    sns.regplot(\n",
    "                        x=self.scatter_plot.data_prep.independent_variable,\n",
    "                        y=self.scatter_plot.data_prep.dependent_variable,\n",
    "                        data=cohort_df,\n",
    "                        ax=scatter_ax,\n",
    "                        label=cohort,\n",
    "                        color=self.scatter_plot.colors[j],\n",
    "                        ci=None\n",
    "                    )\n",
    "            else:\n",
    "                sns.regplot(\n",
    "                    x=self.scatter_plot.data_prep.independent_variable,\n",
    "                    y=self.scatter_plot.data_prep.dependent_variable,\n",
    "                    data=scatter_data,\n",
    "                    ax=scatter_ax,\n",
    "                    label=category,\n",
    "                    color=self.scatter_plot.colors[0],\n",
    "                    ci=None\n",
    "                )\n",
    "            scatter_ax.legend()\n",
    "            scatter_ax.set_title(category)\n",
    "            scatter_ax.set_xlabel(self.scatter_plot.data_prep.independent_variable)\n",
    "            scatter_ax.set_ylabel(self.scatter_plot.data_prep.dependent_variable)\n",
    "\n",
    "            # Bar Plot\n",
    "            bar_ax = axes[i, 1]\n",
    "            bar_data = self.bar_plot.data_prep.category_dataframes[category]\n",
    "            correlations = []\n",
    "            p_values = []\n",
    "            cohorts = []\n",
    "\n",
    "            if self.bar_plot.data_prep.cohort_variable is not None:\n",
    "                for j, cohort in enumerate(bar_data[self.bar_plot.data_prep.cohort_variable].unique()):\n",
    "                    cohort_df = bar_data[bar_data[self.bar_plot.data_prep.cohort_variable] == cohort]\n",
    "                    if self.bar_plot.method == 'pearson':\n",
    "                        correlation, p_value = pearsonr(\n",
    "                            cohort_df[self.bar_plot.data_prep.independent_variable],\n",
    "                            cohort_df[self.bar_plot.data_prep.dependent_variable]\n",
    "                        )\n",
    "                    else:\n",
    "                        correlation, p_value = spearmanr(\n",
    "                            cohort_df[self.bar_plot.data_prep.independent_variable],\n",
    "                            cohort_df[self.bar_plot.data_prep.dependent_variable]\n",
    "                        )\n",
    "                    correlations.append(correlation)\n",
    "                    p_values.append(p_value)\n",
    "                    cohorts.append(cohort)\n",
    "            else:\n",
    "                if self.bar_plot.method == 'pearson':\n",
    "                    correlation, p_value = pearsonr(\n",
    "                        bar_data[self.bar_plot.data_prep.independent_variable],\n",
    "                        bar_data[self.bar_plot.data_prep.dependent_variable]\n",
    "                    )\n",
    "                else:\n",
    "                    correlation, p_value = spearmanr(\n",
    "                        bar_data[self.bar_plot.data_prep.independent_variable],\n",
    "                        bar_data[self.bar_plot.data_prep.dependent_variable]\n",
    "                    )\n",
    "                correlations.append(correlation)\n",
    "                p_values.append(p_value)\n",
    "                cohorts.append(category)\n",
    "            \n",
    "            sns.barplot(x=correlations, y=cohorts, ax=bar_ax, palette=self.bar_plot.colors[:len(cohorts)])\n",
    "            bar_ax.set_xlabel('Correlation Strength')\n",
    "            bar_ax.set_title(category)\n",
    "\n",
    "            # Outline bars with p-value < 0.05\n",
    "            for bar, p_value in zip(bar_ax.patches, p_values):\n",
    "                if p_value < 0.05:\n",
    "                    bar.set_edgecolor('black')\n",
    "                    bar.set_linewidth(1.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self.bar_plot.set_shared_y_limits()\n",
    "        return fig, axes\n",
    "\n",
    "    def save_figure(self):\n",
    "        \"\"\"\n",
    "        Saves the combined figure to the specified output directory.\n",
    "        \"\"\"\n",
    "        if self.out_dir:\n",
    "            self.fig.savefig(f\"{self.out_dir}/combined_plot.svg\", format='svg')\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Displays the combined figure.\n",
    "        \"\"\"\n",
    "        import warnings\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"error\", UserWarning)\n",
    "                self.fig.show()\n",
    "        except UserWarning:\n",
    "            from IPython.display import display\n",
    "            display(self.fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data\n",
    "data_prep = DataPreparation(data_df, dependent_variable, independent_variable, categorical_variable, cohort_variable)\n",
    "\n",
    "# Create scatter plot instance\n",
    "scatter_plot = ScatterPlot(data_prep, colors_list=['#d62728', '#1f77b4', '#17becf'])\n",
    "\n",
    "# Create bar plot instance\n",
    "bar_plot = BarPlot(data_prep, method='pearson', colors_list=['#d62728', '#1f77b4', '#17becf'])\n",
    "\n",
    "# Create combined plot instance\n",
    "combined_plot = CombinedPlot(scatter_plot, bar_plot, out_dir=out_dir)\n",
    "combined_plot.save_figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Values within Categories Across Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform permutation analysis\n",
    "perm_analysis = PermutationAnalysis(data_prep, onetail=False, correlation_method=correlation_method)\n",
    "perm_analysis.run_analysis(n_permutations=10000)\n",
    "p_values = perm_analysis.p_values\n",
    "from pprint import pprint\n",
    "pprint(p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Values Across Categories Across Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-category permutation analysis\n",
    "cross_cat_perm_analysis = CrossCategoryPermutationAnalysis(perm_analysis, onetail=False, correlation_method=correlation_method)\n",
    "cross_cat_perm_analysis.run_cross_category_permutation_analysis(n_permutations=10000)\n",
    "# Pretty print the cross-category p_values\n",
    "from pprint import pprint\n",
    "pprint(cross_cat_perm_analysis.p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Values Across Categories (contrast all cohorts within category against all cohorts in another category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform contrast analysis\n",
    "contrast_analysis = ContrastAnalysis(perm_analysis, onetail=True, correlation_method=correlation_method)\n",
    "contrast_analysis.summate_correlations()\n",
    "contrast_analysis.calculate_cross_dataframe_differences(n_permutations=10000)\n",
    "\n",
    "# Pretty print the cross-dataframe p_values\n",
    "pprint(contrast_analysis.cross_dataframe_p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the permutation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pformat\n",
    "\n",
    "def save_dicts_as_py(out_dir, **dicts):\n",
    "    \"\"\"\n",
    "    Save dictionaries as .py files in the specified directory with pretty-print formatting.\n",
    "\n",
    "    Parameters:\n",
    "    out_dir (str): Directory to save the Python files.\n",
    "    **dicts: Arbitrary number of dictionaries to save, with keys as file names (without .py extension).\n",
    "    \"\"\"\n",
    "\n",
    "    # Save each dictionary as a Python file\n",
    "    for file_name, data in dicts.items():\n",
    "        try:\n",
    "            with open(os.path.join(out_dir, f\"{file_name}.py\"), 'w') as py_file:\n",
    "                py_file.write(f\"{file_name} = {pformat(data)}\\n\")\n",
    "            print(f\"Successfully saved {file_name}.py\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while saving {file_name}.py: {e}\")\n",
    "\n",
    "# Save dictionaries as .py files\n",
    "save_dicts_as_py(out_dir,\n",
    "                 cross_cat_perm_analysis_p_values= cross_cat_perm_analysis.p_values,\n",
    "                 contrast_analysis_cross_dataframe_p_values=contrast_analysis.cross_dataframe_p_values,\n",
    "                 perm_analysis_p_values=perm_analysis.p_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Delta Scatterplot (Pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the class and its methods\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "import seaborn as sns\n",
    "class ScatterWithConfidence:\n",
    "    def __init__(self, data_df):\n",
    "        self.data_df = data_df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_analytic_confidence_interval(x, y, x_vals):\n",
    "        slope, intercept = np.polyfit(x, y, 1)\n",
    "        y_fit = slope * x_vals + intercept\n",
    "        \n",
    "        residuals = y - (slope * x + intercept)\n",
    "        stderr = np.sqrt(np.sum(residuals**2) / (len(y) - 2))\n",
    "        \n",
    "        t_value = t.ppf(0.975, df=len(x)-2)\n",
    "        ci = t_value * stderr * np.sqrt(1/len(x) + (x_vals - np.mean(x))**2 / np.sum((x - np.mean(x))**2))\n",
    "        \n",
    "        upper_bound = y_fit + ci\n",
    "        lower_bound = y_fit - ci\n",
    "        \n",
    "        return y_fit, lower_bound, upper_bound\n",
    "\n",
    "    def permute_data_and_difference_in_pearson_r(self, x_one, x_two, split_by, split_value, n_permutations=1000, permute_columns=[]):\n",
    "        original_diff = self.data_df[self.data_df[split_by] < split_value][x_one].corr(self.data_df[self.data_df[split_by] < split_value][x_two]) - \\\n",
    "                       self.data_df[self.data_df[split_by] >= split_value][x_one].corr(self.data_df[self.data_df[split_by] >= split_value][x_two])\n",
    "\n",
    "        permuted_diffs = []\n",
    "\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_df = self.data_df.copy()\n",
    "            for column in permute_columns:\n",
    "                permuted_df[column] = np.random.permutation(permuted_df[column].values)\n",
    "            \n",
    "            diff = permuted_df[permuted_df[split_by] < split_value][x_one].corr(permuted_df[permuted_df[split_by] < split_value][x_two]) - \\\n",
    "                   permuted_df[permuted_df[split_by] >= split_value][x_one].corr(permuted_df[permuted_df[split_by] >= split_value][x_two])\n",
    "            \n",
    "            permuted_diffs.append(diff)\n",
    "\n",
    "        p_value = np.mean([diff <= original_diff for diff in permuted_diffs])\n",
    "        return original_diff, p_value\n",
    "\n",
    "    def plot_with_analytic_ci_manual_pvalue(self, x_one, x_two, \n",
    "                                            split_by, split_value, \n",
    "                                            x_label='X1', y_label='X2', \n",
    "                                            upper_split_legend='Above Split', lower_split_legend='Below Split',\n",
    "                                            alpha=0.3, manual_p_value=None, permute_column=None, \n",
    "                                            save=False, out_dir=None,\n",
    "                                            colour1='red', colour2='blue'):\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        \n",
    "        group1 = self.data_df[self.data_df[split_by] < split_value]\n",
    "        group2 = self.data_df[self.data_df[split_by] >= split_value]\n",
    "\n",
    "        ax.scatter(group1[x_one], group1[x_two], color=colour1, label=lower_split_legend, s=40, alpha=alpha, marker='o')\n",
    "        ax.scatter(group2[x_one], group2[x_two], color=colour2, label=upper_split_legend, s=40, alpha=alpha, marker='o')\n",
    "        \n",
    "        x_vals = np.linspace(self.data_df[x_one].min(), self.data_df[x_one].max(), 400)\n",
    "        \n",
    "        for group, color in [(group1, colour1), (group2, colour2)]:\n",
    "            y_fit, lower_bound, upper_bound = self.compute_analytic_confidence_interval(group[x_one], group[x_two], x_vals)\n",
    "            ax.plot(x_vals, y_fit, color=color)\n",
    "            # ax.fill_between(x_vals, lower_bound, upper_bound, color=color, alpha=alpha/4)\n",
    "        \n",
    "        if manual_p_value is None:\n",
    "            if permute_column:\n",
    "                rho, manual_p_value = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, n_permutations=10000, permute_columns=[permute_column])\n",
    "            else:\n",
    "                rho, manual_p_value = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, n_permutations=10000, permute_columns=[x_one, x_two, split_by])\n",
    "        \n",
    "        ax.set_title(f\"\\u0394 r = {rho:.2f} , p = {manual_p_value:.4f}\")\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.legend(loc='best', frameon=False)\n",
    "        ax.grid(False)\n",
    "        sns.despine(ax=ax)\n",
    "        \n",
    "        if save and out_dir is not None:\n",
    "            plt.savefig(f\"{out_dir}/scatter_with_polyfit_and_analytic_ci.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{out_dir}/scatter_with_polyfit_and_analytic_ci.svg\", bbox_inches='tight')\n",
    "            print(f'Saved to {out_dir}/scatter_with_polyfit_and_analytic_ci.svg')\n",
    "        return fig\n",
    "    \n",
    "class DeltaCorrelation(ScatterWithConfidence):\n",
    "    def __init__(self, data_df):\n",
    "        super().__init__(data_df)\n",
    "\n",
    "    def plot_histogram_of_delta_r(self, x_one, x_two, split_by, split_value, n_permutations=1000, \n",
    "                                permute_columns=[], bins=50, one_tail=False, color_palette='dark'):\n",
    "        # Generate the empirical distribution of delta_r\n",
    "        delta_rs = []\n",
    "        for _ in range(n_permutations):\n",
    "            permuted_df = self.data_df.copy()\n",
    "            for column in permute_columns:\n",
    "                permuted_df[column] = np.random.permutation(permuted_df[column].values)\n",
    "\n",
    "            delta_r = permuted_df[permuted_df[split_by] < split_value][x_one].corr(permuted_df[permuted_df[split_by] < split_value][x_two]) - \\\n",
    "                    permuted_df[permuted_df[split_by] >= split_value][x_one].corr(permuted_df[permuted_df[split_by] >= split_value][x_two])\n",
    "            delta_rs.append(delta_r)\n",
    "\n",
    "        # Calculate the observed delta_r\n",
    "        observed_delta_r, _ = self.permute_data_and_difference_in_pearson_r(x_one, x_two, split_by, split_value, permute_columns=permute_columns)\n",
    "\n",
    "        if one_tail:\n",
    "            observed_delta_r = np.abs(observed_delta_r)\n",
    "            delta_rs = np.abs(delta_rs)\n",
    "\n",
    "        # Calculate p-value\n",
    "        if one_tail:\n",
    "            p_value = np.mean([delta_r >= observed_delta_r for delta_r in delta_rs])\n",
    "        else:\n",
    "            p_value = np.mean([delta_r <= observed_delta_r for delta_r in delta_rs])\n",
    "\n",
    "        # Generate the displot (KDE + Histogram) using Seaborn\n",
    "        sns.set_palette(color_palette)\n",
    "        current_palette = sns.color_palette(color_palette)\n",
    "        chosen_color = current_palette[4]\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        g = sns.displot(delta_rs, kde=True, bins=bins, label=\"Empirical $\\\\Delta r$ Distribution\", element=\"step\",color='blue', alpha=.6)\n",
    "        plt.axvline(x=observed_delta_r, color='red', linestyle='-', linewidth=1.5, label=f\"Observed $\\\\Delta r$\", alpha=0.6)\n",
    "        plt.title(f\"$\\\\Delta r$ = {observed_delta_r}, p = {p_value}\")\n",
    "        plt.xlabel(\"$\\\\Delta r$\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "        \n",
    "        fig = g.fig\n",
    "        \n",
    "        fig.savefig(f\"{out_dir}/hist_kde.png\", bbox_inches='tight')\n",
    "        fig.savefig(f\"{out_dir}/hist_kde.svg\", bbox_inches='tight')\n",
    "        print(f'Saved to {out_dir}/hist_kde.svg')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns = [col for col in data_df.columns if 'Occ' in col]\n",
    "print(filtered_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data To Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Variables\n",
    "dependent_variable = 'Percent_Cognitive_Improvement'\n",
    "independent_variable = 'Subiculum_Connectivity_T_Redone'\n",
    "split_by_var = 'Age' # This is the column which contains the values you are going to split the data by \n",
    "split_value_var = 65 # This is the value which will be used to determine how rows of the given column split the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels for Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Variables\n",
    "x_label = 'Standardized Subiculum Connectivity'\n",
    "y_label = 'Standardized Percent Improvement'\n",
    "legend_string_for_lower_split='under 65'\n",
    "legend_string_for_upper_split='Over 65'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_sctr = ScatterWithConfidence(data_df)\n",
    "delta_sctr_plot = delta_sctr.plot_with_analytic_ci_manual_pvalue(\n",
    "                                            x_one=independent_variable, x_two=dependent_variable,\n",
    "                                            split_by=split_by_var, split_value=split_value_var,\n",
    "                                            x_label=x_label, y_label=y_label,\n",
    "                                            upper_split_legend=legend_string_for_upper_split, lower_split_legend=legend_string_for_lower_split,\n",
    "                                            alpha=0.3, manual_p_value=None, permute_column=None,\n",
    "                                            save=True, out_dir=out_dir,\n",
    "                                            colour1='#FF0000', colour2='#0000FF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object from the new Seaborn-based class\n",
    "histogram_plotter_seaborn = DeltaCorrelation(data_df)\n",
    "\n",
    "# Generate the histogram using Seaborn\n",
    "fig = histogram_plotter_seaborn.plot_histogram_of_delta_r(x_one=independent_variable, x_two=dependent_variable, \n",
    "                                                    split_by=split_by_var, split_value=split_value_var, \n",
    "                                                    n_permutations=10000, \n",
    "                                                    permute_columns=[independent_variable, dependent_variable, split_by_var], \n",
    "                                                    bins=50,\n",
    "                                                    one_tail=True, \n",
    "                                                    color_palette='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlay Multiple Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_lines_of_best_fit_by_combinations(data_df, categorical_columns, x_var, y_var):\n",
    "    # Find unique values in each column\n",
    "    unique_values_per_column = [data_df[col].unique() for col in categorical_columns]\n",
    "\n",
    "    # Generate all possible combinations of these unique values across the columns\n",
    "    all_combinations = list(itertools.product(*unique_values_per_column))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set_palette('tab10')\n",
    "    # For each combination, create a mask and apply it to the DataFrame\n",
    "    for combination in all_combinations:\n",
    "        mask = pd.Series(True, index=data_df.index)\n",
    "        for col, val in zip(categorical_columns, combination):\n",
    "            mask &= (data_df[col] == val)\n",
    "\n",
    "        # Apply the mask to the DataFrame to get the subset\n",
    "        masked_df = data_df[mask]\n",
    "        \n",
    "        # Skip if the masked_df is empty\n",
    "        if masked_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Calculate the line of best fit\n",
    "        slope, intercept = np.polyfit(masked_df[x_var], masked_df[y_var], 1)\n",
    "        x_vals = np.linspace(masked_df[x_var].min(), masked_df[x_var].max(), 100)\n",
    "        y_vals = slope * x_vals + intercept\n",
    "\n",
    "        # Plot the line of best fit\n",
    "        plt.plot(x_vals, y_vals, label=f'{combination}', linestyle='-')\n",
    "\n",
    "    plt.xlabel(x_var)\n",
    "    plt.ylabel(y_var)\n",
    "    plt.title(f'Lines of Best Fit for {x_var} vs {y_var} by Category Combinations')\n",
    "    plt.legend(title='Category Combination', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines_of_best_fit_by_combinations(data_df,\n",
    "                                       categorical_columns=['Age_Group', 'City'], \n",
    "                                       x_var='Z_Scored_Subiculum_T_By_Origin_Group_', \n",
    "                                       y_var='Z_Scored_Percent_Cognitive_Improvement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix and Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_correlation_heatmap(dataframe):\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = dataframe.corr()\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(corr_matrix, annot=False, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={'shrink': .5})\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title('Correlation Heatmap')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sorted_columns = sorted(data_df.columns)\n",
    "data_df_sorted = data_df[sorted_columns]\n",
    "plot_correlation_heatmap(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def perform_cca(data_df, set1_columns, set2_columns):\n",
    "    \"\"\"\n",
    "    Perform Canonical Correlation Analysis (CCA) between two sets of columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        data_df (pd.DataFrame): The DataFrame containing the data.\n",
    "        set1_columns (list): List of column names for the first set of variables.\n",
    "        set2_columns (list): List of column names for the second set of variables.\n",
    "\n",
    "    Returns:\n",
    "        cca (CCA): The CCA model object.\n",
    "        cca_results (dict): A dictionary containing the CCA results.\n",
    "    \"\"\"\n",
    "    # Extract the specified sets of columns from the DataFrame\n",
    "    set1_data = data_df[set1_columns]\n",
    "    set2_data = data_df[set2_columns]\n",
    "\n",
    "    # Initialize the CCA model\n",
    "    cca = CCA(n_components=min(len(set1_columns), len(set2_columns)))\n",
    "\n",
    "    # Fit the CCA model to the data\n",
    "    cca.fit(set1_data, set2_data)\n",
    "\n",
    "    # Transform the data using CCA\n",
    "    set1_transformed, set2_transformed = cca.transform(set1_data, set2_data)\n",
    "\n",
    "    # Calculate canonical correlations from singular values\n",
    "    canonical_correlations = cca.singular_values_\n",
    "\n",
    "    # Store the CCA results in a dictionary\n",
    "    cca_results = {\n",
    "        'Canonical Correlations': canonical_correlations,\n",
    "        'Set 1 Transformed': set1_transformed,\n",
    "        'Set 2 Transformed': set2_transformed\n",
    "    }\n",
    "\n",
    "    return cca, cca_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define 2 lists\n",
    "- 1 is the first set to CCA\n",
    "- 2 is the second set to CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templist = []\n",
    "for val in data_df.columns:\n",
    "    if 'CSF' in val:\n",
    "        templist.insert(-1, val)\n",
    "print(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['frontal', 'temporal', 'parietal', 'occipital',\n",
    "       'cerebellum', 'Mesial_Temporal', 'ventricle']\n",
    "list2 = ['frontal_eh', 'temporal_eh', 'parietal_eh',\n",
    "       'occipital_eh', 'cerebellum_eh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "def perform_cca(data_df, set1_columns, set2_columns):\n",
    "    \"\"\"\n",
    "    Perform Canonical Correlation Analysis (CCA) between two sets of columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        data_df (pd.DataFrame): The DataFrame containing the data.\n",
    "        set1_columns (list): List of column names for the first set of variables.\n",
    "        set2_columns (list): List of column names for the second set of variables.\n",
    "\n",
    "    Returns:\n",
    "        cca (CCA): The CCA model object.\n",
    "        cca_results (dict): A dictionary containing the CCA results.\n",
    "    \"\"\"\n",
    "    # Extract the specified sets of columns from the DataFrame\n",
    "    set1_data = data_df[set1_columns]\n",
    "    set2_data = data_df[set2_columns]\n",
    "\n",
    "    # Initialize the CCA model\n",
    "    cca = CCA(n_components=min(len(set1_columns), len(set2_columns)))\n",
    "\n",
    "    # Fit the CCA model to the data\n",
    "    cca.fit(set1_data, set2_data)\n",
    "\n",
    "    # Calculate canonical correlations using the score method\n",
    "    canonical_correlations = cca.score(set1_data, set2_data)\n",
    "\n",
    "    # Store the CCA results in a dictionary\n",
    "    cca_results = {\n",
    "        'Canonical Correlations': canonical_correlations\n",
    "    }\n",
    "\n",
    "    return cca, cca_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca, cca_results = perform_cca(data_df, set1_columns=list1, set2_columns=list2)\n",
    "cca_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intraclass Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "from pingouin import intraclass_corr\n",
    "from calvin_utils.statistical_utils.distribution_statistics import BootstrappedDistributionStatistics\n",
    "from calvin_utils.statistical_utils.resampling_functions import Bootstrap\n",
    "\n",
    "def calculate_icc(df, col1, col2):\n",
    "    # Select only the specified columns and rename them for compatibility with pingouin\n",
    "    data = df[[col1, col2]].rename(columns={col1: 'rating1', col2: 'rating2'})\n",
    "\n",
    "    # Reshape data for pingouin's intraclass_corr function\n",
    "    df_melted = data.melt(value_vars=['rating1', 'rating2'], var_name='rater', value_name='rating')\n",
    "    df_melted['subject'] = df_melted.groupby('rater').cumcount()\n",
    "\n",
    "    # Calculate ICC\n",
    "    icc_result = intraclass_corr(data=df_melted, targets='subject', raters='rater', ratings='rating')\n",
    "    \n",
    "    # Return ICC(3,1)\n",
    "    return icc_result.set_index('Type').loc['ICC3', 'ICC']\n",
    "\n",
    "def bootstrap_icc(data, col1, col2, bootstrap_samples=2500):\n",
    "    # Perform the bootstrap resampling using the Bootstrap class\n",
    "    bootstrap = Bootstrap(data=data, func=calculate_icc, func_args={'col1': col1, 'col2': col2}, bootstrap_samples=bootstrap_samples)\n",
    "    bootstrap_results = bootstrap.bootstrap_function()\n",
    "\n",
    "    # Calculate the confidence intervals using the BootstrappedDistributionStatistics class\n",
    "    distribution_statistics = BootstrappedDistributionStatistics(bootstrap_results)\n",
    "    lower_bound, upper_bound = distribution_statistics.percentile_ci(alpha=0.05)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def plot_icc_forest(comparisons_dict, dataframe, bootstrap_samples=2500, full_legend_patches=False):\n",
    "    # figure = plt.figure(figsize=(4, len(comparisons_dict)*1.2))\n",
    "    figure = plt.figure(figsize=(4, 5))\n",
    "    \n",
    "    # Create a color palette with enough unique colors\n",
    "    colors = sns.color_palette(\"tab10\", len(comparisons_dict))\n",
    "    \n",
    "    # Create legend patches\n",
    "    legend_patches = []\n",
    "    \n",
    "    # Iterate through the dictionary and plot ICC for each comparison\n",
    "    for idx, (col1_name, col2_name) in enumerate(comparisons_dict.items()):\n",
    "        # Calculate ICC\n",
    "        icc_value = calculate_icc(dataframe, col1_name, col2_name)\n",
    "\n",
    "        # Bootstrap 95% confidence interval\n",
    "        ci_lower, ci_upper = bootstrap_icc(dataframe, col1_name, col2_name, bootstrap_samples=bootstrap_samples)\n",
    "\n",
    "        # Plot ICC with confidence interval\n",
    "        plt.errorbar(x=icc_value, y=idx, xerr=[[icc_value - ci_lower], [ci_upper - icc_value]], fmt='o', color=colors[idx], capsize=5)\n",
    "        \n",
    "        # Add legend patch\n",
    "        if full_legend_patches:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{col1_name} vs {col2_name}'))\n",
    "        else:\n",
    "            legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{col1_name}'))\n",
    "\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(-1, len(comparisons_dict))\n",
    "    plt.yticks([])\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlabel('ICC')\n",
    "    plt.title('Intraclass Correlation Coefficients (ICC) with 95% Confidence Intervals')\n",
    "    plt.grid(axis='x', linestyle='--')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(handles=legend_patches, frameon=False, loc=(0.05, 0.1))\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exptl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['City'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/mambaforge/base/envs/nimlab_py310/lib/python3.10/site-packages/scipy/stats/stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Variables\n",
    "y_variable = 'Z_Scored_Percent_Cognitive_Improvement'\n",
    "x_variable = 'Subiculum_Connectivity_T_Redone'\n",
    "\n",
    "# Filter data for 'young' age group\n",
    "young_data = data_df[data_df['Age_Group'] == 'old']\n",
    "\n",
    "# Function to perform bootstrap and calculate correlation differences and individual correlations\n",
    "def bootstrap_corr_diff(data, y_var, x_var, group_col, n_bootstraps=1000):\n",
    "    corr_diffs = []\n",
    "    all_correlations = {group: [] for group in data[group_col].unique()}\n",
    "    unique_groups = data[group_col].unique()\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_diffs = []\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            group_data = data[data[group_col] == group].sample(frac=1, replace=True)\n",
    "            corr, _ = pearsonr(group_data[x_var], group_data[y_var])\n",
    "            all_correlations[group].append(corr)\n",
    "            sample_diffs.append(corr)\n",
    "        \n",
    "        if len(sample_diffs) == 2:\n",
    "            corr_diffs.append(sample_diffs[0] - sample_diffs[1])\n",
    "    \n",
    "    return corr_diffs, all_correlations\n",
    "\n",
    "# Perform bootstrapping\n",
    "bootstrap_diffs, bootstrap_correlations = bootstrap_corr_diff(young_data, y_variable, x_variable, group_col='City')\n",
    "\n",
    "# Save the bootstrap differences to a .txt file\n",
    "np.savetxt('/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/3_cohort_delta_r/andy_similarity_test/bayes_old/bootstrap_diffs.txt', bootstrap_diffs)\n",
    "\n",
    "# Save the bootstrap correlations for each city to separate .txt files\n",
    "for city, correlations in bootstrap_correlations.items():\n",
    "    np.savetxt(f'/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/3_cohort_delta_r/andy_similarity_test/bayes_old/bootstrap_correlations_{city}.txt', correlations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
