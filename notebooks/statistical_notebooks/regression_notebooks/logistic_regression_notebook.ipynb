{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Any Kind of Logistic Regression (Binomial, Multinomial, etc.)\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: March 16, 2024\n",
    "\n",
    "Use this to run/test a statistical model on a spreadsheet.\n",
    "\n",
    "Notes:\n",
    "- To best use this notebook, you should be familar with GLM design and Contrast Matrix design. See this webpage to get started:\n",
    "[FSL's GLM page](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/results/ctrl_vs_dz/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/differential_diagnoses_train_jan6.csv'\n",
    "sheet = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "drop_list = ['DX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "column = 'Cohort'  # The column you'd like to evaluate\n",
    "condition = 'equal'  # The condition to check ('equal', 'above', 'below', 'not')\n",
    "value = 0 # The value to drop if found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['DX', 'SUBID'] # ['Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group', 'Z_Scored_Subiculum_T_By_Origin_Group_'] #['Age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Define Your Formula\n",
    "\n",
    "This is the formula relating outcome to predictors, and takes the form:\n",
    "- y = B0 + B1 + B2 + B3 + . . . BN\n",
    "\n",
    "It is defined using the columns of your dataframe instead of the variables above:\n",
    "- 'Apples_Picked ~ hours_worked + owns_apple_picking_machine'\n",
    "\n",
    "____\n",
    "**Normal Logistic**\n",
    "- Assesses the impact of multiple predictors on an outcome.\n",
    "- formula = 'Binary Outcome ~ Predictor1 + Predictor2'\n",
    "\n",
    "**Multiple Logistic**\n",
    "- Assesses the impact of predictor on an outcome.\n",
    "- formula = 'Ordinal Outcome ~ Predictor1 + Predictor2'\n",
    "\n",
    "____\n",
    "Use the printout below to design your formula. \n",
    "- Left of the \"~\" symbol is the thing to be predicted. \n",
    "- Right of the \"~\" symbol are the predictors. \n",
    "- \":\" indicates an interaction between two things. \n",
    "- \"*\" indicates and interactions AND it accounts for the simple effects too. \n",
    "- \"+\" indicates that you want to add another predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "formula = \"CN_1 ~ AD + SV + LBD + BV + PNFA + CBS + PSP + CN + Peak_Values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Visualize Your Design Matrix\n",
    "\n",
    "This is the explanatory variable half of your regression formula\n",
    "_______________________________________________________\n",
    "Create Design Matrix: Use the create_design_matrix method. You can provide a list of formula variables which correspond to column names in your dataframe.\n",
    "\n",
    "- design_matrix = palm.create_design_matrix(formula_vars=[\"var1\", \"var2\", \"var1*var2\"])\n",
    "- To include interaction terms, use * between variables, like \"var1*var2\".\n",
    "- By default, an intercept will be added unless you set intercept=False\n",
    "- **don't explicitly add the 'intercept' column. I'll do it for you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Define the design matrix\n",
    "outcome_matrix, design_matrix = cal_palm.define_design_matrix(formula, data_df)\n",
    "design_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check multicollinearity in design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Multico. Check\n",
    "from calvin_utils.statistical_utils.statistical_measurements import calculate_vif\n",
    "calculate_vif(design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Visualize Your Dependent Variable\n",
    "\n",
    "I have generated this for you based on the formula you provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# outcome_matrix = outcome_matrix.iloc[:, [0]]\n",
    "outcome_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CRITICAL IN MULTINOMIAL LOGISTIC REGRESSION**\n",
    "- A multinomial logistic reg. will set results RELATIVE TO A REFERENCE class. \n",
    "- The reference class is the first classification the multinomial encounters.\n",
    "- **Especially if you are running a multinomial logistic regression, set your reference class below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "reference = 'Diagnosis[Control]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Run the Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Results Are Displayed Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will run a binomial or a multinomial logit dependig on your outcome matrix. \n",
    "- A multinomial logit will display N-1 categories, where N is the number of potential classifications you have. This occurs because everything is set in reference to that class. \n",
    "- So, the reference will either be the first column in your outcomes_matrix, or you can manually set it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.logistic_regression import LogisticRegression\n",
    "logreg = LogisticRegression(outcome_matrix, design_matrix)\n",
    "results = logreg.run()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Receiver Operating Characteristic\n",
    "- The ROC considers clasisfications acoss ALL POSSIBLE PROBABILITIES, demonstrating what is ultiamtely accomplishable at the best possible threshold\n",
    "\n",
    "- First curve is ROC for classifcation of each class with respect to all other classes\n",
    "- Second Curve (Macro Average) is basically a meta-analytic ROC with equal weight per class.\n",
    "- Third Curve (Micro Average) is basically a meta-analytic ROC with weight proportional to class sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.classification_statistics import ComprehensiveMulticlassROC\n",
    "evaluator = ComprehensiveMulticlassROC(fitted_model=results, observation_df=outcome_matrix, normalization='pred', thresholds=None, out_dir=out_dir)\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADVANCED\n",
    "- code specific manual thresholds to intervene upon classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: relate integer (index) to class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# evaluator.relate_index_to_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: in a dictionary of the indices (corresponding to class), key in the lambda function to edit the probability. \n",
    "- Code from left to right, giving priority to each method. \n",
    "- Example:\n",
    "```\n",
    ">thresholds = {\n",
    ">            0: lambda probs: 0 if probs[0] > 0.5 else (1 if probs[0] > 0.25 else 2),  # Adjust class_0 predictions\n",
    ">            1: lambda probs: None,  # No threshold adjustment for class_1\n",
    ">            2: lambda probs: None   # No threshold adjustment for class_2\n",
    ">        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    0: lambda prob: 0,  # Always keep class 0\n",
    "    1: lambda prob: 1,  # Always keep class 1\n",
    "    2: lambda prob: 2 if prob[2] > 0.5 else (1 if prob[1] > 0.3 else 0)  # Conditional adjustment for class 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Check the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# from calvin_utils.statistical_utils.classification_statistics import ComprehensiveMulticlassROC\n",
    "# evaluator = ComprehensiveMulticlassROC(fitted_model=results, observation_df=outcome_matrix, normalization='pred', thresholds=thresholds, out_dir=out_dir)\n",
    "# evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: YOU MUST LOOCV AND VALIDATE IN OUT-OF-SAMPLE DATA.\n",
    "- add thresholds as an argument to any further calls to ComprehensiveMulticlassROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap the Micro Average AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from calvin_utils.statistical_utils.classification_statistics import bootstrap_auc\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend\n",
    "\n",
    "mean_auc, lower_ci, upper_ci = bootstrap_auc(outcome_matrix, design_matrix, n_iterations=1000)\n",
    "print(f'Mean AUC: {mean_auc}, 95% CI: ({lower_ci}, {upper_ci})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation Test Two AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "f1 = \"Diagnosis ~ CerebellumCSF + ParietalCSF + MTLCSF + OccipitalCSF + FrontalCSF + temp_ins_csf + SubcortexCSF\"\n",
    "f2 = \"Diagnosis ~ CerebellumGM + ParietalGM + MTLGM + OccipitalGM + FrontalGM + temp_ins_gm + SubcortexGM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend\n",
    "from calvin_utils.statistical_utils.classification_statistics import permute_auc_difference\n",
    "obs_diff, lower_ci, upper_ci, p_value = permute_auc_difference(data_df, formula1=f1, \n",
    "                                                                  formula2=f2,\n",
    "                                                                  cal_palm=cal_palm, n_iterations=1000)\n",
    "print(f'Observde AUC Difference: {obs_diff}, 95% CI: ({lower_ci}, {upper_ci}), p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Visualize the Regression as a Forest Plot\n",
    "- This will probably look poor if you ran a regression without standardizing your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import MultinomialForestPlot\n",
    "\n",
    "multinomial_forest = MultinomialForestPlot(model=results, sig_digits=2, out_dir='/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/results/ctrl_vs_dz/train/forest_plots', table=False)\n",
    "multinomial_forest.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Generate Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import PartialDependencePlot\n",
    "pdp = PartialDependencePlot(formula=formula, data_df=data_df, model=results, design_matrix=design_matrix, outcomes_df=outcome_matrix, data_range=None, out_dir='/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/results/ctrl_vs_dz/train/forest_plots', marginal_method='mean', debug=False)\n",
    "pdp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Visualize the Partial Regression Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.statistical_measurements import PartialRegressionPlot\n",
    "partial_plot = PartialRegressionPlot(model=results, design_matrix=design_matrix, out_dir=out_dir, palette=None)\n",
    "partial_plot = partial_plot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from calvin_utils.statistical_utils.logistic_regression import LogisticRegression\n",
    "from calvin_utils.statistical_utils.classification_statistics import ComprehensiveMulticlassROC\n",
    "y_true, y_pred, test_prob = LogisticRegression.run_loocv(outcome_matrix, design_matrix)\n",
    "loocv_evaluator = ComprehensiveMulticlassROC(fitted_model=None, predictions_df=pd.DataFrame(test_prob, columns=outcome_matrix.columns), observation_df=outcome_matrix, normalization='pred', thresholds=None, out_dir=out_dir)\n",
    "loocv_evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Predict Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "new_csv_path='/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/differential_diagnoses_test_jan6.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - Get New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# # Instantiate the PalmPrepararation class\n",
    "new_palm = CalvinStatsmodelsPalm(input_csv_path=new_csv_path, output_dir='/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/results/test', sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "other_df = new_palm.read_and_display_data()\n",
    "other_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# column_renaming_map = {\n",
    "#                        'Mesial_Temporal': 'mesial_temporal_eh', \n",
    "#                        'parietal': 'parietal_eh', \n",
    "#                        'frontal': 'frontal_eh',\n",
    "#                        'temporal': 'temporal_eh', \n",
    "#                        'ventricle':'ventricle_eh',\n",
    "#                        'occipital': 'occipital_eh'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# other_df.rename(columns=column_renaming_map, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Define the design matrix\n",
    "# subset_df = other_df.groupby('DIAGNOSIS_BL').apply(lambda x: x.sample(min(len(x), 200))).reset_index(drop=True)\n",
    "import pandas as pd\n",
    "other_outcome_matrix, other_design_matrix = cal_palm.define_design_matrix(formula, other_df)\n",
    "\n",
    "# Ensure both matrices have the same columns\n",
    "if len(other_outcome_matrix.columns) != len(outcome_matrix.columns):\n",
    "    # Create a zero-filled DataFrame with the same columns as outcome_matrix\n",
    "    zero_df = pd.DataFrame(0, index=other_outcome_matrix.index, columns=outcome_matrix.columns)\n",
    "    \n",
    "    # Fill zero_df with values from other_outcome_matrix where columns exist\n",
    "    common_columns = other_outcome_matrix.columns.intersection(outcome_matrix.columns)\n",
    "    zero_df.loc[:, common_columns] = other_outcome_matrix.loc[:, common_columns]\n",
    "    \n",
    "    other_outcome_matrix = zero_df\n",
    "\n",
    "other_design_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds = {\n",
    "#     0: lambda prob: 0 if prob < 0.33 else 1,\n",
    "#     1: lambda prob: 1 if prob > 0.33 else 0\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.classification_statistics import ComprehensiveMulticlassROC\n",
    "loocv_evaluator = ComprehensiveMulticlassROC(fitted_model=None, predictions_df=results.predict(other_design_matrix), observation_df=other_outcome_matrix, normalization='true', thresholds=None, out_dir='/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/results/ctrl_vs_dz/test')\n",
    "loocv_evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get One Vs. All Confidence Intervals on AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, bootstrap = ComprehensiveMulticlassROC.bootstrap_ovr_auroc(raw_observations=loocv_evaluator.raw_observations, raw_predictions=loocv_evaluator.raw_predictions, outcome_matrix_cols=loocv_evaluator.outcome_matrix.columns)\n",
    "ComprehensiveMulticlassROC.plot_ovr_auc_with_ci(df, out_dir='/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/results/ctrl_vs_dz/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Confidence Intervals on Sensitivity, Specificity, NPV, PPV, and Accuracy for Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_youden(\n",
    "    raw_observations: np.ndarray,\n",
    "    raw_predictions: np.ndarray,\n",
    "    outcome_matrix_cols\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates Youden's J for each class, and bootstraps Sensitivity, Specificity, NPV, PPV, and Accuracy\n",
    "    at the corresponding cut point. Returns a summary DataFrame for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_observations : np.ndarray\n",
    "        Ground truth one-hot encoded array of shape (n_samples, n_classes).\n",
    "    raw_predictions : np.ndarray\n",
    "        Predicted probabilities for each class of shape (n_samples, n_classes).\n",
    "    outcome_matrix_cols : list-like\n",
    "        The column (class) labels, typically from `outcome_matrix.columns`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics_dfs : dict\n",
    "        two dictionaries where keys are class names are keys. values for first are youden's j, values for second are threhsold for each youdens j\n",
    "    \"\"\"\n",
    "    n_classes = raw_observations.shape[1]\n",
    "\n",
    "    youden_dict = {}\n",
    "    threshold_dict = {}\n",
    "    for i in range(n_classes):\n",
    "        class_name = outcome_matrix_cols[i]\n",
    "        y_true = raw_observations[:, i]\n",
    "        y_score = raw_predictions[:, i]\n",
    "\n",
    "        # Calculate the optimal threshold using Youden's J\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "        youden_j = tpr - fpr\n",
    "        optimal_idx = np.argmax(youden_j)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # Store the summary DataFrame for the current class\n",
    "        youden_dict[class_name] = youden_j[optimal_idx]\n",
    "        threshold_dict[class_name] = optimal_threshold\n",
    "    return youden_dict, threshold_dict\n",
    "\n",
    "def calculate_metrics_at_threshold(\n",
    "    raw_observations: np.ndarray,\n",
    "    raw_predictions: np.ndarray,\n",
    "    outcome_matrix_cols,\n",
    "    threshold_dict,\n",
    "    n_bootstraps: int = 1000,\n",
    "    random_state: int = None,\n",
    "    ci_alpha: float = 0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates Youden's J for each class, and bootstraps Sensitivity, Specificity, NPV, PPV, and Accuracy\n",
    "    at the corresponding cut point. Returns a summary DataFrame for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_observations : np.ndarray\n",
    "        Ground truth one-hot encoded array of shape (n_samples, n_classes).\n",
    "    raw_predictions : np.ndarray\n",
    "        Predicted probabilities for each class of shape (n_samples, n_classes).\n",
    "    outcome_matrix_cols : list-like\n",
    "        The column (class) labels, typically from `outcome_matrix.columns`.\n",
    "    threshold_dict : dict\n",
    "        the dictionary where keys correspond to classes and values correspond to thresholds to evaluate \n",
    "    n_bootstraps : int\n",
    "        Number of bootstrap iterations.\n",
    "    random_state : int\n",
    "        Controls reproducibility of the random sampling.\n",
    "    ci_alpha : float\n",
    "        Confidence interval coverage (0 < ci_alpha < 1). 0.95 -> 95% CI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics_dfs : dict\n",
    "        A dictionary where keys are class names and values are DataFrames with rows\n",
    "        as metrics (Sensitivity, Specificity, NPV, PPV, Accuracy) and columns as\n",
    "        mean, lower bound, and upper bound.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    n_classes = raw_observations.shape[1]\n",
    "\n",
    "    metrics_dfs = {}\n",
    "    for i in range(n_classes):\n",
    "        class_name = outcome_matrix_cols[i]\n",
    "        y_true = raw_observations[:, i]\n",
    "        y_score = raw_predictions[:, i]\n",
    "        optimal_threshold = threshold_dict[class_name]\n",
    "\n",
    "        # Call a helper function to calculate bootstrapped metrics\n",
    "        metrics_summary = bootstrap_metrics_at_cutpoint(y_true, y_score, optimal_threshold, n_bootstraps, rng, ci_alpha)\n",
    "\n",
    "        # Store the summary DataFrame for the current class\n",
    "        metrics_dfs[class_name] = metrics_summary\n",
    "    return metrics_dfs\n",
    "\n",
    "def calculate_youden_and_metrics(\n",
    "    raw_observations: np.ndarray,\n",
    "    raw_predictions: np.ndarray,\n",
    "    outcome_matrix_cols,\n",
    "    n_bootstraps: int = 1000,\n",
    "    random_state: int = None,\n",
    "    ci_alpha: float = 0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates Youden's J for each class, and bootstraps Sensitivity, Specificity, NPV, PPV, and Accuracy\n",
    "    at the corresponding cut point. Returns a summary DataFrame for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_observations : np.ndarray\n",
    "        Ground truth one-hot encoded array of shape (n_samples, n_classes).\n",
    "    raw_predictions : np.ndarray\n",
    "        Predicted probabilities for each class of shape (n_samples, n_classes).\n",
    "    outcome_matrix_cols : list-like\n",
    "        The column (class) labels, typically from `outcome_matrix.columns`.\n",
    "    n_bootstraps : int\n",
    "        Number of bootstrap iterations.\n",
    "    random_state : int\n",
    "        Controls reproducibility of the random sampling.\n",
    "    ci_alpha : float\n",
    "        Confidence interval coverage (0 < ci_alpha < 1). 0.95 -> 95% CI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics_dfs : dict\n",
    "        A dictionary where keys are class names and values are DataFrames with rows\n",
    "        as metrics (Sensitivity, Specificity, NPV, PPV, Accuracy) and columns as\n",
    "        mean, lower bound, and upper bound.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    n_classes = raw_observations.shape[1]\n",
    "\n",
    "    metrics_dfs = {}\n",
    "    print(\"--Optimal Threshold--\")\n",
    "    for i in range(n_classes):\n",
    "        class_name = outcome_matrix_cols[i]\n",
    "        y_true = raw_observations[:, i]\n",
    "        y_score = raw_predictions[:, i]\n",
    "\n",
    "        # Calculate the optimal threshold using Youden's J\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "        youden_j = tpr - fpr\n",
    "        optimal_idx = np.argmax(youden_j)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # Call a helper function to calculate bootstrapped metrics\n",
    "        metrics_summary = bootstrap_metrics_at_cutpoint(\n",
    "            y_true, y_score, optimal_threshold, n_bootstraps, rng, ci_alpha\n",
    "        )\n",
    "        print(f\"{class_name}: {optimal_threshold}\")\n",
    "\n",
    "        # Store the summary DataFrame for the current class\n",
    "        metrics_dfs[class_name] = metrics_summary\n",
    "    return metrics_dfs\n",
    "\n",
    "\n",
    "def bootstrap_metrics_at_cutpoint(\n",
    "    y_true: np.ndarray,\n",
    "    y_score: np.ndarray,\n",
    "    cutpoint: float,\n",
    "    n_bootstraps: int,\n",
    "    rng,\n",
    "    ci_alpha: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Bootstraps Sensitivity, Specificity, NPV, PPV, and Accuracy at a given cut point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        Ground truth binary labels.\n",
    "    y_score : np.ndarray\n",
    "        Predicted probabilities for the positive class.\n",
    "    cutpoint : float\n",
    "        The threshold at which to calculate the metrics.\n",
    "    n_bootstraps : int\n",
    "        Number of bootstrap iterations.\n",
    "    rng : np.random.Generator\n",
    "        Random number generator for reproducibility.\n",
    "    ci_alpha : float\n",
    "        Confidence interval coverage (0 < ci_alpha < 1). 0.95 -> 95% CI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics_summary_df : pd.DataFrame\n",
    "        DataFrame with rows as metrics (Sensitivity, Specificity, NPV, PPV, Accuracy)\n",
    "        and columns as mean, lower bound, and upper bound.\n",
    "    \"\"\"\n",
    "    metrics = {\"Sensitivity\": [], \"Specificity\": [], \"NPV\": [], \"PPV\": [], \"Accuracy\": []}\n",
    "\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Sample with replacement\n",
    "        idx = rng.integers(0, n_samples, n_samples)\n",
    "        y_true_boot = y_true[idx]\n",
    "        y_score_boot = y_score[idx]\n",
    "\n",
    "        # Convert probabilities to binary predictions using the cutpoint\n",
    "        y_pred_boot = (y_score_boot >= cutpoint).astype(int)\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_boot, y_pred_boot).ravel()\n",
    "\n",
    "        # Calculate metrics\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan  # Sensitivity\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan  # Specificity\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan  # Positive Predictive Value\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan  # Negative Predictive Value\n",
    "        acc = accuracy_score(y_true_boot, y_pred_boot)     # Accuracy\n",
    "\n",
    "        metrics[\"Sensitivity\"].append(sens)\n",
    "        metrics[\"Specificity\"].append(spec)\n",
    "        metrics[\"NPV\"].append(npv)\n",
    "        metrics[\"PPV\"].append(ppv)\n",
    "        metrics[\"Accuracy\"].append(acc)\n",
    "\n",
    "    # Calculate mean, lower bound, and upper bound for each metric\n",
    "    rows = []\n",
    "    alpha_lower = (1.0 - ci_alpha) / 2.0\n",
    "    alpha_upper = 1.0 - alpha_lower\n",
    "\n",
    "    for metric, values in metrics.items():\n",
    "        values_clean = [v for v in values if not np.isnan(v)]\n",
    "        if len(values_clean) == 0:\n",
    "            rows.append([metric, np.nan, np.nan, np.nan])\n",
    "        else:\n",
    "            mean_val = np.mean(values_clean)\n",
    "            lower_bound = np.quantile(values_clean, alpha_lower)\n",
    "            upper_bound = np.quantile(values_clean, alpha_upper)\n",
    "            rows.append([metric, mean_val, lower_bound, upper_bound])\n",
    "\n",
    "    metrics_summary_df = pd.DataFrame(\n",
    "        rows, columns=[\"Metric\", \"Mean\", f\"{alpha_lower:.3g}%\", f\"{alpha_upper:.3g}%\"]\n",
    "    ).set_index(\"Metric\")\n",
    "\n",
    "    return metrics_summary_df\n",
    "\n",
    "def save_dfs(dfs:dict, out_dir=None):\n",
    "    for name, df in dfs.items():\n",
    "        if out_dir is not None:\n",
    "            df.to_csv(out_dir + '/' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = calculate_youden_and_metrics(raw_observations=loocv_evaluator.raw_observations, raw_predictions=loocv_evaluator.raw_predictions, outcome_matrix_cols=loocv_evaluator.outcome_matrix.columns)\n",
    "save_dfs(dfs, out_dir='/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_5/results/test/metrics')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Overall Micro Average AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv_evaluator.get_micro_auc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###----- Functionally Programmed Functions for Evaluation Using Above Classes -----##\n",
    "'''\n",
    "Hanging imports to facilitate easy transplant of code.\n",
    "'''\n",
    "import numpy as np\n",
    "from calvin_utils.statistical_utils.classification_statistics import ComprehensiveMulticlassROC\n",
    "from calvin_utils.statistical_utils.logistic_regression import LogisticRegression\n",
    "import os\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from tqdm import tqdm\n",
    "\n",
    "def resample_df(data_df):\n",
    "    n_samples = data_df.shape[0]\n",
    "    # Shuffle the indices\n",
    "    shuffled_indices = np.random.permutation(n_samples)\n",
    "    # Reorder the DataFrame based on the shuffled indices but keep the original index\n",
    "    shuffled_df = data_df.iloc[shuffled_indices].reset_index(drop=True).set_index(data_df.index)\n",
    "    return shuffled_df\n",
    "\n",
    "def permute_auc_difference(data_df, formula1, formula2, cal_palm, n_iterations=1000):\n",
    "    auc_diffs = []\n",
    "    for i in tqdm(range(n_iterations)):\n",
    "        try:\n",
    "            with open(os.devnull, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull):\n",
    "                # Define design matrices and outcome matrices for both formulas\n",
    "                outcome_matrix, design_matrix1 = cal_palm.define_design_matrix(formula1, data_df)\n",
    "                _, design_matrix2 = cal_palm.define_design_matrix(formula2, data_df)\n",
    "                \n",
    "                # Permute the outcomes\n",
    "                if i == 0:\n",
    "                    resampled_df = outcome_matrix\n",
    "                else:\n",
    "                    resampled_df = resample_df(outcome_matrix)\n",
    "\n",
    "                # Fit the logistic regression model for the first formula\n",
    "                logreg1 = LogisticRegression(resampled_df, design_matrix1)\n",
    "                results1 = logreg1.run()\n",
    "\n",
    "                # Fit the logistic regression model for the second formula\n",
    "                logreg2 = LogisticRegression(resampled_df, design_matrix2)\n",
    "                results2 = logreg2.run()\n",
    "\n",
    "                # Evaluate the models\n",
    "                evaluator1 = ComprehensiveMulticlassROC(fitted_model=results1, observation_df=resampled_df, normalization='true', thresholds=None, out_dir=None)\n",
    "                micro_auc1 = evaluator1.get_micro_auc()\n",
    "\n",
    "                evaluator2 = ComprehensiveMulticlassROC(fitted_model=results2, observation_df=resampled_df, normalization='true', thresholds=None, out_dir=None)\n",
    "                micro_auc2 = evaluator2.get_micro_auc()\n",
    "\n",
    "                # Store the difference in micro-average AUCs\n",
    "                if i == 0:\n",
    "                    obs_diff = micro_auc1 - micro_auc2\n",
    "                    print(f\"F1: {micro_auc1} | F2: {micro_auc2}\")\n",
    "                else:\n",
    "                    auc_diffs.append(micro_auc1 - micro_auc2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    # Calculate p-value based on the distribution of differences\n",
    "    auc_diffs = np.array(auc_diffs)\n",
    "    p_value = np.mean(auc_diffs >= obs_diff)\n",
    "\n",
    "    # Calculate confidence intervals for the difference\n",
    "    lower_ci = np.percentile(auc_diffs, 2.5)\n",
    "    upper_ci = np.percentile(auc_diffs, 97.5)\n",
    "    \n",
    "    return obs_diff, lower_ci, upper_ci, p_value\n",
    "\n",
    "def bootstrap_auc(outcome_matrix, design_matrix, n_iterations=1000, model=None):\n",
    "    auc_scores = []\n",
    "    n_samples = outcome_matrix.shape[0]\n",
    "    \n",
    "    for i in tqdm(range(n_iterations)):\n",
    "        # Suppress both stdout and stderr\n",
    "        try:\n",
    "            with open(os.devnull, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull):\n",
    "                # Generate a bootstrap sample\n",
    "                resample_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                outcome_matrix_resampled = outcome_matrix.iloc[resample_idx]\n",
    "                design_matrix_resampled = design_matrix.iloc[resample_idx]\n",
    "\n",
    "                # Fit the logistic regression model\n",
    "                if model is None:\n",
    "                    logreg = LogisticRegression(outcome_matrix_resampled, design_matrix_resampled)\n",
    "                    results = logreg.run()\n",
    "                    test = ComprehensiveMulticlassROC(fitted_model=results, observation_df=outcome_matrix_resampled, normalization='true', thresholds=None, out_dir=None)\n",
    "                else:\n",
    "                    results = model.predict(design_matrix_resampled)\n",
    "                    test = ComprehensiveMulticlassROC(fitted_model=None, predictions_df=results, observation_df=outcome_matrix_resampled, normalization='true', thresholds=None, out_dir=None)\n",
    "                # Evaluate the model\n",
    "                micro_auc = test.get_micro_auc()\n",
    "                auc_scores.append(micro_auc)\n",
    "        except:\n",
    "            continue\n",
    "    # Calculate confidence intervals\n",
    "    lower_ci = np.percentile(auc_scores, 2.5)\n",
    "    upper_ci = np.percentile(auc_scores, 97.5)    \n",
    "    return np.mean(auc_scores), lower_ci, upper_ci, auc_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap the Micro Average AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# from calvin_utils.statistical_utils.classification_statistics import bootstrap_auc\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend\n",
    "mean_auc, lower_ci, upper_ci, auc_dist = bootstrap_auc(other_outcome_matrix, other_design_matrix, n_iterations=1000, model=results)\n",
    "print(f'Mean AUC: {mean_auc}, 95% CI: ({lower_ci}, {upper_ci})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "auc_df = pd.DataFrame({'val':auc_dist})\n",
    "auc_df.to_csv('/path/to/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_df = pd.read_csv('/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/atrophy_seeds_2023/Figures/diagnostic_ability/50fit_150pred/auc_dist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "def compare_bootstrapped_means(distribution1, distribution2):\n",
    "    \"\"\"\n",
    "    Compares two bootstrapped distributions using a two-sample t-test and Mann-Whitney U test.\n",
    "    \n",
    "    Parameters:\n",
    "    - distribution1 (list or array): Bootstrapped distribution of group 1.\n",
    "    - distribution2 (list or array): Bootstrapped distribution of group 2.\n",
    "    \n",
    "    Returns:\n",
    "    - t_stat (float): t-statistic from the two-sample t-test.\n",
    "    - t_p_value (float): p-value from the two-sample t-test.\n",
    "    - u_stat (float): U-statistic from the Mann-Whitney U test.\n",
    "    - u_p_value (float): p-value from the Mann-Whitney U test.\n",
    "    \"\"\"\n",
    "    # Perform a two-sample t-test to compare means\n",
    "    t_stat, t_p_value = ttest_ind(distribution1, distribution2)\n",
    "    \n",
    "    # Perform a Mann-Whitney U test to compare distributions\n",
    "    u_stat, u_p_value = mannwhitneyu(distribution1, distribution2)\n",
    "\n",
    "    return t_stat, t_p_value, u_stat, u_p_value\n",
    "\n",
    "# Compare the distributions\n",
    "t_stat, t_p_value, u_stat, u_p_value = compare_bootstrapped_means(auc_df['CSF AUCs'], auc_df['CTh AUCs'])\n",
    "\n",
    "print(f\"Two-sample t-test: t-stat = {t_stat:.4f}, p-value = {t_p_value:.4f}\")\n",
    "print(f\"Mann-Whitney U test: U-stat = {u_stat:.4f}, p-value = {u_p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Bootstraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_custom_bootstrap_auc(df):\n",
    "    \"\"\"\n",
    "    Plots the mean and error bars of AUCs for each column in the DataFrame using Seaborn and Matplotlib,\n",
    "    styled similarly to the reference plot.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing bootstrapped AUC distributions. \n",
    "                         Each column represents a different group.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store plotting data\n",
    "    means = []\n",
    "    lower_errors = []\n",
    "    upper_errors = []\n",
    "    y_labels = []\n",
    "\n",
    "    # Calculate mean and confidence intervals for each column in the DataFrame, ignoring NaNs\n",
    "    for column in df.columns:\n",
    "        bootstrapped_values = df[column].dropna()  # Remove NaN values\n",
    "        if len(bootstrapped_values) > 0:\n",
    "            mean_value = np.mean(bootstrapped_values)\n",
    "            lower_ci = np.percentile(bootstrapped_values, 2.5)\n",
    "            upper_ci = np.percentile(bootstrapped_values, 97.5)\n",
    "            \n",
    "            means.append(mean_value)\n",
    "            lower_errors.append(mean_value - lower_ci)\n",
    "            upper_errors.append(upper_ci - mean_value)\n",
    "            y_labels.append(column)\n",
    "\n",
    "    # Create a horizontal scatter plot with error bars\n",
    "    plt.figure(figsize=(6, len(means) * 1.2))  # Adjust height based on number of groups\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Assign colors from a color palette\n",
    "    palette = sns.color_palette(\"tab10\", len(means))\n",
    "\n",
    "    # Plot the means as points with error bars\n",
    "    for i, (mean, lower_err, upper_err, label) in enumerate(zip(means, lower_errors, upper_errors, y_labels)):\n",
    "        plt.errorbar(x=mean, y=i, xerr=[[lower_err], [upper_err]], fmt='o', color=palette[i], capsize=5, markersize=10)\n",
    "    \n",
    "    # Customize y-ticks to match labels\n",
    "    plt.yticks(range(len(y_labels)), y_labels)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('AUC')\n",
    "    plt.ylabel('CAD Product')\n",
    "    plt.title('Mean AUC with 95% CI for CAD Products')\n",
    "\n",
    "    # Customize the grid and layout\n",
    "    plt.grid(False)\n",
    "    plt.xlim(0.4, 1.0)  # Set x-axis limits for AUC\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(out_dir + '/95ci.svg')\n",
    "\n",
    "plot_custom_bootstrap_auc(auc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation Test Two AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "other_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "f1 = \"Diagnosis ~ CSF_Frontal + CSF_Temporal + CSF_Parietal + CSF_Occipital + CSF_MTL + CSF_Cerebellum + CSF_Subcortex\"\n",
    "f2 = \"Diagnosis ~ GM_Frontal + GM_Temporal + GM_Parietal + GM_Occipital + GM_MTL + GM_Cerebellum + GM_Subcortex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# from calvin_utils.statistical_utils.classification_statistics import permute_auc_difference\n",
    "obs_diff, lower_ci, upper_ci, p_value = permute_auc_difference(other_df, formula1=f1, \n",
    "                                                                  formula2=f2,\n",
    "                                                                  cal_palm=cal_palm, n_iterations=1000)\n",
    "print(f'Observed AUC Difference: {obs_diff}, 95% CI: ({lower_ci}, {upper_ci}), p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
