{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a Model's Classifications\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: July 6, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/KiTH_Solutions/Research/Clinical Trial/study_metadata/all_performances.xlsx'\n",
    "sheet = 'roca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/roca/figures/cognitive_classifier'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.fillna(value=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Age', 'Z_Scored_Percent_Cognitive_Improvement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Cohort'  # The column you'd like to evaluate\n",
    "condition = 'equal'  # The condition to check ('equal', 'above', 'below')\n",
    "value = 3  # The value to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "data_df.rename(columns={\n",
    "    'cube_prediction': 'Cube Prediction',\n",
    "    'infinity_prediction': 'Infinity Prediction',\n",
    "    'clock_prediction': 'Clock Prediction',\n",
    "    'cube_actual': 'Cube Actual',\n",
    "    'infinity_actual': 'Infinity Actual',\n",
    "    'clock_actual': 'Clock Actual',\n",
    "    'Cognitive_Status_Code': 'Cognitively Intact',\n",
    "    'Cognitive_Status': 'Cognitive Status',\n",
    "    'Question_16': 'RoCA Score'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Evaluate Already Administered Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Use the Mapping Dictionary\n",
    "- The mapping dictionary is a crucial component of the BinaryDataMetricsPlotter class. It defines the relationships between columns in your dataframe that you want to compare. Here's how to use it:\n",
    "\n",
    "- Define the Mapping: Create a dictionary where each key-value pair represents a mapping of the columns of ground-truth classification to the column of the experimental classificaiton. \n",
    "\n",
    ">mapping_dict = {\n",
    ">>    'gold_standard_1': 'classification_1',\n",
    ">>\n",
    ">>    'gold_standard_2': 'classification_2',\n",
    ">>\n",
    ">>    add more as needed\n",
    ">>\n",
    ">}\n",
    "\n",
    "**The mapping dict must have gold standards as keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'Cognitively Impaired':'Intact'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "**Visualize Classification Metrics with Barplots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sensitivity (Recall): Measures the proportion of actual positives correctly identified. Identical to recall, it assesses how well the model identifies true positives among the positive cases.\n",
    "\n",
    "- Specificity: Indicates the proportion of actual negatives correctly identified as such. It evaluates the model's ability to recognize true negatives among negative cases.\n",
    "\n",
    "- Precision: Refers to the proportion of positive identifications that are actually correct. It focuses on the accuracy of the positive predictions made by the model.\n",
    "\n",
    "- Recall: Measures the proportion of actual positives that are correctly identified. This is identical to sensitivity, emphasizing the model's accuracy in detecting positive cases.\n",
    "\n",
    "- Accuracy: The ratio of correctly predicted observations to the total observations. It provides an overall measure of the model's performance.\n",
    "\n",
    "- F1 Score: The harmonic mean of precision and recall. This metric is particularly useful when the balance between precision and recall is important.\n",
    "\n",
    "- Positive Predictive Value (PPV): Similar to precision, PPV is the proportion of positive test results that are true positives. It indicates the likelihood that a positive test accurately reflects the underlying condition.\n",
    "\n",
    "- Negative Predictive Value (NPV): The proportion of negative test results that are true negatives. NPV measures the likelihood that a negative test result accurately indicates the absence of the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Measurement', 'Cohort', 'Educational_Status_Coded',\n",
       "       'Educational_Status', 'Employment_Status', 'Randomization_Group_Coded',\n",
       "       'Age', 'Sex_Coded', 'Sex', 'Ethnicity_Coded', 'Ethnicity',\n",
       "       'Cognitive_Exam', 'Cognitive_Status', 'Cognitive_Status_Code',\n",
       "       'Question_16', 'Convolutional_Neural_Network_Equivalent',\n",
       "       'Weighted_Sum', 'Cube_Predicted', 'Infinity_Predicted',\n",
       "       'Clock_Predicted', 'Cube_Actual', 'Infinity_Actual', 'Clock_Actual',\n",
       "       'cube_prediction', 'infinity_prediction', 'clock_prediction',\n",
       "       'cube_actual', 'infinity_actual', 'clock_actual', 'Classification'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "either both or neither of x and y should be given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCube_Actual\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCube_Actual\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfinity_Actual\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfinity_Actual\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClock_Actual\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClock_Actual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCorrect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: either both or neither of x and y should be given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_df['Cube_Actual'] = np.where(data_df['Cube_Actual']=='Correct', 1, 0)\n",
    "data_df['Infinity_Actual'] = np.where(data_df['Infinity_Actual']=='Correct', 1, 0)\n",
    "data_df['Clock_Actual'] = np.where(data_df['Clock_Actual']=='Correct', 1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Measurement</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Educational_Status_Coded</th>\n",
       "      <th>Educational_Status</th>\n",
       "      <th>Employment_Status</th>\n",
       "      <th>Randomization_Group_Coded</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex_Coded</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity_Coded</th>\n",
       "      <th>...</th>\n",
       "      <th>Cube_Actual</th>\n",
       "      <th>Infinity_Actual</th>\n",
       "      <th>Clock_Actual</th>\n",
       "      <th>cube_prediction</th>\n",
       "      <th>infinity_prediction</th>\n",
       "      <th>clock_prediction</th>\n",
       "      <th>cube_actual</th>\n",
       "      <th>infinity_actual</th>\n",
       "      <th>clock_actual</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient_15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>patient_17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patient_19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient_25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>patient_26</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Post-Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>patient_40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>patient_42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>patient_47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>patient_108</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>patient_109</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Post-Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>patient_106</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>patient_104</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>patient_111</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Post-Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>patient_107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>patient_115</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>patient_122</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>patient_126</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>patient_130</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>patient_131</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Post-Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>patient_11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Less than Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>patient_24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>patient_30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>patient_33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>patient_38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>patient_110</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Post-Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>patient_103</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>patient_112</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>patient_102</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>patient_103</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>patient_114</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>patient_119</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Post-Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>patient_121</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>patient_123</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>patient_128</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Less than Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>patient_132</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>patient_1020</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Post-Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>patient_1021</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Post-Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>patient_1003</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>patient_1004</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>patient_1005</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>patient_1006</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>patient_1007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>patient_1008</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>patient_1009</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Employed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>patient_1010</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>patient_1011</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Correct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Measurement  Cohort  Educational_Status_Coded   Educational_Status  \\\n",
       "0     patient_15       1                         1            Secondary   \n",
       "1     patient_17       1                         1            Secondary   \n",
       "2     patient_19       1                         1            Secondary   \n",
       "3     patient_25       1                         1            Secondary   \n",
       "4     patient_26       1                         2       Post-Secondary   \n",
       "5     patient_40       1                         1            Secondary   \n",
       "6     patient_42       1                         1            Secondary   \n",
       "7     patient_47       1                         1            Secondary   \n",
       "8    patient_108       2                         1            Secondary   \n",
       "9    patient_109       2                         3       Post-Secondary   \n",
       "10   patient_106       2                         1            Secondary   \n",
       "11   patient_104       2                         1            Secondary   \n",
       "12   patient_111       2                         2       Post-Secondary   \n",
       "13   patient_107       2                         1            Secondary   \n",
       "14   patient_115       2                         1            Secondary   \n",
       "15   patient_122       4                         1            Secondary   \n",
       "16   patient_126       4                         1            Secondary   \n",
       "17   patient_130       4                         1            Secondary   \n",
       "18   patient_131       4                         2       Post-Secondary   \n",
       "19    patient_11       1                         0  Less than Secondary   \n",
       "20    patient_24       1                         1            Secondary   \n",
       "21    patient_30       1                         1            Secondary   \n",
       "22    patient_33       1                         1            Secondary   \n",
       "23    patient_38       1                         1            Secondary   \n",
       "24   patient_110       2                         2       Post-Secondary   \n",
       "25   patient_103       2                         1            Secondary   \n",
       "26   patient_112       2                         1            Secondary   \n",
       "27   patient_102       2                         1            Secondary   \n",
       "28   patient_103       2                         1            Secondary   \n",
       "29   patient_114       2                         1            Secondary   \n",
       "30   patient_119       4                         2       Post-Secondary   \n",
       "31   patient_121       4                         1            Secondary   \n",
       "32   patient_123       4                         1            Secondary   \n",
       "33   patient_128       4                         0  Less than Secondary   \n",
       "34   patient_132       4                         1            Secondary   \n",
       "35  patient_1020       3                         2       Post-Secondary   \n",
       "36  patient_1021       3                         2       Post-Secondary   \n",
       "37  patient_1003       3                         1            Secondary   \n",
       "38  patient_1004       3                         1            Secondary   \n",
       "39  patient_1005       3                         1            Secondary   \n",
       "40  patient_1006       3                         1            Secondary   \n",
       "41  patient_1007       3                         1            Secondary   \n",
       "42  patient_1008       3                         1            Secondary   \n",
       "43  patient_1009       3                         1            Secondary   \n",
       "44  patient_1010       3                         1            Secondary   \n",
       "45  patient_1011       3                         1            Secondary   \n",
       "\n",
       "   Employment_Status  Randomization_Group_Coded  Age  Sex_Coded     Sex  \\\n",
       "0         Unemployed                        1.0   42          0  Female   \n",
       "1           Employed                        1.0   35          1    Male   \n",
       "2         Unemployed                        1.0   71          0  Female   \n",
       "3           Employed                        1.0   53          0  Female   \n",
       "4           Employed                        1.0   49          0  Female   \n",
       "5           Employed                        1.0   51          1    Male   \n",
       "6           Employed                        1.0   46          1    Male   \n",
       "7           Employed                        1.0   37          1    Male   \n",
       "8         Unemployed                        1.0   35          0  Female   \n",
       "9           Employed                        1.0   41          0  Female   \n",
       "10          Employed                        1.0   39          1    Male   \n",
       "11        Unemployed                        1.0   63          0  Female   \n",
       "12          Employed                        1.0   61          1    Male   \n",
       "13          Employed                        1.0   54          1    Male   \n",
       "14          Employed                        1.0   35          0  Female   \n",
       "15        Unemployed                        1.0   35          0  Female   \n",
       "16          Employed                        1.0   37          0  Female   \n",
       "17        Unemployed                        1.0   47          0  Female   \n",
       "18        Unemployed                        1.0   49          1    Male   \n",
       "19          Employed                        2.0   50          1    Male   \n",
       "20          Employed                        2.0   48          1    Male   \n",
       "21        Unemployed                        2.0   33          0  Female   \n",
       "22          Employed                        2.0   36          0  Female   \n",
       "23          Employed                        2.0   27          0  Female   \n",
       "24          Employed                        2.0   39          1    Male   \n",
       "25          Employed                        2.0   43          0  Female   \n",
       "26        Unemployed                        2.0   83          0  Female   \n",
       "27          Employed                        2.0   39          0  Female   \n",
       "28          Employed                        2.0   43          1    Male   \n",
       "29        Unemployed                        2.0   35          0  Female   \n",
       "30          Employed                        2.0   37          1    Male   \n",
       "31          Employed                        2.0   36          1    Male   \n",
       "32          Employed                        2.0   35          1    Male   \n",
       "33        Unemployed                        2.0   78          1    Male   \n",
       "34          Employed                        2.0   45          1    Male   \n",
       "35          Employed                        NaN   41          1    Male   \n",
       "36          Employed                        NaN   33          0  Female   \n",
       "37        Unemployed                        NaN   65          0  Female   \n",
       "38        Unemployed                        NaN   70          0  Female   \n",
       "39        Unemployed                        NaN   82          0  Female   \n",
       "40        Unemployed                        NaN   80          1    Male   \n",
       "41          Employed                        NaN   62          1    Male   \n",
       "42          Employed                        NaN   52          0  Female   \n",
       "43          Employed                        NaN   49          1    Male   \n",
       "44        Unemployed                        NaN   66          0    Male   \n",
       "45        Unemployed                        NaN   71          0  Female   \n",
       "\n",
       "    Ethnicity_Coded  ... Cube_Actual Infinity_Actual Clock_Actual  \\\n",
       "0                 1  ...           1               0      Correct   \n",
       "1                 1  ...           0               0    Incorrect   \n",
       "2                 1  ...           0               1      Correct   \n",
       "3                 1  ...           0               1      Correct   \n",
       "4                 0  ...           0               1      Correct   \n",
       "5                 0  ...           1               1      Correct   \n",
       "6                 1  ...           1               1      Correct   \n",
       "7                 1  ...           0               1      Correct   \n",
       "8                 0  ...           0               1      Correct   \n",
       "9                 0  ...           0               1      Correct   \n",
       "10                0  ...           0               1      Correct   \n",
       "11                1  ...           0               0      Correct   \n",
       "12                1  ...           0               0    Incorrect   \n",
       "13                1  ...           1               0      Correct   \n",
       "14                0  ...           1               1      Correct   \n",
       "15                0  ...           0               1      Correct   \n",
       "16                0  ...           1               1      Correct   \n",
       "17                1  ...           1               1      Correct   \n",
       "18                0  ...           1               1      Correct   \n",
       "19                1  ...           0               1      Correct   \n",
       "20                0  ...           1               1      Correct   \n",
       "21                0  ...           1               1      Correct   \n",
       "22                1  ...           0               1      Correct   \n",
       "23                1  ...           1               1      Correct   \n",
       "24                1  ...           0               1      Correct   \n",
       "25                0  ...           0               1      Correct   \n",
       "26                1  ...           0               1      Correct   \n",
       "27                0  ...           0               1      Correct   \n",
       "28                1  ...           0               1      Correct   \n",
       "29                0  ...           1               1      Correct   \n",
       "30                0  ...           1               1      Correct   \n",
       "31                1  ...           1               1      Correct   \n",
       "32                0  ...           1               1      Correct   \n",
       "33                0  ...           1               1      Correct   \n",
       "34                0  ...           1               1      Correct   \n",
       "35                1  ...           1               1      Correct   \n",
       "36                1  ...           1               1      Correct   \n",
       "37                1  ...           1               1      Correct   \n",
       "38                1  ...           1               1      Correct   \n",
       "39                1  ...           1               1      Correct   \n",
       "40                1  ...           1               1      Correct   \n",
       "41                1  ...           1               1      Correct   \n",
       "42                0  ...           1               1      Correct   \n",
       "43                1  ...           0               1      Correct   \n",
       "44                1  ...           0               1      Correct   \n",
       "45                1  ...           1               1      Correct   \n",
       "\n",
       "    cube_prediction  infinity_prediction  clock_prediction  cube_actual  \\\n",
       "0                 1                    0                 1            1   \n",
       "1                 0                    0                 0            0   \n",
       "2                 0                    1                 1            0   \n",
       "3                 0                    1                 1            0   \n",
       "4                 0                    1                 1            0   \n",
       "5                 1                    1                 1            1   \n",
       "6                 1                    1                 1            1   \n",
       "7                 0                    1                 1            0   \n",
       "8                 0                    1                 1            0   \n",
       "9                 0                    1                 1            0   \n",
       "10                0                    1                 1            0   \n",
       "11                0                    1                 1            0   \n",
       "12                0                    1                 1            0   \n",
       "13                1                    0                 1            1   \n",
       "14                0                    1                 1            1   \n",
       "15                0                    1                 1            0   \n",
       "16                1                    1                 1            1   \n",
       "17                1                    0                 1            1   \n",
       "18                1                    1                 1            1   \n",
       "19                0                    1                 1            0   \n",
       "20                1                    1                 1            1   \n",
       "21                1                    1                 1            1   \n",
       "22                0                    1                 1            0   \n",
       "23                1                    1                 1            1   \n",
       "24                0                    1                 1            0   \n",
       "25                0                    1                 1            0   \n",
       "26                0                    1                 1            0   \n",
       "27                0                    1                 1            0   \n",
       "28                0                    1                 1            0   \n",
       "29                0                    1                 1            1   \n",
       "30                0                    1                 1            1   \n",
       "31                1                    1                 1            1   \n",
       "32                1                    1                 1            1   \n",
       "33                1                    1                 1            1   \n",
       "34                1                    1                 1            1   \n",
       "35                1                    1                 1            1   \n",
       "36                1                    1                 1            1   \n",
       "37                1                    1                 1            1   \n",
       "38                1                    1                 1            1   \n",
       "39                1                    1                 1            1   \n",
       "40                1                    1                 1            1   \n",
       "41                1                    1                 1            1   \n",
       "42                1                    1                 1            1   \n",
       "43                0                    1                 1            0   \n",
       "44                0                    1                 1            0   \n",
       "45                1                    1                 1            1   \n",
       "\n",
       "   infinity_actual clock_actual Classification  \n",
       "0                0            1              0  \n",
       "1                0            0              0  \n",
       "2                1            1              0  \n",
       "3                1            1              0  \n",
       "4                1            1              0  \n",
       "5                1            1              1  \n",
       "6                1            1              1  \n",
       "7                1            1              0  \n",
       "8                1            1              0  \n",
       "9                1            1              0  \n",
       "10               1            1              0  \n",
       "11               0            1              0  \n",
       "12               0            0              0  \n",
       "13               0            1              0  \n",
       "14               1            1              0  \n",
       "15               1            1              0  \n",
       "16               1            1              1  \n",
       "17               1            1              0  \n",
       "18               1            1              1  \n",
       "19               1            1              0  \n",
       "20               1            1              1  \n",
       "21               1            1              1  \n",
       "22               1            1              0  \n",
       "23               1            1              1  \n",
       "24               1            1              0  \n",
       "25               1            1              0  \n",
       "26               1            1              0  \n",
       "27               1            1              0  \n",
       "28               1            1              0  \n",
       "29               1            1              0  \n",
       "30               1            1              0  \n",
       "31               1            1              1  \n",
       "32               1            1              1  \n",
       "33               1            1              1  \n",
       "34               1            1              1  \n",
       "35               1            1              1  \n",
       "36               1            1              1  \n",
       "37               1            1              1  \n",
       "38               1            1              1  \n",
       "39               1            1              1  \n",
       "40               1            1              1  \n",
       "41               1            1              1  \n",
       "42               1            1              1  \n",
       "43               1            1              0  \n",
       "44               1            1              0  \n",
       "45               1            1              1  \n",
       "\n",
       "[46 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the List with the Specific Metrics You Would Like to Display\n",
    "- Set to None to display all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specified_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on Confusion Matrix Normalization\n",
    "- Normalization by True Labels (Rows):\n",
    "    - Each entry in a row is divided by the sum of that row.\n",
    "    - Useful to understand how well each actual class is classified.\n",
    "    - Gives insights into the recall (sensitivity) for each class.\n",
    "​\n",
    "- Normalization by Predicted Labels (Columns):\n",
    "    - Each entry in a column is divided by the sum of that column.\n",
    "    - Useful to understand the precision of each class.\n",
    "    - Gives insights into how reliable each prediction is.\n",
    " \n",
    "- Normalization by the Total Sum of the Matrix:\n",
    "    - Each entry is divided by the total number of samples.\n",
    "    - Provides a proportion of the total data that falls into each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.classification_statistics import BinaryDataMetricsPlotter\n",
    "plotter = BinaryDataMetricsPlotter(dataframe=data_df, mapping_dict=mapping_dict, specified_metrics=['Accuracy', 'Sensitivity', 'Specificity', 'NPV', 'PPV'], \n",
    "                                   out_dir=None, cm_normalization='pred')\n",
    "plotter.run_plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.convert_metrics_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[:, ['Cognitively Intact', 'Cognitively Impaired', 'RoCA Score', 'Cognitive Status', 'Classification']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Evaluate a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "class BinaryClassificationEvaluation:\n",
    "    \"\"\"\n",
    "    Initializes the BinaryClassificationEvaluation with model results and the true outcomes.\n",
    "\n",
    "    Args:\n",
    "        fitted_model: The result object from a fitted statsmodels MNLogit model or similar.\n",
    "        observation_df: A pandas DataFrame with the true class outcomes.\n",
    "        normalization: Normalization method for the confusion matrix (None, 'true', 'pred', 'all').\n",
    "        predictions_df: DataFrame of predicted probabilities or values.\n",
    "        out_dir: Directory where plots and results will be saved.\n",
    "        threshold: The threshold to classify probabilities into binary outcomes.\n",
    "        positive_label: The label to be considered as the positive class. This is critical as the ROC curve calculations and thresholding will be based on this label.\n",
    "    \"\"\"\n",
    "    def __init__(self, fitted_model=None, observation_df=None, normalization=None, predictions_df=None, out_dir=None, threshold=None, positive_label=1):\n",
    "        self.results = fitted_model\n",
    "        self.observation_df = observation_df\n",
    "        self.normalization = normalization\n",
    "        self.predictions_df = predictions_df\n",
    "        self.out_dir = out_dir\n",
    "        self.threshold = threshold\n",
    "        self.positive_label = positive_label\n",
    "        \n",
    "    def find_optimal_threshold(self):\n",
    "        \"\"\"\n",
    "        Calculates the optimal threshold using Youden's J statistic from an ROC curve.\n",
    "\n",
    "        Returns:\n",
    "            float: The optimal threshold based on Youden's J statistic.\n",
    "        \"\"\"\n",
    "        if self.positive_label is None:\n",
    "            self.positive_label = 1\n",
    "\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true=self.observation_df, y_score=self.predictions_df, pos_label=self.positive_label)\n",
    "        youden_j = tpr - fpr\n",
    "\n",
    "        max_index = youden_j.argmax()\n",
    "        optimal_threshold = thresholds[max_index]\n",
    "        print(\"Optimal threshold by Youden's J: \", optimal_threshold)\n",
    "        \n",
    "        if optimal_threshold not in self.predictions_df.unique():\n",
    "            print('----- \\n WARNING: OPTIMAL THRESHOLD NOT IN OBSERVED SCORES. IF USING DISCRETE THRESHOLDS, THIS IS A PROBLEM AND SUGGESTS ARG:positive_label IS INCORRECT. \\n')\n",
    "        \n",
    "        if self.threshold is None:\n",
    "            self.threshold = optimal_threshold\n",
    "        return optimal_threshold, youden_j\n",
    "\n",
    "\n",
    "    def threshold_predictions(self, probabilities):\n",
    "        \"\"\"\n",
    "        Converts probabilities into binary predictions based on the threshold.\n",
    "\n",
    "        Args:\n",
    "            probabilities: Array of predicted probabilities.\n",
    "\n",
    "        Returns:\n",
    "            Array of binary predictions.\n",
    "        \"\"\"\n",
    "        print(f\"Positive cases are being set to those under: {self.threshold}. \\n Ensure this is logically sound.\")\n",
    "        return (probabilities < self.threshold).astype(int)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        \"\"\"\n",
    "        Generates predictions from the fitted model or predictions DataFrame.\n",
    "        \"\"\"\n",
    "        if self.predictions_df is not None:\n",
    "            self.raw_predictions = self.predictions_df.to_numpy()\n",
    "        else:\n",
    "            self.raw_predictions = self.results.predict()\n",
    "        self.predictions = self.threshold_predictions(self.raw_predictions)\n",
    "\n",
    "    def get_observations(self):\n",
    "        \"\"\"\n",
    "        Converts the observation DataFrame into a flattened array.\n",
    "        \"\"\"\n",
    "        self.observations = self.observation_df.to_numpy().flatten()\n",
    "\n",
    "    def calculate_confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Calculates the confusion matrix from predictions and observations.\n",
    "        \"\"\"\n",
    "        self.conf_matrix = confusion_matrix(self.observations, self.predictions, normalize=self.normalization)\n",
    "\n",
    "    def extract_confusion_components(self):\n",
    "        \"\"\"\n",
    "        Extracts True Positive, True Negative, False Positive, and False Negative counts from the confusion matrix.\n",
    "        \"\"\"\n",
    "        self.TP = self.conf_matrix[1, 1]\n",
    "        self.TN = self.conf_matrix[0, 0]\n",
    "        self.FP = self.conf_matrix[0, 1]\n",
    "        self.FN = self.conf_matrix[1, 0]\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculates accuracy, sensitivity, specificity, PPV, and NPV based on the confusion matrix components.\n",
    "        \"\"\"\n",
    "        self.accuracy = (self.TP + self.TN) / (self.TP + self.TN + self.FP + self.FN)\n",
    "        self.sensitivity = self.TP / (self.TP + self.FN)\n",
    "        self.specificity = self.TN / (self.TN + self.FP)\n",
    "        self.PPV = self.TP / (self.TP + self.FP)\n",
    "        self.NPV = self.TN / (self.TN + self.FN)\n",
    "\n",
    "    def display_metrics(self):\n",
    "        print(\"Accuracy:\", self.accuracy)\n",
    "        print(\"Sensitivity:\", self.sensitivity)\n",
    "        print(\"Specificity:\", self.specificity)\n",
    "        print(\"PPV:\", self.PPV)\n",
    "        print(\"NPV:\", self.NPV)\n",
    "\n",
    "    def plot_confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Plots a heatmap of the confusion matrix.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(self.conf_matrix, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        if self.out_dir:\n",
    "            os.makedirs(self.out_dir, exist_ok=True)\n",
    "            plt.savefig(os.path.join(self.out_dir, \"confusion_matrix.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "    def bootstrap_confidence_intervals(self, n_bootstraps=1000, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculates confidence intervals for evaluation metrics using bootstrapping.\n",
    "\n",
    "        Args:\n",
    "            n_bootstraps: Number of bootstrap samples.\n",
    "            alpha: Significance level for confidence intervals.\n",
    "\n",
    "        Returns:\n",
    "            dict: Confidence intervals for AUC, accuracy, sensitivity, specificity, PPV, and NPV.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        bootstrapped_metrics = {\n",
    "            'auc': [],\n",
    "            'accuracy': [],\n",
    "            'sensitivity': [],\n",
    "            'specificity': [],\n",
    "            'ppv': [],\n",
    "            'npv': []\n",
    "        }\n",
    "\n",
    "        data = pd.DataFrame({'observations': self.observations, 'predictions': self.predictions, 'raw_predictions': self.raw_predictions.flatten()})\n",
    "        \n",
    "        for _ in range(n_bootstraps):\n",
    "            bootstrap_sample = data.sample(n=len(data), replace=True)\n",
    "            y_true = bootstrap_sample['observations'].to_numpy()\n",
    "            y_pred = bootstrap_sample['predictions'].to_numpy()\n",
    "            y_scores = bootstrap_sample['raw_predictions'].to_numpy()\n",
    "            \n",
    "            # AUC\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_scores, pos_label=self.positive_label)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            bootstrapped_metrics['auc'].append(roc_auc)\n",
    "            \n",
    "            # Confusion matrix and derived metrics\n",
    "            conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "            TP = conf_matrix[1, 1]\n",
    "            TN = conf_matrix[0, 0]\n",
    "            FP = conf_matrix[0, 1]\n",
    "            FN = conf_matrix[1, 0]\n",
    "\n",
    "            accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "            sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "            specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "            ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            npv = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "\n",
    "            bootstrapped_metrics['accuracy'].append(accuracy)\n",
    "            bootstrapped_metrics['sensitivity'].append(sensitivity)\n",
    "            bootstrapped_metrics['specificity'].append(specificity)\n",
    "            bootstrapped_metrics['ppv'].append(ppv)\n",
    "            bootstrapped_metrics['npv'].append(npv)\n",
    "            \n",
    "        self.bootstrapped_metrics = bootstrapped_metrics\n",
    "        confidence_intervals = {}\n",
    "        for metric in bootstrapped_metrics:\n",
    "            sorted_metrics = np.sort(bootstrapped_metrics[metric])\n",
    "            lower_bound = np.percentile(sorted_metrics, alpha / 2 * 100)\n",
    "            upper_bound = np.percentile(sorted_metrics, (1 - alpha / 2) * 100)\n",
    "            confidence_intervals[metric] = (lower_bound, upper_bound)\n",
    "        return confidence_intervals\n",
    "    \n",
    "    def calculate_p_values(self, random_classifier_metrics):\n",
    "        \"\"\"\n",
    "        Calculates p-values for bootstrapped metrics compared to the random classifier's metrics.\n",
    "\n",
    "        Args:\n",
    "            bootstrapped_metrics: Dictionary containing lists of bootstrapped metric values.\n",
    "            random_classifier_metrics: Dictionary containing the random classifier's metric values.\n",
    "\n",
    "        Returns:\n",
    "            dict: P-values for each metric.\n",
    "        \"\"\"\n",
    "        p_values = {}\n",
    "        for metric in self.bootstrapped_metrics:\n",
    "            metric_values = self.bootstrapped_metrics[metric]\n",
    "            random_value = random_classifier_metrics[metric]\n",
    "            p_value = np.mean(np.array(metric_values) < random_value)\n",
    "            p_values[metric] = p_value\n",
    "        return p_values\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the calculation and display of all evaluation metrics.\n",
    "        \"\"\"\n",
    "        self.find_optimal_threshold()\n",
    "        self.get_predictions()\n",
    "        self.get_observations()\n",
    "        self.calculate_confusion_matrix()\n",
    "        self.extract_confusion_components()\n",
    "        self.calculate_metrics()\n",
    "        self.display_metrics()\n",
    "        self.plot_confusion_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__(self, fitted_model=None, observation_df=None, normalization=None, predictions_df=None, out_dir=None, threshold=0.5):\n",
    "bce = BinaryClassificationEvaluation(fitted_model=None, normalization='pred', \n",
    "                                     predictions_df=data_df['RoCA Score'], observation_df=data_df['Cognitively Impaired'], \n",
    "                                     threshold=None, positive_label=0)\n",
    "bce.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the Optimal Threshold and Derive Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence intervals\n",
    "confidence_intervals = bce.bootstrap_confidence_intervals()\n",
    "print(\"Confidence Intervals:\", confidence_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get P-Values\n",
    "- The dictionary of random classifier baselines are provided in section 05\n",
    "    - {'Sensitivity': 0, 'Specificity': 0.5, 'Precision': 0.0, 'PPV': 0.0, 'NPV': 1.0, 'Accuracy': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline = {'Sensitivity': 0, 'Specificity': 0.5, 'Precision': 0.0, 'PPV': 0.0, 'NPV': 1.0, 'Accuracy': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline = {'sensitivity': 0, 'specificity': 0.5,  'ppv': 0.0, 'npv': 0.63, 'accuracy': 0.5, 'auc': 0.5}\n",
    "bce.calculate_p_values(random_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Evaluate a Multiclass Classifier\n",
    "- If you have ground truths and some sort of continuous variable which can be used for classification, this will help you visualize that.\n",
    "```\n",
    "Args:\n",
    "    fitted_model: The result object from a fitted statsmodels MNLogit model.\n",
    "    outcome_matrix: A pandas DataFrame with the true class outcomes in one-hot encoded format.\n",
    "    normalization: Normalization method for the confusion matrix (None, 'true', 'pred', 'all').\n",
    "    predictions_df: Manually entered DataFrame of predictions, can contain probabilities or dummy-coded predictions.\n",
    "    thresholds (dict): a dictionary mapping the index of the threshold to the probability threshold to make that classification. \n",
    "    assign_labels (bool): Scipy's confusion matrix orders by minimum to maximum occurence of the predictions. It will output the confusion matrix by this. \n",
    "            If set to False, we will organize our confusion matrix as per scipy's order. \n",
    "```\n",
    "- The ROC considers clasisfications acoss ALL POSSIBLE PROBABILITIES, demonstrating what is ultiamtely accomplishable at the best possible threshold\n",
    "\n",
    "- First curve is ROC for classifcation of each class with respect to all other classes\n",
    "- Second Curve (Macro Average) is basically a meta-analytic ROC with equal weight per class.\n",
    "- Third Curve (Micro Average) is basically a meta-analytic ROC with weight proportional to class sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have predictions and observations organized as:\n",
    "```\n",
    "Observations (columns are a given classificaiton)\n",
    "    [[0, 1, 0, 0],\n",
    "      1, 0 ,0 ,0]]\n",
    "predictions (columns are a given classification, value is the p(class))      \n",
    "    It will expect predictions dataframeto take the form where prediction for a given classificition is an array of probability:\n",
    "    [[0.2, 0.7, 0.1, 0.0],\n",
    "      0.9, 0.05, 0.05, 0.0]]\n",
    "```\n",
    "then shape them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def create_observations_df(df, outcome_column):\n",
    "    # One-hot encode the binary outcomes\n",
    "    observations_df = pd.get_dummies(df[outcome_column])\n",
    "    return observations_df\n",
    "\n",
    "def create_predictions_df(df, probability_column):\n",
    "    # Assuming binary classification, the probability of the negative class is 1 minus the probability of the positive class\n",
    "    predictions_df = pd.DataFrame({\n",
    "        0: 1 - df[probability_column],\n",
    "        1: df[probability_column]\n",
    "    })\n",
    "    return predictions_df\n",
    "observations_df = create_observations_df(data_df, 'Cognitive_Status')\n",
    "predictions_df = create_predictions_df(data_df, 'Ensemble')\n",
    "observations_df = observations_df.iloc[:, [1]]\n",
    "predictions_df = predictions_df.iloc[:, [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.classification_statistics import ComprehensiveMulticlassROC, MulticlassClassificationEvaluation, MicroAverageROC\n",
    "evaluator = MulticlassClassificationEvaluation(fitted_model=None, observation_df=observations_df, predictions_df=predictions_df, normalization='pred',\n",
    "                                     thresholds=None, out_dir=out_dir)\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Evaluate a Random Classifier\n",
    "- Set the mapping_dict so keys are columns and values are the positive hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Measurement', 'Cohort', 'Educational_Status_Coded',\n",
       "       'Educational_Status', 'Employment_Status', 'Randomization_Group_Coded',\n",
       "       'Age', 'Sex_Coded', 'Sex', 'Ethnicity_Coded', 'Ethnicity',\n",
       "       'Cognitive_Exam', 'Cognitive_Status', 'Cognitive_Status_Code',\n",
       "       'Question_16', 'Convolutional_Neural_Network_Equivalent',\n",
       "       'Weighted_Sum', 'Cube_Predicted', 'Infinity_Predicted',\n",
       "       'Clock_Predicted', 'Cube_Actual', 'Infinity_Actual', 'Clock_Actual',\n",
       "       'cube_prediction', 'infinity_prediction', 'clock_prediction',\n",
       "       'cube_actual', 'infinity_actual', 'clock_actual', 'Classification'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'Cognitive_Status':'Correct'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from calvin_utils.statistical_utils.classification_statistics import BinaryDataMetricsPlotter\n",
    "\n",
    "class EvaluateRandomClassifier(BinaryDataMetricsPlotter):\n",
    "    def __init__(self, dataframe, mapping_dict, out_dir=None, cm_normalization=None, n_classes=2):\n",
    "        '''\n",
    "        mapping_dict (dict): a dicitonary where keys represent the column with observed classes. Keys are the observation corresponding to a hit. \n",
    "        '''\n",
    "        self.dataframe = dataframe\n",
    "        self.mapping_dict = mapping_dict\n",
    "        self.specified_metrics = None\n",
    "        self.save_dir = out_dir\n",
    "        self.n_classes=n_classes\n",
    "        self.metrics = self.calculate_metrics()\n",
    "        self.confusion_matrices = self.get_confusion_matrices(normalize=cm_normalization)\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        metrics = {}\n",
    "        for class_col, positive_class in self.mapping_dict.items():\n",
    "            if len(self.dataframe[class_col].unique()) != 2:\n",
    "                raise ValueError(\"Only 2 classes acceptable in this class. Do not pass columns with more than 2 classes.\")\n",
    "            tp = 1/self.n_classes * (self.dataframe[class_col]==positive_class).sum() / (self.dataframe[class_col]).count()\n",
    "            tn = (1 - 1/self.n_classes) * (self.dataframe[class_col]!=positive_class).sum() / (self.dataframe[class_col]).count()\n",
    "            fp = 1/self.n_classes * (self.dataframe[class_col]!=positive_class).sum() / (self.dataframe[class_col]).count()\n",
    "            fn = (1 - 1/self.n_classes) * (self.dataframe[class_col]==positive_class).sum() / (self.dataframe[class_col]).count()\n",
    "            \n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "            ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Positive Predictive Value\n",
    "            npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Negative Predictive Value\n",
    "            acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "            precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "            recall = sensitivity  # Recall is the same as sensitivity\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            metrics[class_col] = {\n",
    "                'Sensitivity': sensitivity,\n",
    "                'Specificity': specificity,\n",
    "                'Precision': precision,\n",
    "                'PPV': ppv,\n",
    "                'NPV': npv,\n",
    "                'Accuracy': acc,\n",
    "                'F1 Score': f1,\n",
    "                'TP': tp,\n",
    "                'TN': tn,\n",
    "                'FP': fp,\n",
    "                'FN': fn\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def get_confusion_matrices(self, normalize=None):\n",
    "        confusion_matrices = {}\n",
    "        for class_col in self.mapping_dict.keys():\n",
    "            tp = self.metrics[class_col]['TP']\n",
    "            tn = self.metrics[class_col]['TN']\n",
    "            fp = self.metrics[class_col]['FP']\n",
    "            fn = self.metrics[class_col]['FN']\n",
    "            cm = np.array([[tn, fp], [fn, tp]])\n",
    "            if normalize == 'true':\n",
    "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            elif normalize == 'pred':\n",
    "                cm = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]\n",
    "            elif normalize == 'all':\n",
    "                cm = cm.astype('float') / cm.sum()\n",
    "            else: \n",
    "                pass\n",
    "            confusion_matrices[class_col] = cm\n",
    "        return confusion_matrices\n",
    "    \n",
    "    def plot_confusion_matrices(self):\n",
    "        confusion_matrices = self.confusion_matrices\n",
    "        num_matrices = len(confusion_matrices)\n",
    "        fig, axes = plt.subplots(1, num_matrices, figsize=(6 * num_matrices, 6))\n",
    "        \n",
    "        if num_matrices == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for ax, (class_col, cm) in zip(axes, confusion_matrices.items()):\n",
    "            sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', ax=ax,\n",
    "                        xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'],\n",
    "                        annot_kws={\"size\": 16})  # Set annotation font size\n",
    "            ax.set_xlabel(f'Predicted: {self.mapping_dict[class_col]}', fontsize=16)\n",
    "            ax.set_ylabel(f'Actual: {class_col}', fontsize=16)\n",
    "            ax.set_title(f'Confusion Matrix for {class_col} vs {self.mapping_dict[class_col]}', fontsize=16)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "            \n",
    "        if self.save_dir is not None:\n",
    "            subdir = \"confusion_matrix\"\n",
    "            os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "            file_name_svg = \"conf_matrix.svg\"\n",
    "            path_svg = os.path.join(self.save_dir, subdir, file_name_svg)\n",
    "            plt.savefig(path_svg, format='svg')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_probability_influence(self):\n",
    "        \"\"\"\n",
    "        Visualize how the predicted probabilities influence the random classifier.\n",
    "        \"\"\"\n",
    "        for class_col, metrics in self.metrics.items():\n",
    "            # Extract the required metrics\n",
    "            tp = int(metrics['TP'])\n",
    "            tn = int(metrics['TN'])\n",
    "            fp = int(metrics['FP'])\n",
    "            fn = int(metrics['FN'])\n",
    "\n",
    "            # Simulate probabilities based on the metric values\n",
    "            total = tp + tn + fp + fn\n",
    "            pred_prob = np.random.rand(total)\n",
    "            true_prob = np.concatenate((\n",
    "                np.repeat(1, tp + fn),  # True positives and false negatives\n",
    "                np.repeat(0, tn + fp)   # True negatives and false positives\n",
    "            ))\n",
    "\n",
    "            # Simulate true labels based on probabilities\n",
    "            true_labels = np.concatenate((\n",
    "                np.repeat(1, tp), np.repeat(0, fn),\n",
    "                np.repeat(0, tn), np.repeat(1, fp)\n",
    "            ))\n",
    "\n",
    "            # Determine predicted labels based on a threshold of 0.5 for visualization purposes\n",
    "            pred_labels = (pred_prob > 0.5).astype(int)\n",
    "\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            # Determine color based on correct classification\n",
    "            correct_classification = true_labels == pred_labels\n",
    "            colors = np.array(sns.color_palette(\"tab10\"))[correct_classification.astype(int)]\n",
    "\n",
    "            # Scatter plot\n",
    "            scatter = plt.scatter(pred_prob, true_prob, c=colors, alpha=0.6, edgecolor='w', linewidth=0.5)\n",
    "\n",
    "            # Colorbar settings\n",
    "            cbar = plt.colorbar(scatter, boundaries=[0, 0.5, 1], ticks=[0.25, 0.75])\n",
    "            cbar.set_ticklabels(['Incorrect', 'Correct'])\n",
    "            cbar.set_label('Classification Result', fontsize=12)\n",
    "\n",
    "            # Plot settings\n",
    "            plt.xlabel(\"Predicted Probability of Positive Class\", fontsize=14)\n",
    "            plt.ylabel(\"True Probability of Positive Class\", fontsize=14)\n",
    "            plt.title(f\"Probability Influence on Random Classifier for {class_col}\", fontsize=16)\n",
    "\n",
    "            # Grid and layout settings\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Show the plot\n",
    "            if self.save_dir is not None:\n",
    "                subdir = \"probability_influence\"\n",
    "                os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "                file_name_svg = f\"{class_col}_probability_influence.svg\"\n",
    "                path_svg = os.path.join(self.save_dir, subdir, file_name_svg)\n",
    "                plt.savefig(path_svg, format='svg')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    def plot_radar_charts(self):\n",
    "        if self.specified_metrics is None:\n",
    "            self.specified_metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'PPV', 'NPV']\n",
    "        \n",
    "        color_map = sns.color_palette(\"tab10\", len(self.mapping_dict))\n",
    "\n",
    "        for idx, (class_col, metric_values) in enumerate(self.metrics.items()):\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            ax = plt.subplot(111, polar=True)\n",
    "\n",
    "            categories = self.specified_metrics\n",
    "            N = len(categories)\n",
    "\n",
    "            angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "            angles += angles[:1]\n",
    "\n",
    "            ax.set_theta_offset(pi / 2)\n",
    "            ax.set_theta_direction(-1)\n",
    "\n",
    "            plt.xticks(angles[:-1], categories)\n",
    "\n",
    "            ax.set_rlabel_position(0)\n",
    "            plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\", \"0.4\", \"0.6\", \"0.8\"], color=\"black\", size=12)\n",
    "            plt.ylim(0, 1)\n",
    "\n",
    "            values = [metric_values[metric] for metric in self.specified_metrics]\n",
    "            values += values[:1]\n",
    "            ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'{class_col}', color=color_map[idx % len(color_map)])\n",
    "            ax.fill(angles, values, alpha=0.25, color=color_map[idx % len(color_map)])\n",
    "\n",
    "            plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "            plt.title(f'Metrics for \"{class_col}\"', size=15, color='black', y=1.1)\n",
    "\n",
    "            if self.save_dir is not None:\n",
    "                radar_plots_subdir = \"radar_plots\"\n",
    "                os.makedirs(os.path.join(self.save_dir, radar_plots_subdir), exist_ok=True)\n",
    "                file_name_svg = f\"{class_col}_radar.svg\"\n",
    "                path_svg = os.path.join(self.save_dir, radar_plots_subdir, file_name_svg)\n",
    "                plt.savefig(path_svg, format='svg')\n",
    "            plt.show()\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "    def run(self):\n",
    "        self.plot_confusion_matrices()\n",
    "        self.plot_radar_charts()\n",
    "        self.visualize_probability_influence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_classifier = EvaluateRandomClassifier(dataframe=data_df, mapping_dict=mapping_dict, out_dir=None, cm_normalization=None,\n",
    "                                                    n_classes=4)\n",
    "random_classifier.run()\n",
    "print(random_classifier.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Scatterplot a Value to a Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import expit\n",
    "\n",
    "# Define logistic function\n",
    "def logistic_function(x, beta_0, beta_1):\n",
    "    return expit(beta_0 + beta_1 * x)\n",
    "\n",
    "# Function to create scatter plot and fit sigmoid using logistic regression\n",
    "def plot_with_logistic_regression(df, dv_col, iv_col):\n",
    "    # Extract data\n",
    "    x = df[iv_col].values.reshape(-1, 1)\n",
    "    y = df[dv_col].values\n",
    "\n",
    "    # Normalize the dependent variable to be between 0 and 1\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    y_normalized = (y - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x, y_normalized)\n",
    "\n",
    "    # Create fitted sigmoid curve data\n",
    "    x_fit = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)\n",
    "    y_fit = logistic_function(x_fit, model.intercept_[0], model.coef_[0][0])\n",
    "    \n",
    "    # Create a JointGrid for the scatter plot and KDEs\n",
    "    g = sns.JointGrid(x=df[iv_col], y=y_normalized, space=0, height=8, ratio=5)\n",
    "    g.plot_joint(sns.scatterplot, color=\"tab:blue\", alpha=0.6)\n",
    "    g.plot_joint(plt.plot, x_fit, y_fit, color='blue', label='Logistic Fit')\n",
    "    g.plot_marginals(sns.kdeplot, fill=True, color='tab:blue')\n",
    "\n",
    "    # Labels and title\n",
    "    g.set_axis_labels(iv_col, f'Normalized {dv_col}', fontsize=14)\n",
    "    plt.suptitle(f'Scatter Plot with Logistic Regression Fit and KDE: {iv_col} vs {dv_col}', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()\n",
    "\n",
    "    # Plot\n",
    "    # plt.figure(figsize=(6, 6))\n",
    "    # sns.scatterplot(x=x.flatten(), y=y_normalized, palette=\"tab10\", label=None)\n",
    "    # plt.plot(x_fit, y_fit, color='blue', label='Logistic Fit')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import expit\n",
    "import os\n",
    "# Define logistic function\n",
    "def logistic_function(x, beta_0, beta_1):\n",
    "    return expit(beta_0 + beta_1 * x)\n",
    "\n",
    "# Function to create scatter plot and fit sigmoid using logistic regression with KDE plots\n",
    "def plot_with_logistic_regression_and_kde(df, dv_col, iv_col, xlim=None, ylim=None, out_dir=None):\n",
    "    # Extract data\n",
    "    x = df[iv_col].values.reshape(-1, 1)\n",
    "    y = df[dv_col].values\n",
    "\n",
    "    # Normalize the dependent variable to be between 0 and 1\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    y_normalized = (y - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x, y_normalized)\n",
    "\n",
    "    # Create fitted sigmoid curve data\n",
    "    x_fit = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)\n",
    "    y_fit = logistic_function(x_fit, model.intercept_[0], model.coef_[0][0])\n",
    "\n",
    "    # Create a JointGrid for the scatter plot and KDEs\n",
    "    g = sns.JointGrid(x=df[iv_col], y=y_normalized, space=0, height=8, ratio=5)\n",
    "    g.plot_joint(sns.scatterplot, alpha=0.6)\n",
    "    g.plot_marginals(sns.kdeplot, fill=True, color='tab:blue')\n",
    "    \n",
    "    # Plot the logistic fit line separately\n",
    "    g.ax_joint.plot(x_fit, y_fit, color='blue', label='Logistic Fit')\n",
    "\n",
    "    # Labels and title\n",
    "    g.set_axis_labels(iv_col, f'{dv_col}', fontsize=16)\n",
    "    plt.suptitle(f'Scatter Plot with Logistic Regression Fit and KDE:\\n{iv_col} vs {dv_col}', fontsize=20)\n",
    "\n",
    "    # plt.subplots_adjust(top=0.95)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.tight_layout()\n",
    "    plt.yticks([0,1])\n",
    "    if xlim:\n",
    "        g.ax_joint.set_xlim(xlim)\n",
    "    if ylim:\n",
    "        g.ax_joint.set_ylim(ylim)\n",
    "\n",
    "    # Show the plot\n",
    "    if out_dir is not None:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(out_dir, 'sigmoid_scatter.svg'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/roca/figures/cognitive_classifier/clock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_logistic_regression_and_kde(data_df, dv_col='Cognitively Intact', iv_col='Clock Prediction', ylim=(-0.05, 1.05), xlim=(-0.05,1.05), out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create jitter plot with KDE plots\n",
    "def plot_jitter_with_kde(df, category_col, value_col):\n",
    "    # Create a FacetGrid\n",
    "    g = sns.FacetGrid(df, col=category_col, col_wrap=4, sharex=False, sharey=False, height=4)\n",
    "    \n",
    "    # Map the stripplot (jitter plot) to the FacetGrid\n",
    "    g.map(sns.stripplot, value_col, jitter=True, alpha=0.6, color=\"tab:blue\")\n",
    "    \n",
    "    # Map the KDE plot to the FacetGrid\n",
    "    g.map(sns.kdeplot, value_col, fill=True, color=\"tab:blue\", alpha=0.6)\n",
    "    \n",
    "    # Adjust the layout\n",
    "    g.set_axis_labels(value_col, '')\n",
    "    g.set_titles(col_template='{col_name}')\n",
    "    g.fig.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(f'Jitter Plot with KDE: {value_col} across {category_col}', fontsize=16)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_jitter_with_kde(data_df, 'Cognitively Intact', 'Clock Prediction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def bootstrap_metrics(data_df, true_col, pred_col, n_iterations=1000):\n",
    "    def calculate_metrics(true, pred):\n",
    "        accuracy = accuracy_score(true, pred)\n",
    "        sensitivity = recall_score(true, pred, zero_division=0)  # Sensitivity is the same as recall\n",
    "        specificity = recall_score(true, pred, pos_label=0, zero_division=0)\n",
    "        precision = precision_score(true, pred, zero_division=0)\n",
    "        f1 = f1_score(true, pred, zero_division=0)\n",
    "        return accuracy, sensitivity, specificity, precision, f1\n",
    "    # Suppress specific warnings\n",
    "        \n",
    "    # Observed metrics\n",
    "    obs_metrics = calculate_metrics(data_df[true_col], data_df[pred_col])\n",
    "    \n",
    "    # Bootstrapping\n",
    "    bootstrap_metrics = []\n",
    "    for _ in range(n_iterations):\n",
    "        boot_df = resample(data_df)\n",
    "        metrics = calculate_metrics(boot_df[true_col], boot_df[pred_col])\n",
    "        bootstrap_metrics.append(metrics)\n",
    "    \n",
    "    bootstrap_metrics = np.array(bootstrap_metrics)\n",
    "    \n",
    "    # Confidence intervals\n",
    "    conf_intervals = np.percentile(bootstrap_metrics, [2.5, 97.5], axis=0)\n",
    "    \n",
    "    # Print observed metrics and confidence intervals\n",
    "    print(f\"Observed Metrics:\\nAccuracy: {obs_metrics[0]}, Sensitivity: {obs_metrics[1]}, Specificity: {obs_metrics[2]}, Precision: {obs_metrics[3]}, F1 Score: {obs_metrics[4]}\")\n",
    "    print(f\"\\nConfidence Intervals (95%):\")\n",
    "    print(f\"Accuracy: {conf_intervals[:, 0]}\")\n",
    "    print(f\"Sensitivity: {conf_intervals[:, 1]}\")\n",
    "    print(f\"Specificity: {conf_intervals[:, 2]}\")\n",
    "    print(f\"Precision: {conf_intervals[:, 3]}\")\n",
    "    print(f\"F1 Score: {conf_intervals[:, 4]}\")\n",
    "\n",
    "# Example usage\n",
    "bootstrap_metrics(data_df, 'cube_actual', 'cube_prediction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
