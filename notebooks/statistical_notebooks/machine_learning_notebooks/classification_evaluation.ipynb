{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a Model's Classifications\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: July 6, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/metadata/dataset_2/UB_correlations_w_LBD.csv'\n",
    "sheet = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/collaborations/barotono_disease_classification/figure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.fillna(value=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Age', 'Z_Scored_Percent_Cognitive_Improvement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'DX'  # The column you'd like to evaluate\n",
    "condition = 'equal'  # The condition to check ('equal', 'above', 'below')\n",
    "value = 'PD'  # The value to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "data_df.rename(columns={\n",
    "    'cube_prediction': 'Cube Prediction',\n",
    "    'infinity_prediction': 'Infinity Prediction',\n",
    "    'clock_prediction': 'Clock Prediction',\n",
    "    'cube_actual': 'Cube Actual',\n",
    "    'infinity_actual': 'Infinity Actual',\n",
    "    'clock_actual': 'Clock Actual',\n",
    "    'Cognitive_Status_Code': 'Cognitively Intact',\n",
    "    'Cognitive_Status': 'Cognitive Status',\n",
    "    'Question_16': 'RoCA Score'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Evaluate Already Administered Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Use the Mapping Dictionary\n",
    "- The mapping dictionary is a crucial component of the BinaryDataMetricsPlotter class. It defines the relationships between columns in your dataframe that you want to compare. Here's how to use it:\n",
    "\n",
    "- Define the Mapping: Create a dictionary where each key-value pair represents a mapping of the columns of ground-truth classification to the column of the experimental classificaiton. \n",
    "\n",
    ">mapping_dict = {\n",
    ">>    'gold_standard_1': 'classification_1',\n",
    ">>\n",
    ">>    'gold_standard_2': 'classification_2',\n",
    ">>\n",
    ">>    add more as needed\n",
    ">>\n",
    ">}\n",
    "\n",
    "**The mapping dict must have gold standards as keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Diagnosis'] = np.where(data_df['Question_16'] >= 7 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'Diagnosis':'Cognitive_Status_Code'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "**Visualize Classification Metrics with Barplots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sensitivity (Recall): Measures the proportion of actual positives correctly identified. Identical to recall, it assesses how well the model identifies true positives among the positive cases.\n",
    "\n",
    "- Specificity: Indicates the proportion of actual negatives correctly identified as such. It evaluates the model's ability to recognize true negatives among negative cases.\n",
    "\n",
    "- Precision: Refers to the proportion of positive identifications that are actually correct. It focuses on the accuracy of the positive predictions made by the model.\n",
    "\n",
    "- Recall: Measures the proportion of actual positives that are correctly identified. This is identical to sensitivity, emphasizing the model's accuracy in detecting positive cases.\n",
    "\n",
    "- Accuracy: The ratio of correctly predicted observations to the total observations. It provides an overall measure of the model's performance.\n",
    "\n",
    "- F1 Score: The harmonic mean of precision and recall. This metric is particularly useful when the balance between precision and recall is important.\n",
    "\n",
    "- Positive Predictive Value (PPV): Similar to precision, PPV is the proportion of positive test results that are true positives. It indicates the likelihood that a positive test accurately reflects the underlying condition.\n",
    "\n",
    "- Negative Predictive Value (NPV): The proportion of negative test results that are true negatives. NPV measures the likelihood that a negative test result accurately indicates the absence of the condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the List with the Specific Metrics You Would Like to Display\n",
    "- Set to None to display all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specified_metrics = ['Accuracy', 'Sensitivity',  'NPV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on Confusion Matrix Normalization\n",
    "- Normalization by True Labels (Rows):\n",
    "    - Each entry in a row is divided by the sum of that row.\n",
    "    - Useful to understand how well each actual class is classified.\n",
    "    - Gives insights into the recall (sensitivity) for each class.\n",
    "​\n",
    "- Normalization by Predicted Labels (Columns):\n",
    "    - Each entry in a column is divided by the sum of that column.\n",
    "    - Useful to understand the precision of each class.\n",
    "    - Gives insights into how reliable each prediction is.\n",
    " \n",
    "- Normalization by the Total Sum of the Matrix:\n",
    "    - Each entry is divided by the total number of samples.\n",
    "    - Provides a proportion of the total data that falls into each category.\n",
    "\n",
    "Options:\n",
    "'all', 'pred', 'true', None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization='pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, auc, roc_curve, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from calvin_utils.statistical_utils.distribution_statistics import bootstrap_distribution_statistics\n",
    "from math import pi\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class BinaryDataMetricsPlotter:\n",
    "    def __init__(self, dataframe, mapping_dict, specified_metrics=None, out_dir=None, cm_normalization=None):\n",
    "        \"\"\"\n",
    "        Initialize with a dataframe containing binary data and a dictionary mapping columns.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.mapping_dict = mapping_dict\n",
    "        self.specified_metrics = specified_metrics\n",
    "        self.save_dir = out_dir\n",
    "        self.metrics = self.calculate_metrics()\n",
    "        self.confusion_matrices = self.get_confusion_matrices(normalize=cm_normalization)\n",
    "        \n",
    "    def bootstrap_metrics(self, n_iterations=1000):\n",
    "        def calculate_metrics(true, pred):\n",
    "            tn, fp, fn, tp = confusion_matrix(true, pred).ravel()\n",
    "            \n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "            ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Positive Predictive Value\n",
    "            npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Negative Predictive Value\n",
    "            acc = accuracy_score(true, pred)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(true, pred, average='binary')\n",
    "            \n",
    "            return {\n",
    "                'Sensitivity': sensitivity,\n",
    "                'Specificity': specificity,\n",
    "                'Precision': precision,\n",
    "                'PPV': ppv,\n",
    "                'NPV': npv,\n",
    "                'Accuracy': acc,\n",
    "                'F1 Score': f1,\n",
    "                'TP': tp,\n",
    "                'TN': tn,\n",
    "                'FP': fp,\n",
    "                'FN': fn\n",
    "            }\n",
    "        \n",
    "        # Suppress warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        bootstrap_results = {}\n",
    "\n",
    "        for truth, prediction in self.mapping_dict.items():\n",
    "            # Observed metrics\n",
    "            obs_metrics = calculate_metrics(self.dataframe[truth], self.dataframe[prediction])\n",
    "            \n",
    "            # Bootstrapping\n",
    "            bootstrap_metrics = []\n",
    "            for _ in range(n_iterations):\n",
    "                boot_df = resample(self.dataframe)\n",
    "                metrics = calculate_metrics(boot_df[truth], boot_df[prediction])\n",
    "                bootstrap_metrics.append(metrics)\n",
    "            \n",
    "            # Convert list of dictionaries to a dictionary of lists\n",
    "            bootstrap_metrics = {key: [d[key] for d in bootstrap_metrics] for key in bootstrap_metrics[0]}\n",
    "            \n",
    "            # Confidence intervals\n",
    "            conf_intervals = {key: np.percentile(values, [2.5, 97.5]) for key, values in bootstrap_metrics.items()}\n",
    "            \n",
    "            # Store results for this pair of columns\n",
    "            bootstrap_results[(truth, prediction)] = {\n",
    "                'observed': obs_metrics,\n",
    "                'conf_intervals': conf_intervals\n",
    "            }\n",
    "        \n",
    "        return bootstrap_results\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        metrics = {}\n",
    "        for truth, prediction in self.mapping_dict.items():\n",
    "            tn, fp, fn, tp = confusion_matrix(self.dataframe[truth], self.dataframe[prediction]).ravel()\n",
    "            \n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "            ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Positive Predictive Value\n",
    "            npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Negative Predictive Value\n",
    "            acc = accuracy_score(self.dataframe[truth], self.dataframe[prediction])\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(self.dataframe[truth], self.dataframe[prediction], average='binary')\n",
    "\n",
    "            metrics[(truth, prediction)] = {\n",
    "                'Sensitivity': sensitivity,\n",
    "                'Specificity': specificity,\n",
    "                'Precision': precision,\n",
    "                'PPV': ppv,\n",
    "                'NPV': npv,\n",
    "                'Accuracy': acc,\n",
    "                'F1 Score': f1,\n",
    "                'TP': tp,\n",
    "                'TN': tn,\n",
    "                'FP': fp,\n",
    "                'FN': fn\n",
    "            }\n",
    "        return metrics\n",
    "    \n",
    "    def get_confusion_matrices(self, normalize=False):\n",
    "        confusion_matrices = {}\n",
    "        for ground_truth, predicted in self.mapping_dict.items():\n",
    "            cm = confusion_matrix(self.dataframe[ground_truth], self.dataframe[predicted], normalize=normalize)\n",
    "            confusion_matrices[(ground_truth, predicted)] = cm\n",
    "        return confusion_matrices\n",
    "\n",
    "    def plot_confusion_matrices(self):\n",
    "        confusion_matrices = self.confusion_matrices\n",
    "        num_matrices = len(confusion_matrices)\n",
    "        fig, axes = plt.subplots(1, num_matrices, figsize=(6 * num_matrices, 6))\n",
    "        \n",
    "        if num_matrices == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for ax, ((ground_truth, predicted), cm) in zip(axes, confusion_matrices.items()):\n",
    "            sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', ax=ax,\n",
    "                        xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'],\n",
    "                        annot_kws={\"size\": 16})  # Set annotation font size\n",
    "            ax.set_ylabel(f'Predicted: {predicted}', fontsize=16)\n",
    "            ax.set_xlabel(f'Actual: {ground_truth}', fontsize=16)\n",
    "            ax.set_title(f'Confusion Matrix for {ground_truth} vs {predicted}', fontsize=16)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "            \n",
    "        if self.save_dir is not None:\n",
    "            subdir = \"confusion_matrix\"\n",
    "            os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "            file_name_svg = \"conf_matrix.svg\"\n",
    "            path_svg = os.path.join(self.save_dir, subdir, file_name_svg)\n",
    "            plt.savefig(path_svg, format='svg')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_radar_charts(self):\n",
    "        if self.specified_metrics is None:\n",
    "            self.specified_metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'PPV', 'NPV']\n",
    "            \n",
    "        tab10 = sns.color_palette(\"tab10\", 10)\n",
    "        color_map = sns.color_palette([tab10[i] for i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "\n",
    "        # Get bootstrap metrics\n",
    "        bootstrap_results = self.bootstrap_metrics()\n",
    "        \n",
    "        for idx, ((old_col, new_col), metric_values) in enumerate(self.metrics.items()):\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            ax = plt.subplot(111, polar=True)\n",
    "\n",
    "            categories = self.specified_metrics\n",
    "            N = len(categories)\n",
    "\n",
    "            angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "            angles += angles[:1]\n",
    "\n",
    "            ax.set_theta_offset(pi / 2)\n",
    "            ax.set_theta_direction(-1)\n",
    "\n",
    "            plt.xticks(angles[:-1], categories)\n",
    "\n",
    "            ax.set_rlabel_position(0)\n",
    "            plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\",\"0.4\",\"0.6\",\"0.8\"], color=\"black\", size=12)\n",
    "            plt.ylim(0,1)\n",
    "\n",
    "            values = [metric_values[metric] for metric in self.specified_metrics]\n",
    "            values += values[:1]\n",
    "            ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'{old_col} to {new_col}', color=color_map[idx])\n",
    "            ax.fill(angles, values, alpha=0.25, color=color_map[idx])\n",
    "\n",
    "            # Add error bars to the radar chart\n",
    "            conf_intervals = bootstrap_results[(old_col, new_col)]['conf_intervals']\n",
    "            for i, metric in enumerate(self.specified_metrics):\n",
    "                metric_value = metric_values[metric]\n",
    "                lower_bound = conf_intervals[metric][0]\n",
    "                upper_bound = conf_intervals[metric][1]\n",
    "                error = [[metric_value - lower_bound], [upper_bound - metric_value]]\n",
    "                ax.errorbar(angles[i], metric_value, yerr=error, fmt='o', color=color_map[idx], capsize=5)\n",
    "            \n",
    "            plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "            plt.title(f'Metrics for \"{old_col} to {new_col}\"', size=15, color='black', y=1.1)\n",
    "\n",
    "            if self.save_dir is not None:\n",
    "                radar_plots_subdir = \"radar_plots\"\n",
    "                os.makedirs(os.path.join(self.save_dir, radar_plots_subdir), exist_ok=True)\n",
    "                file_name_svg = f\"{old_col}_to_{new_col}_radar.svg\"\n",
    "                path_svg = os.path.join(self.save_dir, radar_plots_subdir, file_name_svg)\n",
    "                plt.savefig(path_svg, format='svg')\n",
    "            plt.show()\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        if self.specified_metrics is None:\n",
    "            self.specified_metrics = ['Sensitivity', 'Specificity', 'Precision', 'PPV', 'NPV', 'Accuracy', 'F1 Score']\n",
    "\n",
    "        plot_data = []\n",
    "        for (old_col, new_col), metric_values in self.metrics.items():\n",
    "            for metric_name, metric_value in metric_values.items():\n",
    "                if metric_name in self.specified_metrics:\n",
    "                    plot_data.append({\n",
    "                        'Mapping': f'{old_col} to {new_col}',\n",
    "                        'Metric': metric_name,\n",
    "                        'Value': metric_value\n",
    "                    })\n",
    "\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=\"Value\", y=\"Mapping\", hue=\"Metric\", data=plot_df)\n",
    "\n",
    "        plt.xlabel('Metric Value')\n",
    "        plt.ylabel('Column Mapping')\n",
    "        plt.title('Performance Metrics for Each Column Mapping')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        if self.save_dir is not None:\n",
    "            subdir = \"bar_plots\"\n",
    "            os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "            file_name_svg = f\"{old_col}_to_{new_col}_bar.svg\"\n",
    "            path_svg = os.path.join(self.save_dir, subdir, file_name_svg)\n",
    "            plt.savefig(path_svg, format='svg')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    def convert_metrics_to_dataframe(self):\n",
    "        \"\"\"\n",
    "        Converts the provided metrics dictionary into a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "        metrics_dict (dict): A dictionary where each key is a tuple containing two strings\n",
    "                            (categories) and each value is another dictionary containing\n",
    "                            various metrics.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: A DataFrame with the metrics organized in columns and categories in rows.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        # Convert the dictionary to a DataFrame\n",
    "        df = pd.DataFrame(self.metrics).T\n",
    "\n",
    "        # Setting the names for the multi-index and resetting it to make it part of the DataFrame\n",
    "        df.columns.name = 'Metric'\n",
    "        df.index.set_names(['Category', 'Subcategory'], inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "        \n",
    "        if self.save_dir is not None:\n",
    "            subdir = \"metrics_df\"\n",
    "            os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "            df.to_csv(os.path.join(self.save_dir, subdir, 'metrics.csv'))\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def lineplot_metrics(self):\n",
    "        # Convert metrics to DataFrame\n",
    "        metrics_df = self.convert_metrics_to_dataframe()\n",
    "        \n",
    "        # Set up the color palette\n",
    "        palette = sns.color_palette(\"tab10\", 5)\n",
    "        \n",
    "        # Initialize the plot\n",
    "        plt.figure(figsize=(6*len(metrics_df.index), 6))\n",
    "        \n",
    "        # Plot each metric\n",
    "        sns.lineplot(x='Category', y='Accuracy', data=metrics_df, marker='o', label='Accuracy', color=palette[0])\n",
    "        sns.lineplot(x='Category', y='Sensitivity', data=metrics_df, marker='o', label='Sensitivity', color=palette[1])\n",
    "        sns.lineplot(x='Category', y='Specificity', data=metrics_df, marker='o', label='Specificity', color=palette[2])\n",
    "        sns.lineplot(x='Category', y='PPV', data=metrics_df, marker='o', label='PPV', color=palette[3])\n",
    "        sns.lineplot(x='Category', y='NPV', data=metrics_df, marker='o', label='NPV', color=palette[4])\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.xlabel('Class', fontsize=20)\n",
    "        plt.ylabel('Classification Metric Score', fontsize=20)\n",
    "        plt.title('Classification Metrics Across Classes', fontsize=20)\n",
    "        \n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=16)\n",
    "        plt.legend(fontsize=16)\n",
    "        \n",
    "        plt.grid(False)\n",
    "        sns.despine()\n",
    "        \n",
    "        if self.save_dir is not None:\n",
    "            subdir = \"metrics_lineplot\"\n",
    "            os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "            file_name_svg = \"lineplot.svg\"\n",
    "            path_svg = os.path.join(self.save_dir, subdir, file_name_svg)\n",
    "            plt.savefig(path_svg, format='svg')\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "            \n",
    "    def plot_macro_averages(self):\n",
    "        # Convert metrics to DataFrame\n",
    "        metrics_df = self.convert_metrics_to_dataframe()\n",
    "        \n",
    "        # Calculate macro-averages and standard deviations\n",
    "        metric_names = ['Accuracy', 'Sensitivity', 'Specificity', 'PPV', 'NPV']\n",
    "        averages = metrics_df[metric_names].mean()\n",
    "        std_devs = metrics_df[metric_names].std()\n",
    "        print(\"Macro Averages: \", averages)\n",
    "        print(\"Macro Standard Deviations: \", std_devs)\n",
    "\n",
    "        # Create a DataFrame for plotting\n",
    "        macro_df = pd.DataFrame({\n",
    "            'Metric': metric_names,\n",
    "            'Average': averages,\n",
    "            'StdDev': std_devs\n",
    "        })\n",
    "\n",
    "        # Initialize the plot\n",
    "        plt.figure(figsize=(18, 6))\n",
    "\n",
    "        # Create bar plot with error bars\n",
    "        sns.barplot(x='Metric', y='Average', yerr=macro_df['StdDev'], data=macro_df, palette='tab10', capsize=0.5)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.xlabel('Metric', fontsize=20)\n",
    "        plt.ylabel('Macro-Average Score', fontsize=20)\n",
    "        plt.title('Macro-Average Classification Metrics with Standard Deviations', fontsize=20)\n",
    "        \n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        \n",
    "        plt.grid(False)\n",
    "        sns.despine()\n",
    "        \n",
    "        if self.save_dir is not None:\n",
    "            subdir = \"macro_averages\"\n",
    "            os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "            file_name_svg = \"macro_averages.svg\"\n",
    "            path_svg = os.path.join(self.save_dir, subdir, file_name_svg)\n",
    "            plt.savefig(path_svg, format='svg')\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def run_plotting(self):\n",
    "        self.plot_metrics()\n",
    "        self.plot_confusion_matrices()\n",
    "        self.plot_radar_charts()\n",
    "        self.lineplot_metrics()\n",
    "        if len(self.mapping_dict.values()) > 1:\n",
    "            self.plot_macro_averages()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from calvin_utils.statistical_utils.classification_statistics import BinaryDataMetricsPlotter\n",
    "plotter = BinaryDataMetricsPlotter(dataframe=data_df, mapping_dict=mapping_dict, specified_metrics=specified_metrics,\n",
    "                                   out_dir=out_dir, cm_normalization=normalization)\n",
    "plotter.run_plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.convert_metrics_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[:, ['Cognitively Intact', 'Cognitively Impaired', 'RoCA Score', 'Cognitive Status', 'Classification']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Evaluate a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "class BinaryClassificationEvaluation:\n",
    "    \"\"\"\n",
    "    Initializes the BinaryClassificationEvaluation with model results and the true outcomes.\n",
    "\n",
    "    Args:\n",
    "        fitted_model: The result object from a fitted statsmodels MNLogit model or similar.\n",
    "        observation_df: A pandas DataFrame with the true class outcomes.\n",
    "        normalization: Normalization method for the confusion matrix (None, 'true', 'pred', 'all').\n",
    "        predictions_df: DataFrame of predicted probabilities or values.\n",
    "        out_dir: Directory where plots and results will be saved.\n",
    "        threshold: The threshold to classify probabilities into binary outcomes.\n",
    "        positive_label: The label to be considered as the positive class. This is critical as the ROC curve calculations and thresholding will be based on this label.\n",
    "    \"\"\"\n",
    "    def __init__(self, fitted_model=None, observation_df=None, normalization=None, predictions_df=None, out_dir=None, threshold=None, positive_label=1):\n",
    "        self.results = fitted_model\n",
    "        self.observation_df = observation_df\n",
    "        self.normalization = normalization\n",
    "        self.predictions_df = predictions_df\n",
    "        self.out_dir = out_dir\n",
    "        self.threshold = threshold\n",
    "        self.positive_label = positive_label\n",
    "        \n",
    "    def find_optimal_threshold(self):\n",
    "        \"\"\"\n",
    "        Calculates the optimal threshold using Youden's J statistic from an ROC curve.\n",
    "\n",
    "        Returns:\n",
    "            float: The optimal threshold based on Youden's J statistic.\n",
    "        \"\"\"\n",
    "        if self.positive_label is None:\n",
    "            self.positive_label = 1\n",
    "\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true=self.observation_df, y_score=self.predictions_df, pos_label=self.positive_label)\n",
    "        youden_j = tpr - fpr\n",
    "\n",
    "        max_index = youden_j.argmax()\n",
    "        optimal_threshold = thresholds[max_index]\n",
    "        print(\"Optimal threshold by Youden's J: \", optimal_threshold)\n",
    "        \n",
    "        if optimal_threshold not in self.predictions_df.unique():\n",
    "            print('----- \\n WARNING: OPTIMAL THRESHOLD NOT IN OBSERVED SCORES. IF USING DISCRETE THRESHOLDS, THIS IS A PROBLEM AND SUGGESTS ARG:positive_label IS INCORRECT. \\n')\n",
    "        \n",
    "        if self.threshold is None:\n",
    "            self.threshold = optimal_threshold\n",
    "        return optimal_threshold, youden_j\n",
    "\n",
    "\n",
    "    def threshold_predictions(self, probabilities):\n",
    "        \"\"\"\n",
    "        Converts probabilities into binary predictions based on the threshold.\n",
    "\n",
    "        Args:\n",
    "            probabilities: Array of predicted probabilities.\n",
    "\n",
    "        Returns:\n",
    "            Array of binary predictions.\n",
    "        \"\"\"\n",
    "        print(f\"Positive cases are being set to those under: {self.threshold}. \\n Ensure this is logically sound.\")\n",
    "        return (probabilities < self.threshold).astype(int)\n",
    "\n",
    "    def get_predictions(self):\n",
    "        \"\"\"\n",
    "        Generates predictions from the fitted model or predictions DataFrame.\n",
    "        \"\"\"\n",
    "        if self.predictions_df is not None:\n",
    "            self.raw_predictions = self.predictions_df.to_numpy()\n",
    "        else:\n",
    "            self.raw_predictions = self.results.predict()\n",
    "        self.predictions = self.threshold_predictions(self.raw_predictions)\n",
    "\n",
    "    def get_observations(self):\n",
    "        \"\"\"\n",
    "        Converts the observation DataFrame into a flattened array.\n",
    "        \"\"\"\n",
    "        self.observations = self.observation_df.to_numpy().flatten()\n",
    "\n",
    "    def calculate_confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Calculates the confusion matrix from predictions and observations.\n",
    "        \"\"\"\n",
    "        self.conf_matrix = confusion_matrix(self.observations, self.predictions, normalize=self.normalization)\n",
    "\n",
    "    def extract_confusion_components(self):\n",
    "        \"\"\"\n",
    "        Extracts True Positive, True Negative, False Positive, and False Negative counts from the confusion matrix.\n",
    "        \"\"\"\n",
    "        self.TP = self.conf_matrix[1, 1]\n",
    "        self.TN = self.conf_matrix[0, 0]\n",
    "        self.FP = self.conf_matrix[0, 1]\n",
    "        self.FN = self.conf_matrix[1, 0]\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculates accuracy, sensitivity, specificity, PPV, and NPV based on the confusion matrix components.\n",
    "        \"\"\"\n",
    "        self.accuracy = (self.TP + self.TN) / (self.TP + self.TN + self.FP + self.FN)\n",
    "        self.sensitivity = self.TP / (self.TP + self.FN)\n",
    "        self.specificity = self.TN / (self.TN + self.FP)\n",
    "        self.PPV = self.TP / (self.TP + self.FP)\n",
    "        self.NPV = self.TN / (self.TN + self.FN)\n",
    "\n",
    "    def display_metrics(self):\n",
    "        print(\"Accuracy:\", self.accuracy)\n",
    "        print(\"Sensitivity:\", self.sensitivity)\n",
    "        print(\"Specificity:\", self.specificity)\n",
    "        print(\"PPV:\", self.PPV)\n",
    "        print(\"NPV:\", self.NPV)\n",
    "\n",
    "    def plot_confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Plots a heatmap of the confusion matrix.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(self.conf_matrix, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted label\")\n",
    "        plt.ylabel(\"True label\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        if self.out_dir:\n",
    "            os.makedirs(self.out_dir, exist_ok=True)\n",
    "            plt.savefig(os.path.join(self.out_dir, \"confusion_matrix.png\"))\n",
    "        plt.show()\n",
    "        \n",
    "    def bootstrap_confidence_intervals(self, n_bootstraps=1000, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculates confidence intervals for evaluation metrics using bootstrapping.\n",
    "\n",
    "        Args:\n",
    "            n_bootstraps: Number of bootstrap samples.\n",
    "            alpha: Significance level for confidence intervals.\n",
    "\n",
    "        Returns:\n",
    "            dict: Confidence intervals for AUC, accuracy, sensitivity, specificity, PPV, and NPV.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        bootstrapped_metrics = {\n",
    "            'auc': [],\n",
    "            'accuracy': [],\n",
    "            'sensitivity': [],\n",
    "            'specificity': [],\n",
    "            'ppv': [],\n",
    "            'npv': []\n",
    "        }\n",
    "\n",
    "        data = pd.DataFrame({'observations': self.observations, 'predictions': self.predictions, 'raw_predictions': self.raw_predictions.flatten()})\n",
    "        \n",
    "        for _ in range(n_bootstraps):\n",
    "            bootstrap_sample = data.sample(n=len(data), replace=True)\n",
    "            y_true = bootstrap_sample['observations'].to_numpy()\n",
    "            y_pred = bootstrap_sample['predictions'].to_numpy()\n",
    "            y_scores = bootstrap_sample['raw_predictions'].to_numpy()\n",
    "            \n",
    "            # AUC\n",
    "            fpr, tpr, _ = roc_curve(y_true, y_scores, pos_label=self.positive_label)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            bootstrapped_metrics['auc'].append(roc_auc)\n",
    "            \n",
    "            # Confusion matrix and derived metrics\n",
    "            conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "            TP = conf_matrix[1, 1]\n",
    "            TN = conf_matrix[0, 0]\n",
    "            FP = conf_matrix[0, 1]\n",
    "            FN = conf_matrix[1, 0]\n",
    "\n",
    "            accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "            sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "            specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "            ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            npv = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "\n",
    "            bootstrapped_metrics['accuracy'].append(accuracy)\n",
    "            bootstrapped_metrics['sensitivity'].append(sensitivity)\n",
    "            bootstrapped_metrics['specificity'].append(specificity)\n",
    "            bootstrapped_metrics['ppv'].append(ppv)\n",
    "            bootstrapped_metrics['npv'].append(npv)\n",
    "            \n",
    "        self.bootstrapped_metrics = bootstrapped_metrics\n",
    "        confidence_intervals = {}\n",
    "        for metric in bootstrapped_metrics:\n",
    "            sorted_metrics = np.sort(bootstrapped_metrics[metric])\n",
    "            lower_bound = np.percentile(sorted_metrics, alpha / 2 * 100)\n",
    "            upper_bound = np.percentile(sorted_metrics, (1 - alpha / 2) * 100)\n",
    "            confidence_intervals[metric] = (lower_bound, upper_bound)\n",
    "        return confidence_intervals\n",
    "    \n",
    "    def calculate_p_values(self, random_classifier_metrics):\n",
    "        \"\"\"\n",
    "        Calculates p-values for bootstrapped metrics compared to the random classifier's metrics.\n",
    "\n",
    "        Args:\n",
    "            bootstrapped_metrics: Dictionary containing lists of bootstrapped metric values.\n",
    "            random_classifier_metrics: Dictionary containing the random classifier's metric values.\n",
    "\n",
    "        Returns:\n",
    "            dict: P-values for each metric.\n",
    "        \"\"\"\n",
    "        p_values = {}\n",
    "        for metric in self.bootstrapped_metrics:\n",
    "            metric_values = self.bootstrapped_metrics[metric]\n",
    "            random_value = random_classifier_metrics[metric]\n",
    "            p_value = np.mean(np.array(metric_values) < random_value)\n",
    "            p_values[metric] = p_value\n",
    "        return p_values\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the calculation and display of all evaluation metrics.\n",
    "        \"\"\"\n",
    "        self.find_optimal_threshold()\n",
    "        self.get_predictions()\n",
    "        self.get_observations()\n",
    "        self.calculate_confusion_matrix()\n",
    "        self.extract_confusion_components()\n",
    "        self.calculate_metrics()\n",
    "        self.display_metrics()\n",
    "        self.plot_confusion_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __init__(self, fitted_model=None, observation_df=None, normalization=None, predictions_df=None, out_dir=None, threshold=0.5):\n",
    "bce = BinaryClassificationEvaluation(fitted_model=None, normalization='pred', \n",
    "                                     predictions_df=data_df['Classification'], observation_df=data_df['Cognitive_Status_Code'], \n",
    "                                     threshold=None, positive_label=0)\n",
    "bce.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the Optimal Threshold and Derive Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence intervals\n",
    "confidence_intervals = bce.bootstrap_confidence_intervals()\n",
    "print(\"Confidence Intervals:\", confidence_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get P-Values\n",
    "- The dictionary of random classifier baselines are provided in section 05\n",
    "    - {'Sensitivity': 0, 'Specificity': 0.5, 'Precision': 0.0, 'PPV': 0.0, 'NPV': 1.0, 'Accuracy': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline = {'Sensitivity': 0, 'Specificity': 0.5, 'Precision': 0.0, 'PPV': 0.0, 'NPV': 1.0, 'Accuracy': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline = {'sensitivity': 0, 'specificity': 0.5,  'ppv': 0.0, 'npv': 0.63, 'accuracy': 0.5, 'auc': 0.5}\n",
    "bce.calculate_p_values(random_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Evaluate a Multiclass Classifier\n",
    "- If you have ground truths and some sort of continuous variable which can be used for classification, this will help you visualize that.\n",
    "```\n",
    "Args:\n",
    "    fitted_model: The result object from a fitted statsmodels MNLogit model.\n",
    "    outcome_matrix: A pandas DataFrame with the true class outcomes in one-hot encoded format.\n",
    "    normalization: Normalization method for the confusion matrix (None, 'true', 'pred', 'all').\n",
    "    predictions_df (pd.DataFrame, Optional): If no fitted_model provided, pass a dataframe with predictions. \n",
    "        - can contain probabilities or dummy-coded predictions.\n",
    "    thresholds (dict, Optional): a dictionary mapping the index of the threshold to the probability threshold to make that classification. \n",
    "    assign_labels (bool, Optional): Scipy's confusion matrix orders by minimum to maximum occurence of the predictions. It will output the confusion matrix by this. \n",
    "            If set to False, we will organize our confusion matrix as per scipy's order. \n",
    "```\n",
    "- The ROC considers clasisfications acoss ALL POSSIBLE PROBABILITIES, demonstrating what is ultiamtely accomplishable at the best possible threshold\n",
    "\n",
    "- First curve is ROC for classifcation of each class with respect to all other classes\n",
    "- Second Curve (Macro Average) is basically a meta-analytic ROC with equal weight per class.\n",
    "- Third Curve (Micro Average) is basically a meta-analytic ROC with weight proportional to class sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have predictions and observations organized as:\n",
    "```\n",
    "Observations (columns are a given classificaiton)\n",
    "    [[0, 1, 0, 0],\n",
    "      1, 0 ,0 ,0]]\n",
    "predictions (columns are a given classification, value is the p(class))      \n",
    "    It will expect predictions dataframeto take the form where prediction for a given classificition is an array of probability:\n",
    "    [[0.2, 0.7, 0.1, 0.0],\n",
    "      0.9, 0.05, 0.05, 0.0]]\n",
    "```\n",
    "then shape them here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def create_observations_df(df, outcome_column):\n",
    "    # One-hot encode the binary outcomes\n",
    "    observations_df = pd.get_dummies(df[outcome_column])\n",
    "    return observations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Observations DF Prepared**\n",
    "\n",
    "Enter the column which has ground-truth in it. Can be numeric or string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_column = 'DX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_df = create_observations_df(data_df, 'DX')\n",
    "observations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option A - Generate a Predictions DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = data_df.loc[:, observations_df.columns] # set columns to equal observations of observations_df. \n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option B - Fit a Multinomial Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go use calvin_utils_project/notebooks/statistical_notebooks/regression_notebooks/logistic_regression_notebook.ipynb to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.classification_statistics import ComprehensiveMulticlassROC\n",
    "evaluator = ComprehensiveMulticlassROC(fitted_model=None, observation_df=observations_df, predictions_df=predictions_df, normalization='true',\n",
    "                                     thresholds=None, out_dir=out_dir)\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Evaluate a Random Classifier\n",
    "- Set the mapping_dict so keys are columns and values are the positive hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'Cognitive_Status':'Correct'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from calvin_utils.statistical_utils.classification_statistics import BinaryDataMetricsPlotter\n",
    "\n",
    "class EvaluateRandomClassifier(BinaryDataMetricsPlotter):\n",
    "    def __init__(self, dataframe, mapping_dict, out_dir=None, cm_normalization=None, n_classes=2):\n",
    "        '''\n",
    "        mapping_dict (dict): a dicitonary where keys represent the column with observed classes. Keys are the observation corresponding to a hit. \n",
    "        '''\n",
    "        self.dataframe = dataframe\n",
    "        self.mapping_dict = mapping_dict\n",
    "        self.specified_metrics = None\n",
    "        self.save_dir = out_dir\n",
    "        self.n_classes=n_classes\n",
    "        self.metrics = self.calculate_metrics()\n",
    "        self.confusion_matrices = self.get_confusion_matrices(normalize=cm_normalization)\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        metrics = {}\n",
    "        for class_col, positive_class in self.mapping_dict.items():\n",
    "            if len(self.dataframe[class_col].unique()) != 2:\n",
    "                raise ValueError(\"Only 2 classes acceptable in this class. Do not pass columns with more than 2 classes.\")\n",
    "            tp = 1/self.n_classes * (self.dataframe[class_col]==positive_class).sum() / (self.dataframe[class_col]).count()\n",
    "            tn = (1 - 1/self.n_classes) * (self.dataframe[class_col]!=positive_class).sum() / (self.dataframe[class_col]).count()\n",
    "            fp = 1/self.n_classes * (self.dataframe[class_col]!=positive_class).sum() / (self.dataframe[class_col]).count()\n",
    "            fn = (1 - 1/self.n_classes) * (self.dataframe[class_col]==positive_class).sum() / (self.dataframe[class_col]).count()\n",
    "            \n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "            ppv = tp / (tp + fp) if (tp + fp) != 0 else 0  # Positive Predictive Value\n",
    "            npv = tn / (tn + fn) if (tn + fn) != 0 else 0  # Negative Predictive Value\n",
    "            acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "            precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "            recall = sensitivity  # Recall is the same as sensitivity\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            metrics[class_col] = {\n",
    "                'Sensitivity': sensitivity,\n",
    "                'Specificity': specificity,\n",
    "                'Precision': precision,\n",
    "                'PPV': ppv,\n",
    "                'NPV': npv,\n",
    "                'Accuracy': acc,\n",
    "                'F1 Score': f1,\n",
    "                'TP': tp,\n",
    "                'TN': tn,\n",
    "                'FP': fp,\n",
    "                'FN': fn\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def get_confusion_matrices(self, normalize=None):\n",
    "        confusion_matrices = {}\n",
    "        for class_col in self.mapping_dict.keys():\n",
    "            tp = self.metrics[class_col]['TP']\n",
    "            tn = self.metrics[class_col]['TN']\n",
    "            fp = self.metrics[class_col]['FP']\n",
    "            fn = self.metrics[class_col]['FN']\n",
    "            cm = np.array([[tn, fp], [fn, tp]])\n",
    "            if normalize == 'true':\n",
    "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            elif normalize == 'pred':\n",
    "                cm = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]\n",
    "            elif normalize == 'all':\n",
    "                cm = cm.astype('float') / cm.sum()\n",
    "            else: \n",
    "                pass\n",
    "            confusion_matrices[class_col] = cm\n",
    "        return confusion_matrices\n",
    "    \n",
    "    def plot_confusion_matrices(self):\n",
    "        confusion_matrices = self.confusion_matrices\n",
    "        num_matrices = len(confusion_matrices)\n",
    "        fig, axes = plt.subplots(1, num_matrices, figsize=(6 * num_matrices, 6))\n",
    "        \n",
    "        if num_matrices == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for ax, (class_col, cm) in zip(axes, confusion_matrices.items()):\n",
    "            sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', ax=ax,\n",
    "                        xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'],\n",
    "                        annot_kws={\"size\": 16})  # Set annotation font size\n",
    "            ax.set_xlabel(f'Predicted: {self.mapping_dict[class_col]}', fontsize=16)\n",
    "            ax.set_ylabel(f'Actual: {class_col}', fontsize=16)\n",
    "            ax.set_title(f'Confusion Matrix for {class_col} vs {self.mapping_dict[class_col]}', fontsize=16)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "            \n",
    "        if self.save_dir is not None:\n",
    "            subdir = \"confusion_matrix\"\n",
    "            os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "            file_name_svg = \"conf_matrix.svg\"\n",
    "            path_svg = os.path.join(self.save_dir, subdir, file_name_svg)\n",
    "            plt.savefig(path_svg, format='svg')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_probability_influence(self):\n",
    "        \"\"\"\n",
    "        Visualize how the predicted probabilities influence the random classifier.\n",
    "        \"\"\"\n",
    "        for class_col, metrics in self.metrics.items():\n",
    "            # Extract the required metrics\n",
    "            tp = int(metrics['TP'])\n",
    "            tn = int(metrics['TN'])\n",
    "            fp = int(metrics['FP'])\n",
    "            fn = int(metrics['FN'])\n",
    "\n",
    "            # Simulate probabilities based on the metric values\n",
    "            total = tp + tn + fp + fn\n",
    "            pred_prob = np.random.rand(total)\n",
    "            true_prob = np.concatenate((\n",
    "                np.repeat(1, tp + fn),  # True positives and false negatives\n",
    "                np.repeat(0, tn + fp)   # True negatives and false positives\n",
    "            ))\n",
    "\n",
    "            # Simulate true labels based on probabilities\n",
    "            true_labels = np.concatenate((\n",
    "                np.repeat(1, tp), np.repeat(0, fn),\n",
    "                np.repeat(0, tn), np.repeat(1, fp)\n",
    "            ))\n",
    "\n",
    "            # Determine predicted labels based on a threshold of 0.5 for visualization purposes\n",
    "            pred_labels = (pred_prob > 0.5).astype(int)\n",
    "\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            # Determine color based on correct classification\n",
    "            correct_classification = true_labels == pred_labels\n",
    "            colors = np.array(sns.color_palette(\"tab10\"))[correct_classification.astype(int)]\n",
    "\n",
    "            # Scatter plot\n",
    "            scatter = plt.scatter(pred_prob, true_prob, c=colors, alpha=0.6, edgecolor='w', linewidth=0.5)\n",
    "\n",
    "            # Colorbar settings\n",
    "            cbar = plt.colorbar(scatter, boundaries=[0, 0.5, 1], ticks=[0.25, 0.75])\n",
    "            cbar.set_ticklabels(['Incorrect', 'Correct'])\n",
    "            cbar.set_label('Classification Result', fontsize=12)\n",
    "\n",
    "            # Plot settings\n",
    "            plt.xlabel(\"Predicted Probability of Positive Class\", fontsize=14)\n",
    "            plt.ylabel(\"True Probability of Positive Class\", fontsize=14)\n",
    "            plt.title(f\"Probability Influence on Random Classifier for {class_col}\", fontsize=16)\n",
    "\n",
    "            # Grid and layout settings\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Show the plot\n",
    "            if self.save_dir is not None:\n",
    "                subdir = \"probability_influence\"\n",
    "                os.makedirs(os.path.join(self.save_dir, subdir), exist_ok=True)\n",
    "                file_name_svg = f\"{class_col}_probability_influence.svg\"\n",
    "                path_svg = os.path.join(self.save_dir, subdir, file_name_svg)\n",
    "                plt.savefig(path_svg, format='svg')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    def plot_radar_charts(self):\n",
    "        if self.specified_metrics is None:\n",
    "            self.specified_metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'PPV', 'NPV']\n",
    "        \n",
    "        color_map = sns.color_palette(\"tab10\", len(self.mapping_dict))\n",
    "\n",
    "        for idx, (class_col, metric_values) in enumerate(self.metrics.items()):\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            ax = plt.subplot(111, polar=True)\n",
    "\n",
    "            categories = self.specified_metrics\n",
    "            N = len(categories)\n",
    "\n",
    "            angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "            angles += angles[:1]\n",
    "\n",
    "            ax.set_theta_offset(pi / 2)\n",
    "            ax.set_theta_direction(-1)\n",
    "\n",
    "            plt.xticks(angles[:-1], categories)\n",
    "\n",
    "            ax.set_rlabel_position(0)\n",
    "            plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\", \"0.4\", \"0.6\", \"0.8\"], color=\"black\", size=12)\n",
    "            plt.ylim(0, 1)\n",
    "\n",
    "            values = [metric_values[metric] for metric in self.specified_metrics]\n",
    "            values += values[:1]\n",
    "            ax.plot(angles, values, linewidth=1, linestyle='solid', label=f'{class_col}', color=color_map[idx % len(color_map)])\n",
    "            ax.fill(angles, values, alpha=0.25, color=color_map[idx % len(color_map)])\n",
    "\n",
    "            plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "            plt.title(f'Metrics for \"{class_col}\"', size=15, color='black', y=1.1)\n",
    "\n",
    "            if self.save_dir is not None:\n",
    "                radar_plots_subdir = \"radar_plots\"\n",
    "                os.makedirs(os.path.join(self.save_dir, radar_plots_subdir), exist_ok=True)\n",
    "                file_name_svg = f\"{class_col}_radar.svg\"\n",
    "                path_svg = os.path.join(self.save_dir, radar_plots_subdir, file_name_svg)\n",
    "                plt.savefig(path_svg, format='svg')\n",
    "            plt.show()\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "    def run(self):\n",
    "        self.plot_confusion_matrices()\n",
    "        self.plot_radar_charts()\n",
    "        self.visualize_probability_influence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_classifier = EvaluateRandomClassifier(dataframe=data_df, mapping_dict=mapping_dict, out_dir=None, cm_normalization=None,\n",
    "                                                    n_classes=4)\n",
    "random_classifier.run()\n",
    "print(random_classifier.metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Scatterplot a Value to a Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import expit\n",
    "\n",
    "# Define logistic function\n",
    "def logistic_function(x, beta_0, beta_1):\n",
    "    return expit(beta_0 + beta_1 * x)\n",
    "\n",
    "# Function to create scatter plot and fit sigmoid using logistic regression\n",
    "def plot_with_logistic_regression(df, dv_col, iv_col):\n",
    "    # Extract data\n",
    "    x = df[iv_col].values.reshape(-1, 1)\n",
    "    y = df[dv_col].values\n",
    "\n",
    "    # Normalize the dependent variable to be between 0 and 1\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    y_normalized = (y - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x, y_normalized)\n",
    "\n",
    "    # Create fitted sigmoid curve data\n",
    "    x_fit = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)\n",
    "    y_fit = logistic_function(x_fit, model.intercept_[0], model.coef_[0][0])\n",
    "    \n",
    "    # Create a JointGrid for the scatter plot and KDEs\n",
    "    g = sns.JointGrid(x=df[iv_col], y=y_normalized, space=0, height=8, ratio=5)\n",
    "    g.plot_joint(sns.scatterplot, color=\"tab:blue\", alpha=0.6)\n",
    "    g.plot_joint(plt.plot, x_fit, y_fit, color='blue', label='Logistic Fit')\n",
    "    g.plot_marginals(sns.kdeplot, fill=True, color='tab:blue')\n",
    "\n",
    "    # Labels and title\n",
    "    g.set_axis_labels(iv_col, f'Normalized {dv_col}', fontsize=14)\n",
    "    plt.suptitle(f'Scatter Plot with Logistic Regression Fit and KDE: {iv_col} vs {dv_col}', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()\n",
    "\n",
    "    # Plot\n",
    "    # plt.figure(figsize=(6, 6))\n",
    "    # sns.scatterplot(x=x.flatten(), y=y_normalized, palette=\"tab10\", label=None)\n",
    "    # plt.plot(x_fit, y_fit, color='blue', label='Logistic Fit')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import expit\n",
    "import os\n",
    "# Define logistic function\n",
    "def logistic_function(x, beta_0, beta_1):\n",
    "    return expit(beta_0 + beta_1 * x)\n",
    "\n",
    "# Function to create scatter plot and fit sigmoid using logistic regression with KDE plots\n",
    "def plot_with_logistic_regression_and_kde(df, dv_col, iv_col, xlim=None, ylim=None, out_dir=None):\n",
    "    # Extract data\n",
    "    x = df[iv_col].values.reshape(-1, 1)\n",
    "    y = df[dv_col].values\n",
    "\n",
    "    # Normalize the dependent variable to be between 0 and 1\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    y_normalized = (y - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Fit logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x, y_normalized)\n",
    "\n",
    "    # Create fitted sigmoid curve data\n",
    "    x_fit = np.linspace(x.min(), x.max(), 100).reshape(-1, 1)\n",
    "    y_fit = logistic_function(x_fit, model.intercept_[0], model.coef_[0][0])\n",
    "\n",
    "    # Create a JointGrid for the scatter plot and KDEs\n",
    "    g = sns.JointGrid(x=df[iv_col], y=y_normalized, space=0, height=8, ratio=5)\n",
    "    g.plot_joint(sns.scatterplot, alpha=0.6)\n",
    "    g.plot_marginals(sns.kdeplot, fill=True, color='tab:blue')\n",
    "    \n",
    "    # Plot the logistic fit line separately\n",
    "    g.ax_joint.plot(x_fit, y_fit, color='blue', label='Logistic Fit')\n",
    "\n",
    "    # Labels and title\n",
    "    g.set_axis_labels(iv_col, f'{dv_col}', fontsize=16)\n",
    "    plt.suptitle(f'Scatter Plot with Logistic Regression Fit and KDE:\\n{iv_col} vs {dv_col}', fontsize=20)\n",
    "\n",
    "    # plt.subplots_adjust(top=0.95)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.tight_layout()\n",
    "    plt.yticks([0,1])\n",
    "    if xlim:\n",
    "        g.ax_joint.set_xlim(xlim)\n",
    "    if ylim:\n",
    "        g.ax_joint.set_ylim(ylim)\n",
    "\n",
    "    # Show the plot\n",
    "    if out_dir is not None:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(out_dir, 'sigmoid_scatter.svg'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/roca/figures/cognitive_classifier/clock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_logistic_regression_and_kde(data_df, dv_col='Cognitively Intact', iv_col='Clock Prediction', ylim=(-0.05, 1.05), xlim=(-0.05,1.05), out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create jitter plot with KDE plots\n",
    "def plot_jitter_with_kde(df, category_col, value_col):\n",
    "    # Create a FacetGrid\n",
    "    g = sns.FacetGrid(df, col=category_col, col_wrap=4, sharex=False, sharey=False, height=4)\n",
    "    \n",
    "    # Map the stripplot (jitter plot) to the FacetGrid\n",
    "    g.map(sns.stripplot, value_col, jitter=True, alpha=0.6, color=\"tab:blue\")\n",
    "    \n",
    "    # Map the KDE plot to the FacetGrid\n",
    "    g.map(sns.kdeplot, value_col, fill=True, color=\"tab:blue\", alpha=0.6)\n",
    "    \n",
    "    # Adjust the layout\n",
    "    g.set_axis_labels(value_col, '')\n",
    "    g.set_titles(col_template='{col_name}')\n",
    "    g.fig.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(f'Jitter Plot with KDE: {value_col} across {category_col}', fontsize=16)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_jitter_with_kde(data_df, 'Cognitively Intact', 'Clock Prediction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def bootstrap_metrics(data_df, true_col, pred_col, n_iterations=1000):\n",
    "    def calculate_metrics(true, pred):\n",
    "        accuracy = accuracy_score(true, pred)\n",
    "        sensitivity = recall_score(true, pred, zero_division=0)  # Sensitivity is the same as recall\n",
    "        specificity = recall_score(true, pred, pos_label=0, zero_division=0)\n",
    "        precision = precision_score(true, pred, zero_division=0)\n",
    "        f1 = f1_score(true, pred, zero_division=0)\n",
    "        return accuracy, sensitivity, specificity, precision, f1\n",
    "    # Suppress specific warnings\n",
    "        \n",
    "    # Observed metrics\n",
    "    obs_metrics = calculate_metrics(data_df[true_col], data_df[pred_col])\n",
    "    \n",
    "    # Bootstrapping\n",
    "    bootstrap_metrics = []\n",
    "    for _ in range(n_iterations):\n",
    "        boot_df = resample(data_df)\n",
    "        metrics = calculate_metrics(boot_df[true_col], boot_df[pred_col])\n",
    "        bootstrap_metrics.append(metrics)\n",
    "    \n",
    "    bootstrap_metrics = np.array(bootstrap_metrics)\n",
    "    \n",
    "    # Confidence intervals\n",
    "    conf_intervals = np.percentile(bootstrap_metrics, [2.5, 97.5], axis=0)\n",
    "    \n",
    "    # Print observed metrics and confidence intervals\n",
    "    print(f\"Observed Metrics:\\nAccuracy: {obs_metrics[0]}, Sensitivity: {obs_metrics[1]}, Specificity: {obs_metrics[2]}, Precision: {obs_metrics[3]}, F1 Score: {obs_metrics[4]}\")\n",
    "    print(f\"\\nConfidence Intervals (95%):\")\n",
    "    print(f\"Accuracy: {conf_intervals[:, 0]}\")\n",
    "    print(f\"Sensitivity: {conf_intervals[:, 1]}\")\n",
    "    print(f\"Specificity: {conf_intervals[:, 2]}\")\n",
    "    print(f\"Precision: {conf_intervals[:, 3]}\")\n",
    "    print(f\"F1 Score: {conf_intervals[:, 4]}\")\n",
    "\n",
    "# Example usage\n",
    "bootstrap_metrics(data_df, 'cube_actual', 'cube_prediction')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
