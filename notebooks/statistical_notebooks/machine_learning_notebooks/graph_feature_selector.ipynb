{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/CALVIN_FANCY_ANALYSIS/figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/CALVIN_FANCY_ANALYSIS/data.csv'\n",
    "sheet = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['GOSEbin','impact_lab_prob']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Cohort'  # The column you'd like to evaluate\n",
    "condition = 'equal'  # The condition to check ('equal', 'above', 'below', 'not')\n",
    "value = 0 # The value to drop if found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['DX_M12'] # ['Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group', 'Z_Scored_Subiculum_T_By_Origin_Group_'] #['Age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Generate Formulas to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this code is specifically to whittle down sam's massive amount of data. \n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming data_df is your dataframe\n",
    "# Ensure 'GOSEbin' is present in the dataframe\n",
    "if 'GOSEbin' not in data_df.columns:\n",
    "    raise KeyError(\"GOSEbin not found in dataframe columns.\")\n",
    "\n",
    "# Calculate Spearman correlation for each variable with 'GOSEbin'\n",
    "correlations = []\n",
    "for column in data_df.columns:\n",
    "    if column != 'GOSEbin':\n",
    "        # Calculate Spearman correlation\n",
    "        rho, _ = stats.spearmanr(data_df[column], data_df['GOSEbin'])\n",
    "        correlations.append((column, rho))\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "correlation_df = pd.DataFrame(correlations, columns=['Variable', 'Spearman_Rho'])\n",
    "\n",
    "# Plot the correlations using barplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Variable', y='Spearman_Rho', data=correlation_df, palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Spearman Correlation of Each Variable with GOSEbin')\n",
    "plt.ylabel('Spearman Rho')\n",
    "plt.xlabel('Variables')\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming data_df is your dataframe\n",
    "# Ensure 'GOSEbin' is present in the dataframe\n",
    "if 'GOSEbin' not in data_df.columns:\n",
    "    raise KeyError(\"GOSEbin not found in dataframe columns.\")\n",
    "\n",
    "# Calculate Spearman correlation for each variable with 'GOSEbin'\n",
    "significant_columns = []\n",
    "for column in data_df.columns:\n",
    "    if column != 'GOSEbin':\n",
    "        # Calculate Spearman correlation\n",
    "        rho, _ = stats.spearmanr(data_df[column], data_df['GOSEbin'])\n",
    "        # Check if absolute value of rho is greater than 0.15\n",
    "        if abs(rho) > 0.15:\n",
    "            significant_columns.append(column)\n",
    "\n",
    "# Return the list of significant columns\n",
    "len(significant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "data_df['GOSEbin'] = pd.to_numeric(data_df['GOSEbin'], errors='coerce')\n",
    "\n",
    "# Step 1: Regress impact_lab_prob out of GOSEbin\n",
    "X = sm.add_constant(data_df['impact_lab_prob'])\n",
    "model = sm.OLS(data_df['GOSEbin'], X).fit()\n",
    "residuals = model.resid\n",
    "\n",
    "# Step 2: Calculate Spearman correlation for each variable with the residuals\n",
    "significant_columns_after = []\n",
    "for column in data_df.columns:\n",
    "    if column not in ['GOSEbin', 'impact_lab_prob']:\n",
    "        rho, _ = stats.spearmanr(data_df[column], residuals)\n",
    "        if abs(rho) > 0.15:\n",
    "            significant_columns_after.append(column)\n",
    "\n",
    "# Step 3: Calculate the intersection with original significant columns\n",
    "\n",
    "# Intersection of new significant columns with old ones\n",
    "new_significant_columns = list(set(significant_columns_after) - set(significant_columns))\n",
    "\n",
    "# Display new columns that popped out\n",
    "new_significant_columns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Step 4: Calculate the Spearman correlation values for the new significant columns\n",
    "correlation_values = {}\n",
    "for column in new_significant_columns:\n",
    "    rho, _ = stats.spearmanr(data_df[column], residuals)\n",
    "    correlation_values[column] = rho\n",
    "\n",
    "# Convert the correlation values to a dataframe for easy plotting\n",
    "correlation_df = pd.DataFrame.from_dict(correlation_values, orient='index', columns=['Spearman Correlation'])\n",
    "\n",
    "# Plot the barplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=correlation_df.index, y='Spearman Correlation', data=correlation_df)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Spearman Correlation of New Significant Columns with GOSEbin Residuals')\n",
    "plt.ylabel('Spearman Correlation')\n",
    "plt.xlabel('Columns')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'GOSEbin'\n",
    "indep_var_list = new_significant_columns + significant_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all possible regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.ml_utils.graph_feature_selector.loocv import FormulaGenerator\n",
    "formulae = FormulaGenerator.generate_all_formulas(predictors=indep_var_list, dep_var=dep_var, max_predictors_in_model=3, max_interaction_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(formulae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.ml_utils.graph_feature_selector.loocv_manager import LOOCVManager\n",
    "performance_df = LOOCVManager(data_df, formulae, model_type='binomial_logit').run_all(multiprocess=False)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your hard-fought data if you'd like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.to_csv('/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/CALVIN_FANCY_ANALYSIS/computed_results_2lvl.csv')\n",
    "import pandas as pd\n",
    "# performance_df = pd.read_csv('/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/CALVIN_FANCY_ANALYSIS/computed_results_2lvl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get their p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.ml_utils.graph_feature_selector.loocv_perm import LOOCVPermutationTester\n",
    "performance_df = LOOCVPermutationTester(data_df=data_df, results_df=performance_df, \n",
    "                           model_type='binomial_logit', num_permutations=100, \n",
    "                           multiprocess=True, max_workers=None).run()\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your hard-fought data if you'd like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "performance_df = pd.read_csv('/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/CALVIN_FANCY_ANALYSIS/computed_results_p.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove non-significant clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = performance_df[performance_df['Entropy'] < 0.70]\n",
    "display(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Cluster Out Highly Performing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class PerformanceClusterer:\n",
    "    def __init__(self, performance_df, num_clusters=5):\n",
    "        \"\"\"\n",
    "        Initializes the PerformanceClusterer with performance data and clustering parameters.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        performance_df : pd.DataFrame\n",
    "            DataFrame containing columns: R2, RMSE, Entropy, Average_AIC, Formula, p-value.\n",
    "        num_clusters : int, optional, default=5\n",
    "            The number of clusters to form.\n",
    "        \"\"\"\n",
    "        self.original_df = performance_df.copy()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.clustered_df = None  # To store the DataFrame with cluster labels\n",
    "        self._prepare_data()\n",
    "        self._perform_clustering()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the data for clustering by selecting relevant features and handling NaNs.\n",
    "        \"\"\"\n",
    "        # Select relevant columns: choose Entropy if available; otherwise, RMSE\n",
    "        self.original_df['Metric'] = self.original_df.apply(\n",
    "            lambda row: row['Entropy'] if pd.notna(row['Entropy']) else row['RMSE'], axis=1\n",
    "        )\n",
    "        \n",
    "        # Select features: R2, Metric, Average_AIC\n",
    "        if self.original_df['RMSE'].notna().sum() != 0:\n",
    "            self.features = self.original_df[['R2', 'RMSE', 'Average_AIC']].copy()\n",
    "        else:\n",
    "            self.features = self.original_df[['R2', 'Entropy', 'Average_AIC']].copy()\n",
    "        \n",
    "        # Drop rows with any NaNs in the selected features\n",
    "        self.features = self.features.dropna()\n",
    "        self.clustered_df = self.original_df.loc[self.features.index].reset_index(drop=True)\n",
    "        self.features = self.features.reset_index(drop=True)\n",
    "        \n",
    "        # Standardize the features\n",
    "        self.scaled_features = StandardScaler().fit_transform(self.features)\n",
    "    \n",
    "    def _perform_clustering(self):\n",
    "        \"\"\"\n",
    "        Applies Agglomerative Clustering to the standardized features and assigns cluster labels.\n",
    "        \"\"\"\n",
    "        clustering = AgglomerativeClustering(n_clusters=self.num_clusters, affinity='euclidean', linkage='ward')\n",
    "        self.clustered_df['Cluster'] = clustering.fit_predict(self.scaled_features)\n",
    "    \n",
    "    def get_clustered_data(self):\n",
    "        \"\"\"\n",
    "        Returns the clustered DataFrame with an added 'cluster' column.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing original performance metrics along with cluster labels.\n",
    "        \"\"\"\n",
    "        return self.clustered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = PerformanceClusterer(performance_df=performance_df, num_clusters=3).get_clustered_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Number of Interactions in Each Formula\n",
    "\n",
    "```\n",
    "\n",
    "        def count_predictors(formula):\n",
    "            right_side = formula.split(\"~\")[1].strip()\n",
    "\n",
    "            # Check for both '*' and '+' in the formula\n",
    "            if '*' in right_side and '+' in right_side:\n",
    "                terms = []\n",
    "                for term in right_side.split('+'):\n",
    "                    terms.extend(term.split('*'))\n",
    "                return len(terms)\n",
    "\n",
    "            # Check for '*' in the formula\n",
    "            elif '*' in right_side:\n",
    "                return len(right_side.split('*'))\n",
    "\n",
    "            # Check for '+' in the formula\n",
    "            elif '+' in right_side:\n",
    "                return len(right_side.split('+'))\n",
    "\n",
    "            # If neither exists, it means there's only one predictor\n",
    "            else:\n",
    "                return 1\n",
    "                ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class InteractionCounter:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def count_interactions(self, formula):\n",
    "        \"\"\"\n",
    "        Counts the number of variables involved in the interaction term.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        formula : str\n",
    "            The formula string to parse.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        int\n",
    "            Number of variables in the formula.\n",
    "        \"\"\"\n",
    "        # Remove whitespace and split the formula by '*'\n",
    "        variables = re.split(r'\\*', formula.replace(' ', ''))\n",
    "        # Clean up variable names by removing any non-alphanumeric characters\n",
    "        variables = [re.sub(r'[^a-zA-Z0-9_]', '', var) for var in variables]\n",
    "        # Return the number of unique variables\n",
    "        return len(set(variables))\n",
    "\n",
    "    def apply_interaction_count(self):\n",
    "        \"\"\"\n",
    "        Applies the count_interactions method to the 'Formula' column in the DataFrame\n",
    "        and creates a new column 'Interaction_Level' with the count of variables.\n",
    "        \"\"\"\n",
    "        self.df['Interaction_Level'] = self.df['Formula'].apply(self.count_interactions)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = InteractionCounter(performance_df)\n",
    "performance_df = counter.apply_interaction_count()\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.Interaction_Level.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "\n",
    "class PerformancePlotter:\n",
    "    def __init__(self, clustered_df):\n",
    "        \"\"\"\n",
    "        Initializes the PerformancePlotter with clustered performance data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        clustered_df : pd.DataFrame\n",
    "            DataFrame containing columns: R2, RMSE, Entropy, Average_AIC, Formula, p-value, cluster.\n",
    "        \"\"\"\n",
    "        self.df = clustered_df.copy()\n",
    "     \n",
    "    def _get_significant_points(self):\n",
    "        if 'p-value' in self.df.columns:\n",
    "            self.df['significant'] = self.df['p-value'] < 0.05\n",
    "            significant_points = self.df[self.df['significant']]\n",
    "            non_significant_points = self.df[~self.df['significant']]\n",
    "        else:\n",
    "            significant_points = None\n",
    "            non_significant_points = self.df\n",
    "        return significant_points, non_significant_points\n",
    "\n",
    "    def create_hover_text(self, df):\n",
    "        \"\"\"\n",
    "        Creates custom hover text for each data point.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            The DataFrame slice for which to create hover text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            A list of hover text strings.\n",
    "        \"\"\"\n",
    "        hover_text = []\n",
    "        for index, row in df.iterrows():\n",
    "            texts = [\n",
    "                f\"Formula: {row['Formula']}\",\n",
    "                f\"R²: {row['R2']:.2f}\",\n",
    "                f\"Entropy/RMSE: {row['Entropy'] if 'Entropy' in row else row['RMSE']}\",\n",
    "                f\"Average AIC: {row['Average_AIC']}\",\n",
    "                f\"Cluster: {row['Cluster']}\"\n",
    "            ]\n",
    "            if 'p-value' in df.columns:\n",
    "                texts[\"p-value\"] = row['p-value']\n",
    "            hover_text.append('<br>'.join(texts))\n",
    "        return hover_text\n",
    "\n",
    "    def visualize_3D_scatter(self):\n",
    "        # Create symbol mapping for Interaction_Level\n",
    "        symbol_sequence = ['circle', 'square', 'diamond', 'cross', 'x', 'triangle-up', 'triangle-down', 'star', 'hexagram']\n",
    "        interaction_levels = sorted(self.df['Interaction_Level'].unique())\n",
    "        symbol_map = {level: symbol_sequence[(level - 1) % len(symbol_sequence)] for level in interaction_levels}\n",
    "        self.df['symbol'] = self.df['Interaction_Level'].map(symbol_map)\n",
    "\n",
    "        significant_points, non_significant_points = self._get_significant_points()\n",
    "        marker_size = 10\n",
    "\n",
    "        # Initialize the figure\n",
    "        fig = go.Figure()\n",
    "        # Add non-significant points\n",
    "        hover_text_non_sig = self.create_hover_text(non_significant_points)\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=non_significant_points['R2'],\n",
    "                y=non_significant_points['Entropy'] if 'Entropy' in self.df.columns else non_significant_points['RMSE'],\n",
    "                z=non_significant_points['Average_AIC'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    symbol=non_significant_points['symbol'],\n",
    "                    size=5,\n",
    "                    color=non_significant_points['Cluster'],\n",
    "                    colorscale='Viridis',\n",
    "                    line=dict(width=0),\n",
    "                    opacity=0.6\n",
    "                ),\n",
    "                text=hover_text_non_sig,\n",
    "                hoverinfo='text'\n",
    "            )\n",
    "        )\n",
    "        if significant_points is not None:\n",
    "            # Add significant points with outline\n",
    "            hover_text_sig = self.create_hover_text(significant_points)\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=significant_points['R2'],\n",
    "                    y=significant_points['Entropy'] if 'Entropy' in self.df.columns else significant_points['RMSE'],\n",
    "                    z=significant_points['Average_AIC'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        symbol=significant_points['symbol'],\n",
    "                        size=significant_points['marker_size'],\n",
    "                        color=significant_points['Cluster'],\n",
    "                        colorscale='Viridis',\n",
    "                        line=dict(width=50, color='black'),\n",
    "                        opacity=1.0\n",
    "                    ),\n",
    "                    text=hover_text_sig,\n",
    "                    hoverinfo='text'\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=\"3D Scatter Plot of Model Performance Metrics with Interaction Levels and Significance\",\n",
    "            scene=dict(\n",
    "                xaxis=dict(title='R²'),\n",
    "                yaxis=dict(title='Entropy' if 'Entropy' in self.df.columns else 'RMSE'),\n",
    "                zaxis=dict(title='Average AIC')\n",
    "            ),\n",
    "            legend_title_text='Cluster',\n",
    "            height=800\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "        return fig \n",
    "    \n",
    "    \n",
    "    def save_plotly_figure_as_html(self, fig, filename='3D_scatter_plot.html'):\n",
    "        \"\"\"\n",
    "        Save a Plotly figure as an HTML file.\n",
    "\n",
    "        Parameters:\n",
    "        - fig: Plotly figure object to be saved.\n",
    "        - filename: Name of the HTML file to save the figure. Default is '3D_scatter_plot.html'.\n",
    "        \"\"\"\n",
    "        fig.write_html(filename)\n",
    "\n",
    "\n",
    "    def get_plot_data(self):\n",
    "        \"\"\"\n",
    "        Returns the DataFrame prepared for plotting.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing performance metrics, cluster labels, and plotting attributes.\n",
    "        \"\"\"\n",
    "        return self.df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = PerformancePlotter(performance_df)\n",
    "fig = plotter.visualize_3D_scatter()\n",
    "plotter.save_plotly_figure_as_html(fig, filename=out_dir+'/p_under_05_models.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Identify the Highly Performing Predictors\n",
    "- Rank predictors based on their overall impact on each regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class PredictorPerformanceAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initializes the analyzer with the performance DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            DataFrame containing model performance metrics and formulas.\n",
    "            Expected columns: 'R2', 'RMSE', 'Entropy', 'Average_AIC', 'Formula'\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.predictor_metrics_df = None\n",
    "        self.aggregated_df = None\n",
    "        self.standardized_df = None\n",
    "        self.ranked_df = None\n",
    "        self.metric_name = None  # Will be set based on available metric ('RMSE' or 'Entropy')\n",
    "\n",
    "    def extract_predictors_from_formula(self, formula):\n",
    "        \"\"\"\n",
    "        Extracts predictors from a regression formula string.\n",
    "        \"\"\"\n",
    "        # Remove the response variable\n",
    "        rhs = formula.split('~')[1]\n",
    "        rhs = re.sub(r'[\\+\\*]', ',', rhs)\n",
    "        predictors = [pred.strip() for pred in rhs.split(',')]\n",
    "        # Remove empty strings and duplicates\n",
    "        predictors = list(set(filter(None, predictors)))\n",
    "        return predictors\n",
    "\n",
    "    def associate_predictors_with_metrics(self):\n",
    "        \"\"\"\n",
    "        Associates each predictor with the performance metrics from its models.\n",
    "        \"\"\"\n",
    "        predictor_metrics = []\n",
    "\n",
    "        # Determine which metric to use: RMSE or Entropy\n",
    "        if self.df['Entropy'].notna().any():\n",
    "            self.metric_name = 'Entropy'\n",
    "        elif self.df['RMSE'].notna().any():\n",
    "            self.metric_name = 'RMSE'\n",
    "        else:\n",
    "            raise ValueError(\"Neither 'RMSE' nor 'Entropy' metrics are available in the DataFrame.\")\n",
    "\n",
    "        # Iterate over each model (row in the DataFrame)\n",
    "        for _, row in self.df.iterrows():\n",
    "            formula = row['Formula']\n",
    "            predictors = self.extract_predictors_from_formula(formula)\n",
    "\n",
    "            # Use RMSE or Entropy based on which is not NaN\n",
    "            metric_value = row[self.metric_name]\n",
    "            if pd.isna(metric_value):\n",
    "                continue  # Skip if the metric is NaN\n",
    "\n",
    "            metrics = {\n",
    "                'R2': row['R2'],\n",
    "                self.metric_name: metric_value,\n",
    "                'Average_AIC': row['Average_AIC']\n",
    "            }\n",
    "\n",
    "            for predictor in predictors:\n",
    "                predictor_metrics.append({\n",
    "                    'Predictor': predictor,\n",
    "                    'R2': metrics['R2'],\n",
    "                    self.metric_name: metrics[self.metric_name],\n",
    "                    'Average_AIC': metrics['Average_AIC']\n",
    "                })\n",
    "\n",
    "        # Create DataFrame from the list\n",
    "        self.predictor_metrics_df = pd.DataFrame(predictor_metrics)\n",
    "\n",
    "    def compute_aggregated_metrics(self):\n",
    "        \"\"\"\n",
    "        Computes mean performance metrics per predictor.\n",
    "        \"\"\"\n",
    "        if self.predictor_metrics_df is None:\n",
    "            raise ValueError(\"Please run 'associate_predictors_with_metrics' method first.\")\n",
    "\n",
    "        aggregated_df = self.predictor_metrics_df.groupby('Predictor').agg({\n",
    "            'R2': 'mean',\n",
    "            self.metric_name: 'mean',\n",
    "            'Average_AIC': 'mean',\n",
    "            'R2': 'count'  # Number of models the predictor appears in\n",
    "        }).rename(columns={'R2': 'R2_mean', self.metric_name: f'{self.metric_name}_mean', 'Average_AIC': 'Average_AIC_mean', 'count': 'Model_Count'}).reset_index()\n",
    "\n",
    "        self.aggregated_df = aggregated_df\n",
    "\n",
    "    def standardize_metrics(self):\n",
    "        \"\"\"\n",
    "        Standardizes the performance metrics.\n",
    "        \"\"\"\n",
    "        if self.aggregated_df is None:\n",
    "            raise ValueError(\"Please run 'compute_aggregated_metrics' method first.\")\n",
    "\n",
    "        metrics_to_standardize = ['R2_mean', f'{self.metric_name}_mean', 'Average_AIC_mean']\n",
    "        scaler = StandardScaler()\n",
    "        standardized_values = scaler.fit_transform(self.aggregated_df[metrics_to_standardize])\n",
    "        self.aggregated_df[['z_R2_mean', f'z_{self.metric_name}_mean', 'z_Average_AIC_mean']] = standardized_values\n",
    "\n",
    "    def compute_composite_score(self):\n",
    "        \"\"\"\n",
    "        Computes a composite score for each predictor.\n",
    "        \"\"\"\n",
    "        if self.aggregated_df is None:\n",
    "            raise ValueError(\"Please run 'standardize_metrics' method first.\")\n",
    "\n",
    "        # Higher R2 is better (+), lower Entropy/RMSE and AIC are better (-)\n",
    "        self.aggregated_df['Composite_Score'] = (\n",
    "            self.aggregated_df['z_R2_mean'] - \n",
    "            self.aggregated_df[f'z_{self.metric_name}_mean'] - \n",
    "            self.aggregated_df['z_Average_AIC_mean']\n",
    "        )\n",
    "\n",
    "    def rank_predictors(self):\n",
    "        \"\"\"\n",
    "        Ranks predictors based on the composite score.\n",
    "        \"\"\"\n",
    "        if 'Composite_Score' not in self.aggregated_df.columns:\n",
    "            raise ValueError(\"Please run 'compute_composite_score' method first.\")\n",
    "\n",
    "        self.ranked_df = self.aggregated_df.sort_values(by='Composite_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    def visualize_predictor_composite_scores(self):\n",
    "        \"\"\"\n",
    "        Generates a bar plot for predictors' composite scores.\n",
    "        \"\"\"\n",
    "        if self.ranked_df is None:\n",
    "            raise ValueError(\"Please run 'rank_predictors' method first.\")\n",
    "\n",
    "        plt.figure(figsize=(30, 30))\n",
    "        sns.barplot(data=self.ranked_df, x='Composite_Score', y='Predictor', orient='h', palette='viridis')\n",
    "        plt.title('Predictor Ranking Based on Composite Score')\n",
    "        plt.xlabel('Composite Score')\n",
    "        plt.ylabel('Predictor')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def run(self):\n",
    "        # Initialize the analyzer\n",
    "        self.associate_predictors_with_metrics()\n",
    "        self.compute_aggregated_metrics()\n",
    "        self.standardize_metrics()\n",
    "        self.compute_composite_score()\n",
    "        self.rank_predictors()\n",
    "        self.visualize_predictor_composite_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = PredictorPerformanceAnalyzer(performance_df)\n",
    "analyzer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Graph the Relationships within Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Performance of Each Predictor and Each Interaction within clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class ClassInteractionAnalyzer:\n",
    "    def __init__(self, performance_df, predictor_to_class_dict):\n",
    "        \"\"\"\n",
    "        Initializes the ClassInteractionAnalyzer with the performance DataFrame\n",
    "        and the predictor-to-class mapping dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        performance_df : pd.DataFrame\n",
    "            DataFrame containing model performance metrics and formulas.\n",
    "        predictor_to_class_dict : dict\n",
    "            Dictionary mapping predictors to their corresponding classes.\n",
    "        \"\"\"\n",
    "        self.performance_df = performance_df.copy()\n",
    "        self.predictor_to_class_dict = predictor_to_class_dict\n",
    "        self.cluster_labels = self.performance_df['Cluster'].unique()\n",
    "        self.class_scores = {}\n",
    "        self.class_frequencies = {}\n",
    "        self.class_interaction_scores = {}\n",
    "        self.class_interaction_frequencies = {}\n",
    "\n",
    "    def calculate_composite_scores(self):\n",
    "        \"\"\"\n",
    "        Calculates the composite score for each formula based on performance metrics.\n",
    "        \"\"\"\n",
    "        metrics = ['R2', 'Entropy', 'Average_AIC']\n",
    "        available_metrics = [metric for metric in metrics if metric in self.performance_df.columns]\n",
    "\n",
    "        self.performance_df['Composite_Score'] = (\n",
    "            self.performance_df['R2'] -\n",
    "            self.performance_df['Entropy'] -\n",
    "            self.performance_df['Average_AIC']\n",
    "        )\n",
    "        \n",
    "        self.performance_df['Composite_Score'] = ( self.performance_df['Composite_Score'] - np.mean(self.performance_df['Composite_Score']) ) / np.std(self.performance_df['Composite_Score'])\n",
    "\n",
    "    def map_predictors_to_classes(self, predictors):\n",
    "        \"\"\"\n",
    "        Maps predictors to their corresponding classes.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictors : iterable of str\n",
    "            Predictors to map.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        predictor_classes : dict\n",
    "            Dictionary mapping predictors to classes.\n",
    "        \"\"\"\n",
    "        return {predictor: self.predictor_to_class_dict.get(predictor, 'Unknown') for predictor in predictors}\n",
    "    \n",
    "    def calculate_class_scores(self, predictor_scores, predictor_classes):\n",
    "        \"\"\"\n",
    "        Calculates average composite scores for each class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictor_scores : dict\n",
    "            Dictionary mapping predictors to composite scores.\n",
    "        predictor_classes : dict\n",
    "            Dictionary mapping predictors to classes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        class_average_scores : dict\n",
    "            Dictionary mapping classes to average composite scores.\n",
    "        \"\"\"\n",
    "        class_scores = {}\n",
    "        for predictor, scores in predictor_scores.items():\n",
    "            cls = predictor_classes[predictor]\n",
    "            if cls not in class_scores: # init\n",
    "                class_scores[cls] = []\n",
    "            class_scores[cls].extend(scores) # store the score associated w/ class\n",
    "\n",
    "        class_average_scores = {cls: np.mean(scores) for cls, scores in class_scores.items()} # average scores associated w/ each class\n",
    "        return class_average_scores\n",
    "\n",
    "    def calculate_class_interaction_scores(self, interaction_scores, predictor_classes):\n",
    "        \"\"\"\n",
    "        Calculates average composite scores for each class-class interaction.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        interaction_scores : dict\n",
    "            Dictionary mapping predictor tuples to composite scores.\n",
    "        predictor_classes : dict\n",
    "            Dictionary mapping predictors to classes.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        class_interaction_average_scores : dict\n",
    "            Dictionary mapping class pairs to average composite scores.\n",
    "        \"\"\"\n",
    "        class_interaction_scores = {}\n",
    "        for predictors_tuple, scores in interaction_scores.items():\n",
    "            classes_involved = [predictor_classes.get(pred, 'Unknown') for pred in predictors_tuple] #a list of the classes in the interaction tuple\n",
    "            classes_tuple = tuple(sorted(set(classes_involved))) #a de-duplicated tuple of the classes in the interaction. Sorted for matching purposes.\n",
    "            if len(classes_tuple) < 2: \n",
    "                continue\n",
    "            if classes_tuple not in class_interaction_scores:\n",
    "                class_interaction_scores[classes_tuple] = [] # init\n",
    "            class_interaction_scores[classes_tuple].extend(scores) # store the composite score of the interaction\n",
    "\n",
    "        class_interaction_average_scores = {cls_pair: np.mean(scores) for cls_pair, scores in class_interaction_scores.items()} #gen a dict of (interaction tuple): average score\n",
    "        return class_interaction_average_scores\n",
    "    \n",
    "    def calculate_class_frequencies(self, predictor_scores, predictor_classes):\n",
    "        \"\"\"\n",
    "        Calculates the normalized frequency of each class based on predictor_scores and predictor_classes.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictor_scores : dict\n",
    "            Dictionary mapping predictors to lists of composite scores.\n",
    "            Example: {'Predictor1': [0.5, 0.6], 'Predictor2': [0.7]}\n",
    "        predictor_classes : dict\n",
    "            Dictionary mapping predictors to their corresponding classes.\n",
    "            Example: {'Predictor1': 'ClassA', 'Predictor2': 'ClassB'}\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        class_frequencies : dict\n",
    "            Dictionary mapping classes to normalized frequencies.\n",
    "            Example: {'ClassA': 0.66, 'ClassB': 0.33}\n",
    "        \"\"\"\n",
    "        # Count the frequency of each predictor\n",
    "        predictor_frequencies = {predictor: len(scores) for predictor, scores in predictor_scores.items()}\n",
    "        \n",
    "        # Aggregate frequencies for each class\n",
    "        class_frequency_counts = {}\n",
    "        for predictor, frequency in predictor_frequencies.items():\n",
    "            cls = predictor_classes.get(predictor, 'Unknown')\n",
    "            class_frequency_counts[cls] = class_frequency_counts.get(cls, 0) + frequency\n",
    "        \n",
    "        # Normalize the frequencies\n",
    "        total_frequency = sum(class_frequency_counts.values())\n",
    "        class_frequencies = {cls: count / total_frequency for cls, count in class_frequency_counts.items()}\n",
    "        \n",
    "        return class_frequencies\n",
    "\n",
    "    def calculate_class_interaction_frequencies(self, interaction_scores, predictor_classes):\n",
    "        \"\"\"\n",
    "        Calculates the normalized frequency of each class interaction based on interaction_scores and predictor_classes.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        interaction_scores : dict\n",
    "            Dictionary mapping tuples of predictors to lists of composite scores.\n",
    "            Example: {('Predictor1', 'Predictor2'): [0.6], ('Predictor2', 'Predictor3'): [0.7, 0.8]}\n",
    "        predictor_classes : dict\n",
    "            Dictionary mapping predictors to their corresponding classes.\n",
    "            Example: {'Predictor1': 'ClassA', 'Predictor2': 'ClassB', 'Predictor3': 'ClassC'}\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        class_interaction_frequencies : dict\n",
    "            Dictionary mapping tuples of classes to normalized frequencies.\n",
    "            Example: {('ClassA', 'ClassB'): 0.5, ('ClassB', 'ClassC'): 0.5}\n",
    "        \"\"\"\n",
    "        # Count the frequency of each interaction\n",
    "        interaction_frequencies = {predictors_tuple: len(scores) for predictors_tuple, scores in interaction_scores.items()}\n",
    "        \n",
    "        # Map predictor interactions to class interactions and aggregate frequencies\n",
    "        class_interaction_counts = {}\n",
    "        for predictors_tuple, frequency in interaction_frequencies.items():\n",
    "            classes_involved = [predictor_classes.get(pred, 'Unknown') for pred in predictors_tuple]\n",
    "            classes_tuple = tuple(sorted(set(classes_involved)))\n",
    "            if len(classes_tuple) < 2: \n",
    "                continue\n",
    "            class_interaction_counts[classes_tuple] = class_interaction_counts.get(classes_tuple, 0) + frequency\n",
    "        \n",
    "        # Normalize the frequencies\n",
    "        total_frequency = sum(class_interaction_counts.values())\n",
    "        class_interaction_frequencies = {cls_tuple: count / total_frequency for cls_tuple, count in class_interaction_counts.items()}\n",
    "        \n",
    "        return class_interaction_frequencies\n",
    "    \n",
    "    def extract_terms(self, formula):\n",
    "        \"\"\"\n",
    "        Extracts main effects and interaction terms from a formula.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        formula : str\n",
    "            The regression formula.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        main_effects : list of str\n",
    "            List of main effect predictors.\n",
    "        interaction_terms : list of tuple\n",
    "            List of tuples representing interaction terms.\n",
    "        \"\"\"\n",
    "        lhs, rhs = formula.split('~')\n",
    "        terms = [term.strip() for term in rhs.strip().split('+')]\n",
    "\n",
    "        main_effects = []\n",
    "        interaction_terms = []\n",
    "\n",
    "        for term in terms:\n",
    "            if '*' in term or ':' in term:\n",
    "                term = term.replace(':', '*')\n",
    "                predictors = [p.strip() for p in term.split('*')]\n",
    "                predictors_tuple = tuple(sorted(predictors))\n",
    "                interaction_terms.append(predictors_tuple)\n",
    "            else:\n",
    "                main_effects.append(term.strip())\n",
    "\n",
    "        return main_effects, interaction_terms\n",
    "    \n",
    "    def extract_predictors_and_interactions(self, df):\n",
    "        predictor_scores = {}\n",
    "        interaction_scores = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            formula = row['Formula']\n",
    "            composite_score = row['Composite_Score']\n",
    "\n",
    "            main_effects, interaction_terms = self.extract_terms(formula)\n",
    "\n",
    "            # Process main effects\n",
    "            for predictor in main_effects:\n",
    "                if predictor not in predictor_scores: # init the predictor\n",
    "                    predictor_scores[predictor] = []\n",
    "                predictor_scores[predictor].append(composite_score) # store its score\n",
    "\n",
    "            # Process interaction terms\n",
    "            for predictors_tuple in interaction_terms:\n",
    "                if predictors_tuple not in interaction_scores: # init the interaction\n",
    "                    interaction_scores[predictors_tuple] = []\n",
    "                interaction_scores[predictors_tuple].append(composite_score) # store interaction score\n",
    "        return predictor_scores, interaction_scores\n",
    "    \n",
    "    def process_cluster(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Processes data for a single cluster by calculating class and interaction scores.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        cluster_label : int or str\n",
    "            The cluster label to process.\n",
    "        \"\"\"\n",
    "        cluster_df = self.performance_df[self.performance_df['Cluster'] == cluster_label].copy()\n",
    "        predictor_scores, interaction_scores = self.extract_predictors_and_interactions(cluster_df)\n",
    "\n",
    "        # Map predictors to classes and calculate average composite scores\n",
    "        predictor_classes = self.map_predictors_to_classes(predictor_scores.keys()) # generate a dict of predictor: class\n",
    "        class_scores = self.calculate_class_scores(predictor_scores, predictor_classes) # dict of class: average score\n",
    "        self.class_scores[cluster_label] = class_scores # assign the dict of class: average score to a dict storing performance of each cluster\n",
    "        self.class_frequencies[cluster_label] = self.calculate_class_frequencies(predictor_scores, predictor_classes)\n",
    "        \n",
    "        # Process interactions between classes\n",
    "        class_interaction_scores = self.calculate_class_interaction_scores(interaction_scores, predictor_classes) # dict of (interaction class): average score\n",
    "        self.class_interaction_scores[cluster_label] = class_interaction_scores # assign the dict of (interaction class): average score to a dict storing performance of each cluster\n",
    "        self.class_interaction_frequencies[cluster_label] = self.calculate_class_interaction_frequencies(interaction_scores, predictor_classes)\n",
    "\n",
    "    def _get_normalized_colors(self, attribute_dict, min_score, max_score):\n",
    "        \"\"\"\n",
    "        Normalizes attribute values and maps them to colors using a colormap.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        attribute_dict : dict\n",
    "            Dictionary of attribute values (composite scores) to normalize.\n",
    "        min_score : float\n",
    "            Minimum composite score across all nodes and edges.\n",
    "        max_score : float\n",
    "            Maximum composite score across all nodes and edges.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        colors : list\n",
    "            List of colors corresponding to the normalized attribute values.\n",
    "        \"\"\"\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        norm_values = [\n",
    "            (value - min_score) / (max_score - min_score) if max_score > min_score else 0.5\n",
    "            for value in attribute_dict.values()\n",
    "        ]\n",
    "        colors = [cmap(norm) for norm in norm_values]\n",
    "        return colors\n",
    "\n",
    "    def _remove_unknown_values(self, cluster_label):\n",
    "        self.class_scores[cluster_label] = {k: v for k, v in self.class_scores[cluster_label].items() if k != 'Unknown'}\n",
    "        self.class_frequencies[cluster_label] = {k: v for k, v in self.class_frequencies[cluster_label].items() if k != 'Unknown'}\n",
    "        self.class_interaction_scores[cluster_label] = {\n",
    "                                                k: v for k, v in self.class_interaction_scores[cluster_label].items() \n",
    "                                                if not (isinstance(k, tuple) and 'Unknown' in k) and k != 'Unknown'\n",
    "                                                }\n",
    "        self.class_interaction_frequencies[cluster_label] = {\n",
    "                                                k: v for k, v in self.class_interaction_frequencies[cluster_label].items() \n",
    "                                                if not (isinstance(k, tuple) and 'Unknown' in k) and k != 'Unknown'\n",
    "                                                }\n",
    "    def create_graph(self, cluster_label):\n",
    "        \"\"\"\n",
    "        Creates and visualizes the class interaction graph for a cluster.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        cluster_label : int or str\n",
    "            The cluster label.\n",
    "        \"\"\"\n",
    "        class_scores = self.class_scores.get(cluster_label, {})\n",
    "        class_frequencies = self.class_frequencies.get(cluster_label, {})\n",
    "        class_interaction_scores = self.class_interaction_scores.get(cluster_label, {})\n",
    "        class_interaction_frequencies = self.class_interaction_frequencies.get(cluster_label, {})\n",
    "\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Add nodes with attributes\n",
    "        for cls, score in class_scores.items():\n",
    "            G.add_node(cls,\n",
    "                    composite_score=score,\n",
    "                    frequency=class_frequencies.get(cls, 0))\n",
    "\n",
    "        # Add edges with attributes\n",
    "        for (class1, class2), score in class_interaction_scores.items():\n",
    "            G.add_edge(class1, class2,\n",
    "                    composite_score=score,\n",
    "                    frequency=class_interaction_frequencies.get((class1, class2), 0))\n",
    "\n",
    "        # Visualization\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "        # Get all composite scores from nodes and edges\n",
    "        composite_scores = nx.get_node_attributes(G, 'composite_score')\n",
    "        edge_composite_scores = nx.get_edge_attributes(G, 'composite_score')\n",
    "        all_composite_scores = list(composite_scores.values()) + list(edge_composite_scores.values())\n",
    "\n",
    "        # Calculate overall min and max composite scores\n",
    "        min_score = min(all_composite_scores)\n",
    "        max_score = max(all_composite_scores)\n",
    "\n",
    "        # Node attributes\n",
    "        frequencies = nx.get_node_attributes(G, 'frequency')\n",
    "        node_sizes = [frequencies[node] * 3000 for node in G.nodes()]  # Adjust scaling factor as needed\n",
    "\n",
    "        # Normalize node composite scores and get colors\n",
    "        node_colors = self._get_normalized_colors(composite_scores, min_score, max_score)\n",
    "\n",
    "        # Edge attributes\n",
    "        edge_frequencies = nx.get_edge_attributes(G, 'frequency')\n",
    "        edge_widths = [edge_frequencies[edge] * 100 for edge in G.edges()]  # Adjust scaling factor as needed\n",
    "\n",
    "        # Normalize edge composite scores and get colors\n",
    "        edge_colors = self._get_normalized_colors(edge_composite_scores, min_score, max_score)\n",
    "\n",
    "        # Draw nodes and labels\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "        # Draw edges (including self-loops)\n",
    "        for idx, edge in enumerate(G.edges()):\n",
    "            nx.draw_networkx_edges(\n",
    "                G, pos,\n",
    "                edgelist=[edge],\n",
    "                width=edge_widths[idx],\n",
    "                edge_color=[edge_colors[idx]],\n",
    "                connectionstyle='arc3, rad = 0.1' if edge[0] == edge[1] else 'arc3, rad = 0.0'\n",
    "            )\n",
    "\n",
    "        # Create a single colorbar for both nodes and edges\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min_score, vmax=max_score))\n",
    "        sm._A = []\n",
    "        cbar = plt.colorbar(sm)\n",
    "        cbar.set_label('Composite Score')\n",
    "\n",
    "        plt.title(f'Class Interaction Graph for Cluster {cluster_label}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def run(self, drop_unknown=False):\n",
    "        \"\"\"\n",
    "        Orchestrates the analysis by calculating composite scores,\n",
    "        processing each cluster, and generating graphs.\n",
    "        \"\"\"\n",
    "        self.calculate_composite_scores()\n",
    "        for cluster_label in self.cluster_labels:\n",
    "            self.process_cluster(cluster_label)\n",
    "            if drop_unknown: self._remove_unknown_values(cluster_label)\n",
    "            self.create_graph(cluster_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_to_class_dict = {}\n",
    "for pred in indep_var_list:\n",
    "    predictor_to_class_dict[pred]= pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grapher = ClassInteractionAnalyzer(performance_df, predictor_to_class_dict)\n",
    "grapher.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_var_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign predictors to classes and re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_to_class_dict = {}\n",
    "for var in indep_var_list:\n",
    "    if 'Dorsa' in var:\n",
    "        base_name = re.split(r'_[LR]_', var)[1]\n",
    "        predictor_to_class_dict[var]= base_name\n",
    "    if 'impact' in var:\n",
    "        predictor_to_class_dict[var] = var\n",
    "predictor_to_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grapher = ClassInteractionAnalyzer(performance_df, predictor_to_class_dict)\n",
    "grapher.run(drop_unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
