{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import platform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paths Input Here\n",
    "analysis = \"principal_component_analysis/5mm_maxima_v2\"\n",
    "if platform.uname().system == 'Darwin': #------------------------------Mac OS X---------------------------------------------------------------\n",
    "    conn_path = r'/Users/cu135/Dropbox (Partners HealthCare)/memory/final_analyses/maxima_and_minima_rois/roi_roi_correlation/maxima_to_vta/matrix_corrMx_T (10)_revised_with_clinical_data_dataframe.csv'\n",
    "    clin_path = r'/Users/cu135/Dropbox (Partners HealthCare)/memory/patient_data/AD_Clinical_Data_CDR_ADAS_COG_13.xlsx'\n",
    "    # clin_path = 'path to clinical values'\n",
    "    out_dir = os.path.join(os.path.dirname(conn_path), f'{analysis}')\n",
    "    #out_dir = r'path to out dir here'\n",
    "    x_roi_names = r'/Users/cu135/Dropbox (Partners HealthCare)/memory/analyses/roi-roi_correl/rios_ferguson_yeo_and_conjunction_cluster_roi_roi_correlation/matrix_corrMx_names.csv'\n",
    "    #roi_names = '<path to roi name location>'\n",
    "    print('I have set pathnames in the Mac style')\n",
    "    print('I will save to:', out_dir)\n",
    "else: #----------------------------------------------------------------Windows----------------------------------------------------------------\n",
    "    conn_path = r'C:\\Users\\calvin.howard\\Dropbox (Partners HealthCare)\\memory\\analyses\\roi-roi_correl\\matrix_corrMx_AvgR.csv'\n",
    "    clin_path = r'C:\\Users\\calvin.howard\\Dropbox (Partners HealthCare)\\memory\\patient_data\\AD_Clinical_Data_CDR_ADAS_COG_13.xlsx'\n",
    "    # clin_path = 'path to clinical values'\n",
    "    out_dir = r'C:\\Users\\calvin.howard\\Dropbox (Partners HealthCare)\\memory\\analyses\\AD_to_memory_net'\n",
    "    #out_dir = r'path to out dir here'\n",
    "    x_roi_names = r'C:\\Users\\calvin.howard\\Dropbox (Partners HealthCare)\\memory\\analyses\\roi-roi_correl\\matrix_corrMx_names.csv'\n",
    "    #roi_names = '<path to roi name location>'\n",
    "    print('I have set pathnames in the Windows style')\n",
    "\n",
    "if os.path.isdir(out_dir) != True:\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "save = True\n",
    "do_transform = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A -- Import Pre-prepared Dataframe\n",
    "Learn relationships ROIs at the ROI-ROI correl level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(conn_path)\n",
    "\n",
    "#Eliminate Columns of no Interest\n",
    "try:\n",
    "    data_df.pop('Unnamed: 0')\n",
    "    data_df.pop('Randomization Arm')\n",
    "    data_df.pop('Patient # CDR, ADAS')\n",
    "    data_df.pop('Age at DOS')\n",
    "    data_df.pop('Baseline CDR (sum of squares)')\n",
    "    data_df.pop('% Change from baseline (CDR)')\n",
    "    data_df.pop('Baseline ADAS-Cog11')    \n",
    "except:\n",
    "    print('Could not find (\"Unnamed: 0\") ')\n",
    "#Rename variables to prevent errors\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------Edit Dataframe----------------------------------------------------------------\n",
    "#Set a list of the variables to be fed into DF\n",
    "# change_string = ''\n",
    "# for i in range (0, len(variables_to_change['original'])):\n",
    "#     change_string += f\"variables_to_change['original'][{i}]: variables_to_change['destination'][{i}],\"\n",
    "# change_string = 'data_df.rename(columns={' + change_string[0:-1] + '})'\n",
    "# data_df = eval(change_string)\n",
    "\n",
    "#Organize the columns\n",
    "# import natsort\n",
    "# natsorted_columns = natsort.natsorted(data_df.columns)\n",
    "# data_df = data_df.reindex(columns=natsorted_columns)\n",
    "\n",
    "#Place response column at start of dataframe\n",
    "# response_series = data_df.pop(variables_to_change['destination'][0])\n",
    "# data_df.insert(0, variables_to_change['destination'][0], response_series)\n",
    "\n",
    "\n",
    "#Remove outlier\n",
    "# outlier_index=[9, 47, 48, 49]\n",
    "# data_df = data_df.drop(index=outlier_index)\n",
    "\n",
    "#Standardize the data\n",
    "preserved_df = data_df.copy()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "#Remove anything you don't want to standardize\n",
    "# age_df = data_df.pop('age')\n",
    "data_df.iloc[:,:] = scaler.fit_transform(data_df.iloc[:,:].values)\n",
    "#reinster things to standardize\n",
    "# data_df['age'] = age_df\n",
    "\n",
    "#Handle NaNs\n",
    "#Drop NANs\n",
    "data_df.dropna(inplace=True)\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "def pca_function(dataframe, dataframe_indices, num_components=None):\n",
    "    \n",
    "    # Define the PCA class/method\n",
    "    pca = PCA(n_components=num_components)\n",
    "    x_vals = dataframe.loc[:, dataframe_indices].to_numpy()\n",
    "    pca.fit(x_vals)\n",
    "\n",
    "    # Transform the independent variables into their principal components\n",
    "    pca_vals = pca.transform(x_vals)\n",
    "\n",
    "    # Create a dataframe from the PCA\n",
    "    pca_df = pd.DataFrame(pca_vals)\n",
    "\n",
    "    # Rename the column names\n",
    "    colnames = []\n",
    "    for col in pca_df.columns:\n",
    "        colnames.append('Component_'+str(col+1))\n",
    "    pca_df.columns = colnames\n",
    "    \n",
    "    # Extract the weights of the principal components\n",
    "    weights = pd.DataFrame(pca.components_, columns=dataframe.loc[:, dataframe_indices].columns.values.tolist())\n",
    "    weights.index = pca_df.columns\n",
    "\n",
    "    return pca_df, pca, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_indices = pd.IndexSlice['ferguson_memnet_ant_med_thalamus_left':]\n",
    "pca_df, pca, weights = pca_function(data_df, dataframe_indices)\n",
    "\n",
    "print('Principal Components Below')\n",
    "display(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get weights from the PCA\n",
    "print('Weights of Principal Components Below')\n",
    "display(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.palm import brain_permutation\n",
    "def permute_pca_weights(df_to_permute, n_permutations, num_components=None):\n",
    "    '''\n",
    "    This function receives only a dataframe of things to permute and perform a PCA on\n",
    "    It then calculates the number of \n",
    "    \n",
    "    Inputs: \n",
    "    df_to_permute - a dataframe of things to permute with no excess rows or columns\n",
    "    n_permutations - number of permutations, simply an integer\n",
    "    \n",
    "    Outputs:\n",
    "    permuted_results - a dataframe of permuted weights for each permutation\n",
    "    '''\n",
    "    #initialize array to hold permuted results\n",
    "    permuted_weights = np.zeros((n_permutations, df_to_permute.shape[1]))\n",
    "    \n",
    "    for i in range(0, n_permutations):\n",
    "        #Permute the input dataframe\n",
    "        \n",
    "        #Permute Rows\n",
    "        for i in range(0, df_to_permute.shape[0]):\n",
    "            df_to_permute.iloc[i,:] = np.random.permutation(df_to_permute.iloc[i,:])\n",
    "        #Permute columns\n",
    "        for i in range(0, df_to_permute.shape[1]):\n",
    "            df_to_permute.iloc[:,i] = np.random.permutation(df_to_permute.iloc[:,i])\n",
    "        # permuted_completed_permutation = np.apply_along_axis(lambda x: np.random.permutation(x), 1, df_to_permute.values)\n",
    "        permuted_df = df_to_permute\n",
    "        \n",
    "        #calculate the weights\n",
    "        dataframe_indices = pd.IndexSlice[:]\n",
    "        pca_df, fitted_pca, weights = pca_function(permuted_df, dataframe_indices, num_components=num_components)\n",
    "        permuted_weights[i,:] = weights.iloc[0,:]\n",
    "    return permuted_weights\n",
    "\n",
    "def calculate_empiric_p_value(observed_df, permuted_df):\n",
    "    permutations_occuring_above_observations = np.where(permuted_df > observed_df, 1, 0)\n",
    "    \n",
    "    num_occurences_above_observations = np.count_nonzero(permutations_occuring_above_observations)\n",
    "    num_permutations = permuted_df.shape[0]\n",
    "    \n",
    "    empiric_p_value = num_occurences_above_observations/num_permutations\n",
    "    \n",
    "    return empiric_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permute the PCA to Analyze the Significance of the Weights\n",
    "from calvin_utils.palm import permute_contrast_matrix\n",
    "permuted_weights = permute_pca_weights(data_df.loc[:,dataframe_indices], 10000, None)\n",
    "permuted_weights = pd.DataFrame(permuted_weights, columns=weights.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the p values\n",
    "p_value = []\n",
    "for i in range(0, len(weights.columns)):\n",
    "    p_value.append(calculate_empiric_p_value(weights.iloc[0,i], permuted_weights.iloc[:,i].values))\n",
    "p_values_df = pd.DataFrame(p_value, weights.columns).transpose()\n",
    "display(p_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform PCA\n",
    "#Merge original and PCA dataframe?\n",
    "merge = True\n",
    "##--------------------------------USER INPUT------------------------------------------------\n",
    "\n",
    "#Merge dataframes\n",
    "if merge:\n",
    "    from calvin_utils.merge_dataframes import merge_dataframes\n",
    "    merged_df = merge_dataframes(data_df.iloc[:, 0:1], pca_df)\n",
    "else:\n",
    "    merged_df = pca_df\n",
    "display(merged_df)\n",
    "#Merge the dataframe back to the orignal\n",
    "if save:\n",
    "    merged_df.to_csv(os.path.join(out_dir, 'pca_with_clinical_data.csv'))\n",
    "    weights.to_csv(os.path.join(out_dir, 'pca_weights.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained Variance Plot\n",
    "# Calculate the explained variance for each component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(explained_variance)\n",
    "sns.set_style('white')\n",
    "sns.set_palette('Greys', 1, desat=1)\n",
    "# Plot the explained variance by each component\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(range(len(explained_variance)), explained_variance)\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()\n",
    "\n",
    "if save:\n",
    "    fig.savefig(os.path.join(out_dir, 'explained_variance_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree Plot\n",
    "# Plot the Scree Plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(range(len(explained_variance)), np.cumsum(explained_variance))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if save:\n",
    "    fig.savefig(os.path.join(out_dir, 'scree_plot.png'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Linear Modelling of PCA to Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "# merged_df = merged_df.rename(columns={'% Change from baseline (ADAS-Cog11)': 'perc_improvement'})\n",
    "results = smf.ols('perc_improvement ~ Component_1 + Component_2 + Component_3', data=merged_df).fit()\n",
    "print(results.summary2())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Map From PCA of the Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform New PCA Using Fit PCA\n",
    "## must use RFz maps and z score them (treat in same manner as PCA ROI-ROI analysis)\n",
    "from calvin_utils.z_score_matrix import z_score_matrix\n",
    "from calvin_utils.import_matrices import import_matrices_from_folder\n",
    "\n",
    "yeo_import = import_matrices_from_folder('/Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/seed_generated_networks/yeo_r_maps/r_fz_map', file_pattern='/*.nii*')\n",
    "\n",
    "#z_score the networks\n",
    "yeo_networks = yeo_import.copy()\n",
    "for i in range(len(yeo_import.columns)):\n",
    "    yeo_networks.iloc[:,i] = z_score_matrix(yeo_import.iloc[:,i])\n",
    "\n",
    "#Set areas outside of the brain back to zero\n",
    "yeo_networks[yeo_import == 0] = 0\n",
    "\n",
    "display(yeo_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate the network\n",
    "yeo_networks = yeo_networks.iloc[:, [1, 2, 3]]\n",
    "display(yeo_networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from calvin_utils.generate_nifti import nifti_from_matrix\n",
    "from nimlab import datasets as nimds\n",
    "from nilearn import plotting\n",
    "\n",
    "#Itialize the holding info\n",
    "pc_1 = []\n",
    "#Iterate over each voxel, considering the voxel at a given location across each network\n",
    "for i in tqdm.tqdm(range(0, len(yeo_networks))):\n",
    "    #Only operate on nonzero voxels\n",
    "    if np.sum(yeo_networks.iloc[i,:]) != 0:\n",
    "        #Perform PCA at that voxel based on previously learned PCA\n",
    "        pc_1.append(pca.transform(yeo_networks.iloc[i,:].to_numpy().reshape(1,-1))[0,0])\n",
    "    #If voxel=zero, just assign zero\n",
    "    else:\n",
    "        pc_1.append(0)\n",
    "#Place in dataframe\n",
    "print(np.max(pc_1))\n",
    "print(np.min(pc_1))\n",
    "pc1_df = pd.DataFrame(pc_1)\n",
    "\n",
    "#Save and initialize the nifti\n",
    "img = nifti_from_matrix(matrix=pc1_df, output_file=f'/Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/principal_component_topology/{analysis}')\n",
    "\n",
    "#Display nifti\n",
    "mask = nimds.get_img(\"mni_icbm152\")\n",
    "ovr_html1 = plotting.view_img(img, cut_coords=(0,0,0), title=(f'pc_1_map'), black_bg=False, opacity=.75, cmap='ocean_hot')\n",
    "ovr_html1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the PC Nifti to Generate a Response Topography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe to use\n",
    "var_one = 'Component_1'\n",
    "input_df = pd.DataFrame({var_one: pc_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "#Generate a new matrix from the input dataframe\n",
    "responses = []\n",
    "\n",
    "#Work on a voxel-wise basis by iterating over index\n",
    "for i in tqdm.tqdm(range(0, len(input_df))):\n",
    "    #Do not calculate on zero values as intercept will be applied in the regressoin\n",
    "    if np.sum(input_df.iloc[i,0]) != 0:\n",
    "        #Assign a temporary dataframe with values that the statsmodels model is expecting\n",
    "        temp_df = pd.DataFrame({var_one: input_df.iloc[i,0]}, index=['temp_vals'])\n",
    "        #Calculate the voxelwise predicted outcome at a given voxel\n",
    "        responses.append(results.predict(temp_df)[0])\n",
    "    else:\n",
    "        #If voxel is zero-connectivity, assign zero so as to avoid application of intercept\n",
    "        responses.append(0)\n",
    "        \n",
    "#Store responses in a dataframe\n",
    "response_df = pd.DataFrame()\n",
    "response_df['response_topology'] = responses\n",
    "#set values outside of brain back to zero\n",
    "response_df.loc[:, 'response_topology'][input_df.iloc[:,0] == 0] = 0\n",
    "\n",
    "#Display the figure\n",
    "img = nifti_from_matrix(response_df, output_file='/Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/response_topology/yeo_pc1_response_topology')\n",
    "\n",
    "mask = nimds.get_img(\"mni_icbm152\")\n",
    "ovr_html1 = plotting.view_img(img, cut_coords=(0,0,0), title=(f'conjunction'), black_bg=False, opacity=.75, cmap='ocean_hot')\n",
    "ovr_html1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (nimlab)",
   "language": "python",
   "name": "nimlab_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "62880161f19d28ddb8a8f59c63374d84ace356c39e36cc839cb3fb3bb03fb010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
