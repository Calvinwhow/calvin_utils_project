{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run A Mixed Effects Model\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: July 6, 2023\n",
    "\n",
    "Use this to assess if a predictors relationship to the predictee is different between two groups. \n",
    "\n",
    "Notes:\n",
    "- To best use this notebook, you should be familar with mixed effects models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Work/KiTH_Solutions/Research/Clinical Trial/study_metadata/all_performances.xlsx'\n",
    "sheet = 'study_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/autonomous_cognitive_examination_rct/figures/covariates_and_acoe_socre/age_interact_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Randomization_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Cohort'  # The column you'd like to evaluate\n",
    "condition = 'equal'  # The condition to check ('equal', 'above', 'below')\n",
    "value = 3  # The value to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def calculate_cohens_kappa_by_category(df: pd.DataFrame, column_mapping: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate Cohen's Kappa score for specified pairs of columns in two pandas DataFrames.\n",
    "    This function is useful for evaluating agreement on a category-by-category basis.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The first DataFrame with ratings by the first rater.\n",
    "        column_mapping (dict): A dictionary mapping columns in df to corresponding columns in df.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with column names from df1 as keys and their corresponding Cohen's Kappa\n",
    "              scores as values.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any specified column does not exist in its respective DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    kappa_scores = {}\n",
    "    for k, v in column_mapping.items():\n",
    "        # Check if columns exist in their respective DataFrames\n",
    "        if k not in df.columns or v not in df.columns:\n",
    "            raise ValueError(f\"Column '{k}' or '{v}' does not exist in the DataFrame.\")\n",
    "\n",
    "        # Calculate Cohen's Kappa for the column pair and add to the result dictionary\n",
    "        kappa_scores[k] = cohen_kappa_score(df[k], df[v])\n",
    "\n",
    "    return kappa_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary Mapping Columns to be Kappa'd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'frontal': 'frontal_eh', \n",
    "        'temporal':  'temporal_eh', \n",
    "        'parietal': 'parietal_eh',\n",
    "        'occipital': 'occipital_eh',\n",
    "        'cerebellum': 'cerebellum_eh',\n",
    "       'mesial_temporal': 'mesial_temporal_eh',\n",
    "       'ventricle': 'ventricle_eh'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_cohens_kappa_by_category(data_df, dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intraclass Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICC1 (Single raters absolute):\n",
    "- Use-Case: Assess the agreement or reliability of ratings provided by a single rater on the same items over multiple trials or observations.\n",
    "Example: Evaluating how consistent a single medical doctor's diagnoses are for the same set of patients over time.\n",
    "\n",
    "ICC2 (Single random raters):\n",
    "- Use-Case: Assess the agreement among multiple raters when raters are randomly selected or if you want to account for random variation among raters.\n",
    "Example: Evaluating the agreement among different teachers when grading the same set of student assignments, with teachers selected randomly.\n",
    "\n",
    "ICC3 (Single fixed raters):\n",
    "- Use-Case: Assess the agreement among multiple raters when raters are fixed or predetermined. Assumes that the same raters provide ratings across all observations.\n",
    "Example: Evaluating the agreement among expert judges who consistently rate the same set of artworks.\n",
    "\n",
    "ICC1k (Average raters absolute):\n",
    "- Use-Case: Assess the agreement or reliability of the average ratings across multiple raters for the same items over multiple trials or observations.\n",
    "Example: Evaluating how consistent the average ratings of a panel of judges are when rating the same set of food dishes for a cooking competition.\n",
    "\n",
    "ICC2k (Average random raters):\n",
    "- Use-Case: Assess the agreement among average ratings provided by multiple raters when raters are randomly selected. Accounts for random variation among raters.\n",
    "Example: Evaluating the agreement among the average scores given by randomly selected sports referees for the same game.\n",
    "\n",
    "ICC3k (Average fixed raters):\n",
    "- Use-Case: Assess the agreement among average ratings provided by multiple raters when raters are fixed or predetermined. Assumes the same raters provide average ratings consistently.\n",
    "Example: Evaluating the agreement among the average scores given by a fixed panel of judges for multiple rounds of a talent competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "\n",
    "def calculate_icc(df: pd.DataFrame, column_mapping: dict) -> dict:\n",
    "    \n",
    "    icc_scores = {}\n",
    "    for k, v in column_mapping.items():\n",
    "    # Combine the series into a DataFrame\n",
    "        series1 = df[k]\n",
    "        series2 = df[v]\n",
    "    \n",
    "        icc_df = series1.to_frame(name='rating1')\n",
    "        icc_df['rating2'] = series2\n",
    "\n",
    "        # Melt the DataFrame to long format\n",
    "        df_melted = icc_df.melt(value_vars=['rating1', 'rating2'], var_name='rater', value_name='rating')\n",
    "        # Add a subject column\n",
    "        df_melted['subject'] = df_melted.groupby('rater').cumcount()\n",
    "        # Calculate ICC\n",
    "        icc_result = pg.intraclass_corr(data=df_melted, targets='subject', raters='rater', ratings='rating')\n",
    "        icc_scores[k] = icc_result\n",
    "    return icc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dict Mapping Columns to Be ICC'd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'Adjusted_Total': 'ACE_Total'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icc = calculate_icc(data_df, dict)\n",
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a Bunch of ICCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pingouin import intraclass_corr\n",
    "import matplotlib.patches as mpatches\n",
    "from calvin_utils.statistical_utils.distribution_statistics import BootstrappedDistributionStatistics\n",
    "from calvin_utils.statistical_utils.resampling_functions import Bootstrap\n",
    "\n",
    "class ICCForest():\n",
    "    def __init__(self, dataframe, comparisons_dict, ICC_Method='ICC1', out_dir=None, full_legend_patches=False, palette=\"tab10\"):\n",
    "        self.ICC_Method = ICC_Method\n",
    "        self.comparisons_dict = comparisons_dict\n",
    "        self.df = dataframe\n",
    "        self.out_dir = out_dir\n",
    "        self.full_legend_patches=full_legend_patches\n",
    "        self.palette = palette\n",
    "\n",
    "    def calculate_icc(self, col1, col2):\n",
    "        # Select only the specified columns and rename them for compatibility with pingouin\n",
    "        data = self.df[[col1, col2]].rename(columns={col1: 'rating1', col2: 'rating2'})\n",
    "\n",
    "        # Reshape data for pingouin's intraclass_corr function\n",
    "        df_melted = data.melt(value_vars=['rating1', 'rating2'], var_name='rater', value_name='rating')\n",
    "        df_melted['subject'] = df_melted.groupby('rater').cumcount()\n",
    "\n",
    "        # Calculate ICC\n",
    "        icc_result = intraclass_corr(data=df_melted, targets='subject', raters='rater', ratings='rating')\n",
    "        \n",
    "        # Return ICC, 95CI[lower, upper], p-value\n",
    "        return icc_result.set_index('Type').loc[self.ICC_Method, 'ICC'], icc_result.set_index('Type').loc[self.ICC_Method, 'CI95%'], icc_result.set_index('Type').loc[self.ICC_Method, 'pval']\n",
    "    \n",
    "    def save_icc_forest(self, icc_forest):\n",
    "        os.makedirs(os.path.join(self.out_dir, 'icc_figures'), exist_ok=True)\n",
    "        icc_forest.savefig(os.path.join(self.out_dir, 'icc_figures/icc_multipl_comparison_barplot.png'))\n",
    "        icc_forest.savefig(os.path.join(self.out_dir, 'icc_figures/icc_multipl_comparison_barplot_barplot.svg'))\n",
    "        print('Figure saved to: ', (os.path.join(self.out_dir, 'icc_figures/icc_multipl_comparison_barplot')))\n",
    "    \n",
    "    def plot_icc_forest(self):\n",
    "        if len(self.comparisons_dict) < 3:\n",
    "             figure = plt.figure(figsize=(6,3))\n",
    "        else:\n",
    "            figure = plt.figure(figsize=(len(self.comparisons_dict)*1.3, len(self.comparisons_dict)*1))\n",
    "        \n",
    "        # Create a color palette with enough unique colors\n",
    "        colors = sns.color_palette(self.palette, len(self.comparisons_dict))\n",
    "        \n",
    "        # Create legend patches\n",
    "        legend_patches = []\n",
    "        \n",
    "        # Iterate through the dictionary and plot ICC for each comparison\n",
    "        for idx, (col1_name, col2_name) in enumerate(self.comparisons_dict.items()):\n",
    "            # Calculate ICC\n",
    "            print(col1_name,\n",
    "            col2_name)\n",
    "            icc_value, CI95, pval = self.calculate_icc(col1_name, col2_name)\n",
    "            print(pval)\n",
    "\n",
    "            # Bootstrap 95% confidence interval\n",
    "            ci_lower, ci_upper = CI95[0], CI95[1]\n",
    "            # Plot ICC with confidence interval\n",
    "            plt.errorbar(x=icc_value, y=idx, xerr=[[icc_value - ci_lower], [ci_upper - icc_value]], fmt='o', color=colors[idx], capsize=5)\n",
    "            \n",
    "            # Add legend patch\n",
    "            if self.full_legend_patches:\n",
    "                legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{col1_name} vs {col2_name}'))\n",
    "            else:\n",
    "                legend_patches.append(mpatches.Patch(color=colors[idx], label=f'{col1_name.capitalize().replace(\"_\",\" \")}, p = {pval:.2e}'))\n",
    "\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(-1, len(self.comparisons_dict))\n",
    "        plt.yticks([])\n",
    "        plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "        plt.xlabel('ICC')\n",
    "        plt.title('Intraclass Correlation Coefficients (ICC) with 95% Confidence Intervals')\n",
    "        plt.grid(axis='x', linestyle='--')\n",
    "        \n",
    "        # Add legend\n",
    "        plt.legend(handles=legend_patches, frameon=False, loc=(0.05, 0.1))\n",
    "        \n",
    "        # Save it\n",
    "        self.save_icc_forest(figure)\n",
    "        return figure            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'Adjusted_Total': 'ACE_Total'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/autonomous_cognitive_examination_rct/figures/age_adjustment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iccplt = ICCForest(dataframe=data_df, comparisons_dict=dict, ICC_Method='ICC3k', out_dir=out_dir, full_legend_patches=False).plot_icc_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
