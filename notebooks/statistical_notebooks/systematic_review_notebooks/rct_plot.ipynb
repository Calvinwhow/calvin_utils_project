{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run An Odds Ratio\n",
    "\n",
    "### Authors: Calvin Howard.\n",
    "\n",
    "#### Last updated: July 6, 2023\n",
    "\n",
    "Use this to run/test a statistical model on a spreadsheet.\n",
    "\n",
    "Notes:\n",
    "- To best use this notebook, you should be familar with GLM design and Contrast Matrix design. See this webpage to get started:\n",
    "[FSL's GLM page](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Import CSV with All Data\n",
    "**The CSV is expected to be in this format**\n",
    "- ID and absolute paths to niftis are critical\n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| ID  | Nifti_File_Path            | Covariate_1  | Covariate_2  | Covariate_3  |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 3.4          |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 3.1          |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 3.5          |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3.2          |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep Output Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you want to save your results to\n",
    "out_dir = '/Users/cu135/Library/CloudStorage/OneDrive-Personal/OneDrive_Documents/Research/2023/subiculum_cognition_and_age/figures/Figures/suplements_3_cohort_age_optimized/unstandardized_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file containing NIFTI paths\n",
    "input_csv_path = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects.xlsx'\n",
    "sheet = 'master_list_proper_subjects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocess Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Z_Scored_Percent_Cognitive_Improvement', 'Subiculum_Group_By_24', 'City', 'Age_Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for dropping rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'City'  # The column you'd like to evaluate\n",
    "condition = 'equal'  # The condition to check ('equal', 'above', 'below', 'not')\n",
    "value = 'Queensland' # The value to drop if found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invert Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.distribution_statistics import invert_distribution\n",
    "mask = data_df['City'] == 'Toronto'\n",
    "data_df.loc[mask, ['Cognitive_Baseline']] = invert_distribution(data_df.loc[mask, ['Cognitive_Baseline']])\n",
    "\n",
    "mask = data_df['City'] == 'Queensland'\n",
    "data_df.loc[mask, ['Cognitive_Baseline']] = invert_distribution(data_df.loc[mask, ['Cognitive_Baseline']])\n",
    "\n",
    "mask = data_df['City'] == 'Toronto'\n",
    "data_df.loc[mask, ['Cognitive_Score_1_Yr']] = invert_distribution(data_df.loc[mask, ['Cognitive_Score_1_Yr']])\n",
    "\n",
    "mask = data_df['City'] == 'Queensland'\n",
    "data_df.loc[mask, ['Cognitive_Score_1_Yr']] = invert_distribution(data_df.loc[mask, ['Cognitive_Score_1_Yr']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = None #['Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group', 'Z_Scored_Subiculum_T_By_Origin_Group_'] #['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Columns by Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "def mask_and_zscore(df, mask_column, zscore_columns, reference_column=None):\n",
    "    \"\"\"\n",
    "    For a given DataFrame, create a mask based on the unique values of a specified column. \n",
    "    Then, for each column in a provided list, replace the values with z-scored counterparts \n",
    "    using only the indices corresponding to the mask.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to operate on.\n",
    "    - mask_column (str): The column name to use for creating the mask based on its unique values.\n",
    "    - zscore_columns (list): A list of column names for which the values will be replaced with their z-scored counterparts.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The modified DataFrame with specified columns z-scored within the mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a mask from unique values in the specified column\n",
    "    unique_values = df[mask_column].unique()\n",
    "    if reference_column is not None:\n",
    "        for cohort in unique_values:\n",
    "            mask = df[mask_column] == cohort\n",
    "\n",
    "            for column in zscore_columns:\n",
    "                if column in df.columns:\n",
    "                    # Use dropna() to ensure no NaNs interfere, though you mentioned there are none\n",
    "                    cohort_values = df.loc[mask, column].dropna()\n",
    "                    reference_values = df.loc[mask, reference_column].dropna() if reference_column else cohort_values\n",
    "\n",
    "                    mean_reference = reference_values.mean()\n",
    "                    std_reference = reference_values.std()\n",
    "\n",
    "                    if std_reference > 0:  # Ensuring standard deviation is not zero\n",
    "                        z_scores = (cohort_values - mean_reference) / std_reference\n",
    "                        df.loc[mask, column] = z_scores\n",
    "                    else:\n",
    "                        # Handle the case where std is 0, if needed, such as assigning a default value\n",
    "                        pass\n",
    "                else:\n",
    "                    print(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "        # for cohort in unique_values:\n",
    "        #     mask = df[mask_column] == cohort\n",
    "\n",
    "        #     # For each column in the list, replace values with z-scored counterparts within the mask\n",
    "        #     for column in zscore_columns:\n",
    "        #         if column in df.columns:\n",
    "        #             # Compute z-scores for the masked subset of the column\n",
    "        #             z_scores = (df.loc[mask, [column]] - np.mean(df.loc[mask, [reference_column]])) / np.std(df.loc[mask, [reference_column]])\n",
    "        #             # Replace the original values with z-scores within the mask\n",
    "        #             df.loc[mask, column] = z_scores\n",
    "        #         else:\n",
    "        #             print(f\"Column '{column}' not found in DataFrame.\")\n",
    "        \n",
    "    else:\n",
    "        for cohort in unique_values:\n",
    "            mask = df[mask_column] == cohort\n",
    "\n",
    "            # For each column in the list, replace values with z-scored counterparts within the mask\n",
    "            for column in zscore_columns:\n",
    "                if column in df.columns:\n",
    "                    # Compute z-scores for the masked subset of the column\n",
    "                    z_scores = (df.loc[mask, [column]] - np.mean(df.loc[mask, [column]])) / np.std(df.loc[mask, [column]])\n",
    "                    # Replace the original values with z-scores within the mask\n",
    "                    df.loc[mask, column] = z_scores\n",
    "                else:\n",
    "                    print(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Cognitive_Baseline'].isna().sum()\n",
    "data_df['Cognitive_Score_1_Yr'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = mask_and_zscore(data_df.copy(), mask_column='City', zscore_columns=['Cognitive_Score_1_Yr'])#, reference_column='Cognitive_Baseline')\n",
    "df2['Cognitive_Score_1_Yr']\n",
    "df2 = mask_and_zscore(df2, mask_column='City', zscore_columns=['Cognitive_Baseline'])\n",
    "df2['Cognitive_Baseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize_minus_one_to_one(series, reference_series=None):\n",
    "    \"\"\"\n",
    "    Normalize a series to the range [-1, 1]. If a reference series is provided,\n",
    "    use its min and max values for normalization; otherwise, use the series' own min and max values.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): The series to be normalized.\n",
    "    reference_series (pd.Series, optional): The reference series to use for normalization.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: The normalized series with values in the range [-1, 1].\n",
    "    \"\"\"\n",
    "    if reference_series is not None:\n",
    "        min_val = reference_series.min()\n",
    "        max_val = reference_series.max()\n",
    "        return 2 * (series - min_val) / (max_val - min_val) - 1\n",
    "    else:\n",
    "        min_val = series.min()\n",
    "        max_val = series.max()\n",
    "        return 2 * (series - min_val) / (max_val - min_val) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_col = 'City'  # Ensures normalization is only applied to rows falling into these categories\n",
    "col_to_normalize = 'Cognitive_Baseline'\n",
    "reference_col = 'Cognitive_Baseline'\n",
    "\n",
    "# Apply the normalization using the reference series\n",
    "data_df[f'{col_to_normalize}_normalized'] = data_df.groupby(grouping_col).apply(\n",
    "    lambda group: min_max_normalize_minus_one_to_one(group[col_to_normalize], group[reference_col])\n",
    ").reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invert a Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_distribution(series):\n",
    "    \"\"\"\n",
    "    Invert the distribution of a series.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): The series to be inverted.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: The series with its distribution inverted.\n",
    "    \"\"\"\n",
    "    max_val = series.max()\n",
    "    return max_val - series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_col = 'City'  # Ensures normalization is only applied to rows falling into these categories\n",
    "specific_group_to_flip = 'Toronto'\n",
    "col_to_normalize = 'Cognitive_Baseline_normalized'\n",
    "import pandas as pd\n",
    "# Apply the invert distribution function only where City == 'Toronto'\n",
    "data_df[col_to_normalize] = data_df.apply(\n",
    "    lambda row: invert_distribution(pd.Series([row[col_to_normalize]]))[0] if row[grouping_col] == specific_group_to_flip else row[col_to_normalize],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define conditions\n",
    "conditions = [\n",
    "    df2['Cognitive_Baseline'] > 2,  # Values over 2\n",
    "    df2['Cognitive_Baseline'] < -2  # Values under -2\n",
    "]\n",
    "\n",
    "# Define choices corresponding to the conditions\n",
    "choices = [\n",
    "    1,  # Choice for values over 2\n",
    "    -1  # Choice for values under -2\n",
    "]\n",
    "\n",
    "# Apply conditions and choices, default value is 0 for values between -2 and 2\n",
    "df2['Cognitive_Baseline'] = np.select(conditions, choices, default=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_dataframe(df, concat_col, category_col):\n",
    "    # Create a new DataFrame where each unique category becomes a column\n",
    "    # and the values from concat_col are listed under these category columns\n",
    "    # First, ensure that the index is reset for the DataFrame to avoid issues during pivoting\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create a new DataFrame where each row will have the category as a column and the corresponding values\n",
    "    # from concat_col under that category\n",
    "    pivoted_df = df.pivot(columns=category_col, values=concat_col)\n",
    "    \n",
    "    return pivoted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pivot_dataframe(data_df, 'Cognitive_Score_1_Yr_normalized','StimMatch')\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCT Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sum(data_df['Cognitive_Baseline_normalized'] == -1)\n",
    "data_df['City'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.rct import RCTPlotter\n",
    "\n",
    "# Initialize the RCTPlotter\n",
    "plotter = RCTPlotter(data=data_df, obs_cols=['Cognitive_Baseline_normalized', 'Cognitive_Score_1_Yr_normalized'], arm_col='StimMatch', category_col=None, out_dir=out_dir)\n",
    "\n",
    "# Run the RCTPlotter and display the plot\n",
    "plotter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differe in Differences Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.rct import DiDAnalysis\n",
    "analysis = DiDAnalysis(data=data_df, obs_cols=['Cognitive_Baseline', 'Cognitive_Score_1_Yr'], arm_col='StimMatch', category_col='City')\n",
    "\n",
    "# Run the DiDAnalysis and display the plot\n",
    "analysis.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity Stratification Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.rct import PropensityStratifiedRCTPlotter\n",
    "ps_rct_plotter = PropensityStratifiedRCTPlotter(data=data_df, obs_cols=['Cognitive_Baseline', 'Cognitive_Score_1_Yr'], arm_col='StimMatch', covariate_cols=['Age', 'Cognitive_Baseline'], n_strata=2)\n",
    "ps_rct_plotter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
