{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/ccm_memory/results/notebook_02'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Get Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Data will be Imported from a CSV which is expected to be in this format**\n",
    "- sub column contents MUST match the names of the neuroimaging files above. \n",
    "    - ID column \n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| sub | Nifti_File_Path            | Indep. Var.  | Covariate_N  | Dataset      |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 1            |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 1            |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 2            |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3            |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/ccm_memory/metadata/master_list_v3.csv'\n",
    "sheet = None # Set to None if CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Nifti_File_Path', 'diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Dataset'\n",
    "condition = 'equal'\n",
    "value = 'adni_memory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Dataset', 'Subject'] # ['Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group', 'Z_Scored_Subiculum_T_By_Origin_Group_'] #['Age']\n",
    "group_col = 'Dataset' #Set to none if there are no specific groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize, group_col)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Import the Data into DataFrames, Control them, and Save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_col = 'Dataset'\n",
    "nifti_path_col = 'Nifti_File_Path'\n",
    "indep_var_col = 'percent_memory_improvement'\n",
    "covariate_cols = ['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_functions import DatasetNiftiImporter\n",
    "data_importer = DatasetNiftiImporter(df=data_df, dataset_col=dataset_col, nifti_col=nifti_path_col, indep_var_col=indep_var_col, covariate_cols=covariate_cols, out_dir=out_dir, regression_method='ols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Begin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_dict_path):\n",
    "        with open(data_dict_path, 'r') as f:\n",
    "            self.dataset_paths_dict = json.load(f)\n",
    "    \n",
    "    def load_dataset(self, dataset_name):\n",
    "        paths = self.dataset_paths_dict[dataset_name]\n",
    "        data = {\n",
    "            'niftis': np.load(paths['niftis']),\n",
    "            'indep_var': np.load(paths['indep_var']),\n",
    "            'covariates': np.load(paths['covariates'])\n",
    "        }\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_dataset_static(data_paths_dict, dataset_name):\n",
    "        paths = data_paths_dict[dataset_name]\n",
    "\n",
    "        data_dict = {\n",
    "            'niftis': np.load(paths['niftis']),\n",
    "            'indep_var': np.load(paths['indep_var']),\n",
    "            'covariates': np.load(paths['covariates'])\n",
    "        }\n",
    "        return data_dict\n",
    "\n",
    "class CorrelationCalculator:\n",
    "    def __init__(self, method='pearson', verbose=False):\n",
    "        self.method = method\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _calculate_pearson_r_map(self, niftis, indep_var):\n",
    "        X = indep_var\n",
    "        Y = niftis\n",
    "        X_BAR = X.mean(axis=0)[:, np.newaxis]\n",
    "        Y_BAR = Y.mean(axis=0)[np.newaxis, :]\n",
    "        X_C = X - X_BAR\n",
    "        Y_C = Y - Y_BAR\n",
    "        NUMERATOR = np.dot(X_C.T, Y_C)\n",
    "        SST_X = np.sum((X - X_BAR)**2, axis=0)\n",
    "        SST_Y = np.sum((Y - Y_BAR)**2, axis=0)\n",
    "        DENOMINATOR = np.sqrt(SST_X * SST_Y)\n",
    "        r = NUMERATOR / DENOMINATOR\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Shape of X: {X.shape}\")\n",
    "            print(f\"Shape of Y: {Y.shape}\")\n",
    "            print(f\"Shape of X_BAR: {X_BAR.shape}\")\n",
    "            print(f\"Shape of Y_BAR: {Y_BAR.shape}\")\n",
    "            print(f\"Shape of X_C: {X_C.shape}\")\n",
    "            print(f\"Shape of Y_C: {Y_C.shape}\")\n",
    "            print(f\"Shape of NUMERATOR: {NUMERATOR.shape}\")\n",
    "            print(f\"Shape of DENOMINATOR: {DENOMINATOR.shape}\")\n",
    "        return r\n",
    "\n",
    "    def _calculate_spearman_r_map(self, niftis, indep_var):\n",
    "        '''Not easily broadcast, sorry!'''\n",
    "        n_voxels = niftis.shape[1]\n",
    "        rho = np.zeros(n_voxels)\n",
    "        for i in tqdm(range(n_voxels), desc='Running Spearman Rho'):\n",
    "            rho[i], _ = spearmanr(indep_var, niftis[:, i])\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"Shape of niftis: {niftis.shape}\")\n",
    "            print(f\"Shape of rho: {rho.shape}\")\n",
    "        return rho\n",
    "    \n",
    "    def _process_data(self, data):\n",
    "        if self.method == 'pearson':\n",
    "            self.correlation_map = self._calculate_pearson_r_map(data['niftis'], data['indep_var'])\n",
    "        elif self.method == 'spearman':\n",
    "            self.correlation_map = self._calculate_spearman_r_map(data['niftis'], data['indep_var'])\n",
    "    \n",
    "    def process_all_datasets(self, data_dict):\n",
    "        correlation_maps = {}\n",
    "        for dataset_name in data_dict.keys():\n",
    "            data = DataLoader.load_dataset_static(data_dict, dataset_name)\n",
    "            self._process_data(data)\n",
    "            correlation_maps[dataset_name] = self.correlation_map\n",
    "        return correlation_maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the path to the dictionary generated by Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/ccm_memory/results/notebook_02/tmp/dataset_dict.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What correlation method would you like to use?\n",
    "- Pearson is very fast, but depends on linearity and is sensitive to outliers\n",
    "- Spearman is slower, but much more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = 'pearson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/946tskyj68b6htgvndtppmz80000gp/T/ipykernel_41006/4252487287.py:47: RuntimeWarning: invalid value encountered in divide\n",
      "  r = NUMERATOR / DENOMINATOR\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(dict_path)\n",
    "correlation_calculator = CorrelationCalculator(method=correlation, verbose=False)\n",
    "corr_map_dict = correlation_calculator.process_all_datasets(data_loader.dataset_paths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grafmann_memory\n",
      "manitoba_memory\n",
      "kahana_memory\n",
      "queensland_memory\n",
      "sante_memory\n",
      "fornix_memory\n"
     ]
    }
   ],
   "source": [
    "for k in corr_map_dict.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, image\n",
    "import nibabel as nib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class ConvergentMapGenerator:\n",
    "    def __init__(self, corr_map_dict, data_loader, mask_path=None, out_dir=None):\n",
    "        self.corr_map_dict = corr_map_dict\n",
    "        self.data_loader = data_loader\n",
    "        self.mask_path = mask_path\n",
    "        self.out_dir = out_dir\n",
    "        self._handle_nans()\n",
    "        \n",
    "    def _handle_nans(self):\n",
    "        for key in self.corr_map_dict.keys():\n",
    "            if np.isnan(self.corr_map_dict[key]).all():\n",
    "                print(f\"Warning: The correlation map for {key} contains only NaNs and will be excluded from the analysis.\")\n",
    "                del self.corr_map_dict[key]\n",
    "            elif np.isnan(self.corr_map_dict[key]).any():\n",
    "                self.corr_map_dict[key] = np.nan_to_num(self.corr_map_dict[key], nan=0, posinf=1, neginf=-1)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    def generate_weighted_average_r_map(self):\n",
    "        weights = []\n",
    "        for dataset_name in self.corr_map_dict.keys():\n",
    "            data = self.data_loader.load_dataset(dataset_name)\n",
    "            weights.append(data['niftis'].shape[0])\n",
    "        weights = np.array(weights)\n",
    "        r_maps = np.array(list(self.corr_map_dict.values()))\n",
    "        return np.average(r_maps, axis=0, weights=weights)\n",
    "\n",
    "    def generate_agreement_map(self):\n",
    "        r_maps = np.array(list(self.corr_map_dict.values()))\n",
    "        signs = np.sign(r_maps)\n",
    "        agreement = np.all(signs == signs[0], axis=0)\n",
    "        return agreement.astype(int)\n",
    "    \n",
    "    def _unmask_array(self, data_array, threshold=0):\n",
    "        if self.mask_path is None:\n",
    "            from nimlab import datasets as nimds\n",
    "            mask = nimds.get_img(\"mni_icbm152\")\n",
    "        else:\n",
    "            mask = nib.load(self.mask_path)\n",
    "\n",
    "        mask_data = mask.get_fdata()\n",
    "        mask_indices = mask_data.flatten() > threshold\n",
    "        \n",
    "        unmasked_array = np.zeros(mask_indices.shape)\n",
    "        unmasked_array[mask_indices] = data_array.flatten()\n",
    "        return unmasked_array.reshape(mask_data.shape), mask.affine\n",
    "\n",
    "    def _save_map(self, map_data, file_name):\n",
    "        unmasked_map, mask_affine = self._unmask_array(map_data)\n",
    "        img = nib.Nifti1Image(unmasked_map, affine=mask_affine)\n",
    "        if self.out_dir is not None:\n",
    "            file_path = os.path.join(out_dir, 'convergence_map', file_name)\n",
    "            nib.save(img, file_path)\n",
    "        return img\n",
    "\n",
    "    def _visualize_map(self, img, title):\n",
    "        plotting.view_img(img, title=title).open_in_browser()\n",
    "        \n",
    "    def generate_and_save_maps(self):\n",
    "        # Generate weighted average r map\n",
    "        weighted_avg_map = self.generate_weighted_average_r_map()\n",
    "        weighted_avg_img = self._save_map(weighted_avg_map, 'weighted_average_r_map.nii.gz')\n",
    "        self._visualize_map(weighted_avg_img, 'Weighted Average R Map')\n",
    "\n",
    "        # Generate agreement map\n",
    "        agreement_map = self.generate_agreement_map()\n",
    "        agreement_img = self._save_map(agreement_map, 'agreement_map.nii.gz')\n",
    "        self._visualize_map(agreement_img, 'Agreement Map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/nilearn/image/resampling.py:297: UserWarning: Resampling binary images with continuous or linear interpolation. This might lead to unexpected results. You might consider using nearest interpolation instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "convergent_map_generator = ConvergentMapGenerator(corr_map_dict, data_loader)\n",
    "convergent_map_generator.generate_and_save_maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "r:\n",
    "    def generate_weighted_average_r_map(self, r_maps, weights):\n",
    "        return np.average(r_maps, axis=0, weights=weights)\n",
    "\n",
    "    def generate_agreement_map(self, r_maps):\n",
    "        signs = np.sign(r_maps)\n",
    "        agreement = np.all(signs == signs[0], axis=0)\n",
    "        return agreement.astype(int)\n",
    "    \n",
    "class MapSaver:\n",
    "    def save_map(self, map_data, file_path):\n",
    "        np.save(file_path, map_data)\n",
    "        \n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "class LOOCVAnalyzer:\n",
    "    def __init__(self, datasets, method='spearman'):\n",
    "        \"\"\"\n",
    "        Initialize the LOOCVAnalyzer.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        datasets : list of dict\n",
    "            List of datasets, where each dataset is a dictionary containing 'niftis' and 'indep_var'.\n",
    "        method : str\n",
    "            Correlation method to use ('spearman' or 'pearson').\n",
    "        \"\"\"\n",
    "        self.datasets = datasets\n",
    "        self.method = method\n",
    "\n",
    "    def perform_loocv(self):\n",
    "        \"\"\"\n",
    "        Perform Leave-One-Out Cross-Validation (LOOCV) analysis.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuple\n",
    "            List of tuples containing the R-value and confidence intervals for each dataset.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i in range(len(self.datasets)):\n",
    "            test_dataset = self.datasets[i]\n",
    "            train_datasets = self.datasets[:i] + self.datasets[i+1:]\n",
    "            convergent_map = self.generate_convergent_map(train_datasets)\n",
    "            similarities = self.calculate_similarity(test_dataset['niftis'], convergent_map)\n",
    "            r, ci_lower, ci_upper = self.correlate_similarity_with_outcomes(similarities, test_dataset['indep_var'])\n",
    "            results.append((r, ci_lower, ci_upper))\n",
    "        return results\n",
    "\n",
    "    def generate_convergent_map(self, train_datasets):\n",
    "        \"\"\"\n",
    "        Generate the convergent map using the training datasets.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_datasets : list of dict\n",
    "            List of training datasets.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.array\n",
    "            Convergent map.\n",
    "        \"\"\"\n",
    "        # Placeholder for generating the convergent map\n",
    "        # This should be implemented based on the chosen method (e.g., weighted average or agreement map)\n",
    "        pass\n",
    "\n",
    "    def calculate_similarity(self, patient_maps, convergent_map):\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between patient maps and the convergent map.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        patient_maps : np.array\n",
    "            Array of patient maps.\n",
    "        convergent_map : np.array\n",
    "            Convergent map.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of float\n",
    "            List of cosine similarity values.\n",
    "        \"\"\"\n",
    "        similarities = [1 - cosine(patient_map, convergent_map) for patient_map in patient_maps]\n",
    "        return similarities\n",
    "\n",
    "    def correlate_similarity_with_outcomes(self, similarities, indep_var):\n",
    "        \"\"\"\n",
    "        Correlate similarity values with independent variables and calculate confidence intervals.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        similarities : list of float\n",
    "            List of cosine similarity values.\n",
    "        indep_var : np.array\n",
    "            Array of independent variable values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            R-value, lower confidence interval, and upper confidence interval.\n",
    "        \"\"\"\n",
    "        resampled_r = []\n",
    "        for _ in range(1000):\n",
    "            resampled_indices = np.random.choice(len(similarities), len(similarities), replace=True)\n",
    "            resampled_similarities = np.array(similarities)[resampled_indices]\n",
    "            resampled_indep_var = np.array(indep_var)[resampled_indices]\n",
    "            if self.method == 'spearman':\n",
    "                resampled_r.append(spearmanr(resampled_similarities, resampled_indep_var)[0])\n",
    "            else:\n",
    "                resampled_r.append(pearsonr(resampled_similarities, resampled_indep_var)[0])\n",
    "        ci_lower = np.percentile(resampled_r, 2.5)\n",
    "        ci_upper = np.percentile(resampled_r, 97.5)\n",
    "        r = np.mean(resampled_r)\n",
    "        return r, ci_lower, ci_upper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (nimlab)",
   "language": "python",
   "name": "nimlab_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
