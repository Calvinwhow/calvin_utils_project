{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/ccm_memory/results/notebook_02'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Get Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Data will be Imported from a CSV which is expected to be in this format**\n",
    "- sub column contents MUST match the names of the neuroimaging files above. \n",
    "    - ID column \n",
    "```\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| sub | Nifti_File_Path            | Indep. Var.  | Covariate_N  | Dataset      |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "| 1   | /path/to/file1.nii.gz      | 0.5          | 1.2          | 1            |\n",
    "| 2   | /path/to/file2.nii.gz      | 0.7          | 1.4          | 1            |\n",
    "| 3   | /path/to/file3.nii.gz      | 0.6          | 1.5          | 2            |\n",
    "| 4   | /path/to/file4.nii.gz      | 0.9          | 1.1          | 3            |\n",
    "| ... | ...                        | ...          | ...          | ...          |\n",
    "+-----+----------------------------+--------------+--------------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/ccm_memory/metadata/master_list_v3.csv'\n",
    "sheet = None # Set to None if CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statsmodels_palm import CalvinStatsmodelsPalm\n",
    "# Instantiate the PalmPrepararation class\n",
    "cal_palm = CalvinStatsmodelsPalm(input_csv_path=input_csv_path, output_dir=out_dir, sheet=sheet)\n",
    "# Call the process_nifti_paths method\n",
    "data_df = cal_palm.read_and_display_data()\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handle NANs**\n",
    "- Set drop_nans=True is you would like to remove NaNs from data\n",
    "- Provide a column name or a list of column names to remove NaNs from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Nifti_File_Path', 'diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.drop_nans_from_columns(columns_to_drop_from=drop_list)\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Row Based on Value of Column**\n",
    "\n",
    "Define the column, condition, and value for dropping rows\n",
    "- column = 'your_column_name'\n",
    "- condition = 'above'  # Options: 'equal', 'above', 'below'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Dataset'\n",
    "condition = 'equal'\n",
    "value = 'adni_memory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, other_df = cal_palm.drop_rows_based_on_value(column, condition, value)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize Data**\n",
    "- Enter Columns you Don't want to standardize into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove anything you don't want to standardize\n",
    "cols_not_to_standardize = ['Dataset', 'Subject'] # ['Z_Scored_Percent_Cognitive_Improvement_By_Origin_Group', 'Z_Scored_Subiculum_T_By_Origin_Group_'] #['Age']\n",
    "group_col = 'Dataset' #Set to none if there are no specific groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = cal_palm.standardize_columns(cols_not_to_standardize, group_col)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Import the Data into DataFrames, Control them, and Save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_col = 'Dataset'\n",
    "nifti_path_col = 'Nifti_File_Path'\n",
    "indep_var_col = 'percent_memory_improvement'\n",
    "covariate_cols = ['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_functions import DatasetNiftiImporter\n",
    "data_importer = DatasetNiftiImporter(df=data_df, dataset_col=dataset_col, nifti_col=nifti_path_col, indep_var_col=indep_var_col, covariate_cols=covariate_cols, out_dir=out_dir, regression_method='ols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Begin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_dict_path):\n",
    "        with open(data_dict_path, 'r') as f:\n",
    "            self.dataset_paths_dict = json.load(f)\n",
    "    \n",
    "    def load_dataset(self, dataset_name):\n",
    "        paths = self.dataset_paths_dict[dataset_name]\n",
    "        data = {\n",
    "            'niftis': np.load(paths['niftis']),\n",
    "            'indep_var': np.load(paths['indep_var']),\n",
    "            'covariates': np.load(paths['covariates'])\n",
    "        }\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_dataset_static(data_paths_dict, dataset_name):\n",
    "        paths = data_paths_dict[dataset_name]\n",
    "\n",
    "        data_dict = {\n",
    "            'niftis': np.load(paths['niftis']),\n",
    "            'indep_var': np.load(paths['indep_var']),\n",
    "            'covariates': np.load(paths['covariates'])\n",
    "        }\n",
    "        return data_dict\n",
    "\n",
    "class CorrelationCalculator:\n",
    "    def __init__(self, method='pearson', verbose=False):\n",
    "        self.method = method\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _calculate_pearson_r_map(self, niftis, indep_var):\n",
    "        X = indep_var\n",
    "        Y = niftis\n",
    "        X_BAR = X.mean(axis=0)[:, np.newaxis]\n",
    "        Y_BAR = Y.mean(axis=0)[np.newaxis, :]\n",
    "        X_C = X - X_BAR\n",
    "        Y_C = Y - Y_BAR\n",
    "        NUMERATOR = np.dot(X_C.T, Y_C)\n",
    "        SST_X = np.sum((X - X_BAR)**2, axis=0)\n",
    "        SST_Y = np.sum((Y - Y_BAR)**2, axis=0)\n",
    "        DENOMINATOR = np.sqrt(SST_X * SST_Y)\n",
    "        r = NUMERATOR / DENOMINATOR\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Shape of X: {X.shape}\")\n",
    "            print(f\"Shape of Y: {Y.shape}\")\n",
    "            print(f\"Shape of X_BAR: {X_BAR.shape}\")\n",
    "            print(f\"Shape of Y_BAR: {Y_BAR.shape}\")\n",
    "            print(f\"Shape of X_C: {X_C.shape}\")\n",
    "            print(f\"Shape of Y_C: {Y_C.shape}\")\n",
    "            print(f\"Shape of NUMERATOR: {NUMERATOR.shape}\")\n",
    "            print(f\"Shape of DENOMINATOR: {DENOMINATOR.shape}\")\n",
    "        return r\n",
    "\n",
    "    def _calculate_spearman_r_map(self, niftis, indep_var):\n",
    "        '''Not easily broadcast, sorry!'''\n",
    "        n_voxels = niftis.shape[1]\n",
    "        rho = np.zeros(n_voxels)\n",
    "        for i in tqdm(range(n_voxels), desc='Running Spearman Rho'):\n",
    "            rho[i], _ = spearmanr(indep_var, niftis[:, i])\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"Shape of niftis: {niftis.shape}\")\n",
    "            print(f\"Shape of rho: {rho.shape}\")\n",
    "        return rho\n",
    "    \n",
    "    def _process_data(self, data):\n",
    "        if self.method == 'pearson':\n",
    "            self.correlation_map = self._calculate_pearson_r_map(data['niftis'], data['indep_var'])\n",
    "        elif self.method == 'spearman':\n",
    "            self.correlation_map = self._calculate_spearman_r_map(data['niftis'], data['indep_var'])\n",
    "    \n",
    "    def process_all_datasets(self, data_dict):\n",
    "        correlation_maps = {}\n",
    "        for dataset_name in data_dict.keys():\n",
    "            data = DataLoader.load_dataset_static(data_dict, dataset_name)\n",
    "            self._process_data(data)\n",
    "            correlation_maps[dataset_name] = self.correlation_map\n",
    "        return correlation_maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the path to the dictionary generated by Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/studies/ccm_memory/results/notebook_02/tmp/dataset_dict.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What correlation method would you like to use?\n",
    "- Pearson is very fast, but depends on linearity and is sensitive to outliers\n",
    "- Spearman is slower, but much more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = 'pearson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/946tskyj68b6htgvndtppmz80000gp/T/ipykernel_48958/4252487287.py:47: RuntimeWarning: invalid value encountered in divide\n",
      "  r = NUMERATOR / DENOMINATOR\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(dict_path)\n",
    "correlation_calculator = CorrelationCalculator(method=correlation, verbose=False)\n",
    "corr_map_dict = correlation_calculator.process_all_datasets(data_loader.dataset_paths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "import nibabel as nib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class ConvergentMapGenerator:\n",
    "    def __init__(self, corr_map_dict, data_loader, mask_path=None, out_dir=None, weight=False):\n",
    "        self.corr_map_dict = corr_map_dict\n",
    "        self.data_loader = data_loader\n",
    "        self.mask_path = mask_path\n",
    "        self.out_dir = out_dir\n",
    "        self.weight = weight\n",
    "        self._handle_nans()\n",
    "        \n",
    "    def _handle_nans(self):\n",
    "        drop_list = []\n",
    "        for key in self.corr_map_dict.keys():\n",
    "            if np.isnan(self.corr_map_dict[key]).all():\n",
    "                print(f\"Warning: The correlation map for {key} contains only NaNs and will be excluded from the analysis.\")\n",
    "                drop_list.append(key)\n",
    "            elif np.isnan(self.corr_map_dict[key]).any():\n",
    "                self.corr_map_dict[key] = np.nan_to_num(self.corr_map_dict[key], nan=0, posinf=1, neginf=-1)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        for key in drop_list:\n",
    "            del self.corr_map_dict[key]\n",
    "            \n",
    "    def generate_weighted_average_r_map(self):\n",
    "        r_maps = np.array(list(self.corr_map_dict.values()))\n",
    "        if self.weight:\n",
    "            weights = []\n",
    "            for dataset_name in self.corr_map_dict.keys():\n",
    "                data = self.data_loader.load_dataset(dataset_name)\n",
    "                weights.append(data['niftis'].shape[0])\n",
    "            weights = np.array(weights)\n",
    "            return np.average(r_maps, axis=0, weights=weights)\n",
    "        else:\n",
    "            return np.mean(r_maps, axis=0)\n",
    "\n",
    "    def generate_agreement_map(self):\n",
    "        r_maps = np.array(list(self.corr_map_dict.values()))\n",
    "        signs = np.sign(r_maps)\n",
    "        agreement = np.all(signs == signs[0], axis=0)\n",
    "        return agreement.astype(int)\n",
    "    \n",
    "    def _unmask_array(self, data_array, threshold=0):\n",
    "        if self.mask_path is None:\n",
    "            from nimlab import datasets as nimds\n",
    "            mask = nimds.get_img(\"mni_icbm152\")\n",
    "        else:\n",
    "            mask = nib.load(self.mask_path)\n",
    "\n",
    "        mask_data = mask.get_fdata()\n",
    "        mask_indices = mask_data.flatten() > threshold\n",
    "        \n",
    "        unmasked_array = np.zeros(mask_indices.shape)\n",
    "        unmasked_array[mask_indices] = data_array.flatten()\n",
    "        return unmasked_array.reshape(mask_data.shape), mask.affine\n",
    "\n",
    "    def _save_map(self, map_data, file_name):\n",
    "        unmasked_map, mask_affine = self._unmask_array(map_data)\n",
    "        img = nib.Nifti1Image(unmasked_map, affine=mask_affine)\n",
    "        if self.out_dir is not None:\n",
    "            file_path = os.path.join(out_dir, 'convergence_map', file_name)\n",
    "            nib.save(img, file_path)\n",
    "        return img\n",
    "\n",
    "    def _visualize_map(self, img, title):\n",
    "        plotting.view_img(img, title=title).open_in_browser()\n",
    "        \n",
    "    def generate_and_save_maps(self):\n",
    "        # Generate weighted average r map\n",
    "        weighted_avg_map = self.generate_weighted_average_r_map()\n",
    "        try:\n",
    "            weighted_avg_img = self._save_map(weighted_avg_map, 'weighted_average_r_map.nii.gz')\n",
    "            self._visualize_map(weighted_avg_img, 'Weighted Average R Map')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Generate agreement map\n",
    "        agreement_map = self.generate_agreement_map()\n",
    "        try:\n",
    "            agreement_img = self._save_map(agreement_map, 'agreement_map.nii.gz')\n",
    "            self._visualize_map(agreement_img, 'Agreement Map')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def save_individual_r_maps(self):\n",
    "        for dataset_name, r_map in self.corr_map_dict.items():\n",
    "            r_img = self._save_map(r_map, f'{dataset_name}_correlation_map.nii.gz')\n",
    "            self._visualize_map(r_img, f'{dataset_name} Correlation Map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the Convergent Maps and Save the Individual Correlation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/nilearn/image/resampling.py:297: UserWarning: Resampling binary images with continuous or linear interpolation. This might lead to unexpected results. You might consider using nearest interpolation instead.\n",
      "  warnings.warn(\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n",
      "/Users/cu135/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:784: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n"
     ]
    }
   ],
   "source": [
    "convergent_map_generator = ConvergentMapGenerator(corr_map_dict, data_loader, weight=False)\n",
    "convergent_map_generator.generate_and_save_maps()\n",
    "convergent_map_generator.save_individual_r_maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CorrelationCalculator:\n",
    "    def __init__(self, method='pearson', verbose=False):\n",
    "        self.method = method\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _calculate_pearson_r_map(self, niftis, indep_var):\n",
    "        X = indep_var\n",
    "        Y = niftis\n",
    "        X_BAR = X.mean(axis=0)[:, np.newaxis]\n",
    "        Y_BAR = Y.mean(axis=0)[np.newaxis, :]\n",
    "        X_C = X - X_BAR\n",
    "        Y_C = Y - Y_BAR\n",
    "        NUMERATOR = np.dot(X_C.T, Y_C)\n",
    "        SST_X = np.sum((X - X_BAR)**2, axis=0)\n",
    "        SST_Y = np.sum((Y - Y_BAR)**2, axis=0)\n",
    "        DENOMINATOR = np.sqrt(SST_X * SST_Y)\n",
    "        r = NUMERATOR / DENOMINATOR\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Shape of X: {X.shape}\")\n",
    "            print(f\"Shape of Y: {Y.shape}\")\n",
    "            print(f\"Shape of X_BAR: {X_BAR.shape}\")\n",
    "            print(f\"Shape of Y_BAR: {Y_BAR.shape}\")\n",
    "            print(f\"Shape of X_C: {X_C.shape}\")\n",
    "            print(f\"Shape of Y_C: {Y_C.shape}\")\n",
    "            print(f\"Shape of NUMERATOR: {NUMERATOR.shape}\")\n",
    "            print(f\"Shape of DENOMINATOR: {DENOMINATOR.shape}\")\n",
    "        return r\n",
    "\n",
    "    def _calculate_spearman_r_map(self, niftis, indep_var):\n",
    "        '''Not easily broadcast, sorry!'''\n",
    "        n_voxels = niftis.shape[1]\n",
    "        rho = np.zeros(n_voxels)\n",
    "        for i in tqdm(range(n_voxels), desc='Running Spearman Rho'):\n",
    "            rho[i], _ = spearmanr(indep_var, niftis[:, i])\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"Shape of niftis: {niftis.shape}\")\n",
    "            print(f\"Shape of rho: {rho.shape}\")\n",
    "        return rho\n",
    "    \n",
    "    def _process_data(self, data):\n",
    "        if self.method == 'pearson':\n",
    "            self.correlation_map = self._calculate_pearson_r_map(data['niftis'], data['indep_var'])\n",
    "        elif self.method == 'spearman':\n",
    "            self.correlation_map = self._calculate_spearman_r_map(data['niftis'], data['indep_var'])\n",
    "    \n",
    "    def process_all_datasets(self, data_dict):\n",
    "        correlation_maps = {}\n",
    "        for dataset_name in data_dict.keys():\n",
    "            data = DataLoader.load_dataset_static(data_dict, dataset_name)\n",
    "            self._process_data(data)\n",
    "            correlation_maps[dataset_name] = self.correlation_map\n",
    "        return correlation_maps\n",
    "\n",
    "class LOOCVAnalyzer(ConvergentMapGenerator):\n",
    "    def __init__(self, corr_map_dict, data_loader, mask_path=None, out_dir=None, weight=False, method='spearman', convergence_type='agreement', similarity='cos', n_bootstrap=1000):\n",
    "        \"\"\"\n",
    "        Initialize the LOOCVAnalyzer.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        corr_map_dict : dict\n",
    "            Dictionary containing correlation maps for each dataset.\n",
    "        data_loader : DataLoader\n",
    "            Instance of DataLoader to load datasets.\n",
    "        mask_path : str, optional\n",
    "            Path to the mask file.\n",
    "        out_dir : str, optional\n",
    "            Output directory to save maps.\n",
    "        weight : bool, optional\n",
    "            Whether to weight the datasets.\n",
    "        method : str, optional\n",
    "            Correlation method to use ('spearman' or 'pearson').\n",
    "        n_bootstrap : int, optional\n",
    "            Number of bootstrap samples to generate.\n",
    "        convergence_type : str, optional\n",
    "            Type of convergence to use ('agreement' or other types). Default is 'agreement'.\n",
    "        similarity : str, optional\n",
    "            Similarity measure to use ('cos' for cosine similarity or other measures). Default is 'cos'.\n",
    "            Number of bootstrap samples to generate. Default is 1000.\n",
    "        \"\"\"\n",
    "        super().__init__(corr_map_dict, data_loader, mask_path, out_dir, weight)\n",
    "        self.method = method\n",
    "        self.n_bootstrap = n_bootstrap\n",
    "        self.similarity = similarity\n",
    "        self.convergence_type = convergence_type\n",
    "        self.correlation_calculator = CorrelationCalculator(method=method)\n",
    "        self.results = self.perform_loocv()\n",
    "        self.results_df = self.results_to_dataframe()\n",
    "    \n",
    "    def results_to_dataframe(self):\n",
    "        \"\"\"\n",
    "        Convert the LOOCV results to a pandas DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing the R-value, lower confidence interval, upper confidence interval, and mean R-value for each dataset.\n",
    "        \"\"\"\n",
    "        columns = ['Dataset', 'Observed R', 'CI Lower', 'CI Upper', 'Mean R']\n",
    "        data = []\n",
    "        for i, (observed_r, ci_lower, ci_upper, mean_r) in enumerate(self.results):\n",
    "            dataset_name = list(self.corr_map_dict.keys())[i]\n",
    "            data.append([dataset_name, observed_r, ci_lower, ci_upper, mean_r])\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    def perform_loocv(self):\n",
    "        \"\"\"\n",
    "        Perform Leave-One-Out Cross-Validation (LOOCV) analysis.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuple\n",
    "            List of tuples containing the R-value and confidence intervals for each dataset.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        dataset_names = list(self.corr_map_dict.keys())\n",
    "        for i, test_dataset_name in enumerate(dataset_names):\n",
    "            print(\"Evaluating dataset:\", test_dataset_name)\n",
    "            # Load the test dataset\n",
    "            test_data = self.data_loader.load_dataset(test_dataset_name)\n",
    "            test_niftis = test_data['niftis']\n",
    "            test_indep_var = test_data['indep_var']\n",
    "\n",
    "            # TRAIN - Generate the convergent map using the training datasets\n",
    "            train_dataset_names = dataset_names[:i] + dataset_names[i+1:]\n",
    "            self.corr_map_dict = self.generate_correlation_maps(train_dataset_names)\n",
    "            self._handle_nans()\n",
    "            if self.convergence_type == 'average':\n",
    "                convergent_map = self.generate_weighted_average_r_map()\n",
    "            elif self.convergence_type == 'agreement':\n",
    "                convergent_map = self.generate_agreement_map()\n",
    "            else:\n",
    "                raise ValueError(\"Invalid convergence type (self.convergence_type). Please choose 'average' or 'agreement'.\")\n",
    "            \n",
    "            # TEST - use the convergent map pm the test dataset\n",
    "            ## Calculate similarity\n",
    "            similarities = self.calculate_similarity(test_niftis, convergent_map)\n",
    "            ## Correlate similarity values with independent variables\n",
    "            observed_r, ci_lower, ci_upper, mean_r = self.correlate_similarity_with_outcomes(similarities, test_indep_var)\n",
    "            results.append((observed_r, ci_lower, ci_upper, mean_r))\n",
    "        return results\n",
    "\n",
    "    def generate_correlation_maps(self, dataset_names):\n",
    "        \"\"\"\n",
    "        Generate correlation maps for the given dataset names.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset_names : list of str\n",
    "            List of dataset names.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing correlation maps for each dataset.\n",
    "        \"\"\"\n",
    "        correlation_maps = {}\n",
    "        for dataset_name in dataset_names:\n",
    "            data = self.data_loader.load_dataset(dataset_name)\n",
    "            self.correlation_calculator._process_data(data)\n",
    "            correlation_maps[dataset_name] = self.correlation_calculator.correlation_map\n",
    "        return correlation_maps\n",
    "\n",
    "    def calculate_similarity(self, patient_maps, convergent_map):\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between patient maps and the convergent map.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        patient_maps : np.array\n",
    "            Array of patient maps.\n",
    "        convergent_map : np.array\n",
    "            Convergent map.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of float\n",
    "            List of cosine similarity values.\n",
    "        \"\"\"\n",
    "        if self.similarity == 'cos':\n",
    "            similarities = [self.cosine_similarity(patient_map, convergent_map) for patient_map in patient_maps]\n",
    "        elif self.similarity == 'spcorr':\n",
    "            similarities = [pearsonr(patient_map, convergent_map)[0] for patient_map in patient_maps]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid similarity measure (self.similarity). Please choose 'cos' or 'spcorr'.\")\n",
    "        return similarities\n",
    "    \n",
    "    def cosine_similarity(self, a, b):\n",
    "        \"\"\"\n",
    "        Calculate the cosine similarity between two vectors.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        a : np.array\n",
    "            First vector.\n",
    "        b : np.array\n",
    "            Second vector.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Cosine similarity value.\n",
    "        \"\"\"\n",
    "        numerator = np.dot(a, b)\n",
    "        denominator = np.sqrt(np.sum(a**2)) * np.sqrt(np.sum(b**2))\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def correlate_similarity_with_outcomes(self, similarities, indep_var):\n",
    "        \"\"\"\n",
    "        Correlate similarity values with independent variables and calculate confidence intervals.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        similarities : list of float\n",
    "            List of cosine similarity values.\n",
    "        indep_var : np.array\n",
    "            Array of independent variable values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            R-value, lower confidence interval, and upper confidence interval.\n",
    "        \"\"\"\n",
    "        resampled_r = []\n",
    "        observed_r = 0\n",
    "        for _ in tqdm(range(self.n_bootstrap), 'Running bootstraps'):\n",
    "            if _ == 0:\n",
    "                resampled_indices = np.arange(len(similarities)) # No replacement for the first iteration\n",
    "            else:\n",
    "                resampled_indices = np.random.choice(len(similarities), len(similarities), replace=True)\n",
    "            resampled_similarities = np.array(similarities)[resampled_indices]\n",
    "            resampled_indep_var = np.array(indep_var)[resampled_indices]\n",
    "            if self.method == 'spearman':\n",
    "                resampled_r.append(spearmanr(resampled_similarities, resampled_indep_var)[0])\n",
    "            else:\n",
    "                resampled_r.append(pearsonr(resampled_similarities, resampled_indep_var)[0])\n",
    "                \n",
    "            if _ == 0:\n",
    "                observed_r = resampled_r\n",
    "                \n",
    "        ci_lower = np.percentile(resampled_r, 2.5)\n",
    "        ci_upper = np.percentile(resampled_r, 97.5)\n",
    "        mean_r = np.mean(resampled_r)\n",
    "        return observed_r, ci_lower, ci_upper, mean_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LOOCVAnalyzer class\n",
    "loocv_analyzer = LOOCVAnalyzer(corr_map_dict, data_loader, method='spearman', n_bootstrap=1000, out_dir=None)\n",
    "results = loocv_analyzer.perform_loocv()\n",
    "display(loocv_analyzer.results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (nimlab)",
   "language": "python",
   "name": "nimlab_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
