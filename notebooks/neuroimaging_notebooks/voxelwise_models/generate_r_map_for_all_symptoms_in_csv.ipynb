{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Instructions\n",
    "This is a program which will create R-Maps. These are used to correlate a voxel to a continuous outcome measure.\n",
    "As it stands, this software employs Pearson Correlation Coefficients, which imply it will be best to have a continuous outcome on a percent scale. If you would like to do otherwise, a Spearman Correlation is possible. \n",
    "\n",
    "The software will walk you through everything. \n",
    "\n",
    "_____\n",
    "# Nifti Configuration\n",
    "\n",
    "**Files are expected to follow a BIDS naming convention.**\n",
    "\n",
    "**Files are expected to have subject ID in them which is identical to subject ID in the CSV**\n",
    "\n",
    "**Files are expected to be in 2x2x2 resolution**\n",
    "_____\n",
    "# CSV configuration:\n",
    "**Subject IDs expected to be in the nifti names**\n",
    "\n",
    "**Subject IDs expected to be in a column of your target CSV labelled \"subject\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Information\n",
    "\n",
    "-Enter the directory you would like to save to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/Users/cu135/Partners HealthCare Dropbox/Calvin Howard/resources/published_networks/Alzheimer Cognition Maps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Instructions**: Please fill out the `path` and `file_pattern` variables. \n",
    "\n",
    "The file_path is the shared base directory holding all files. ie) blah/blah/blah/BIDS\n",
    "\n",
    "The file_pattern is the shared naming architectur in all files ie)  * / * / * subT1 * .nii\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shared path to the folder/csv containing the nifti files/files paths for the neuroimaging files?\n",
    "path_1 = '/Volumes/Expansion/datasets/adni/neuroimaging/all_patients_atrophy_csfgm_connectivity/sub-*+cerebrospinalufluid/connectivity'\n",
    "\n",
    "#What is the shared file architecture of your neuroimaging files after the base path?\n",
    "file_pattern = 'sub-*+cerebrospinalufluid_tome-GSP1000uMF_space-2mm_stat-t_conn.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------DO NOT TOUCH--------------------------------------------------------\n",
    "import os\n",
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "df_1 = import_matrices_from_folder(path_1, file_pattern=file_pattern, subject_id_index=5)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Subject ID From File Names**\n",
    "Using the example filenames that have been printed above, please define a general string:\n",
    "1) Preceding the subject ID. For example in 04-mwp1glanat_resampled.nii, this is \" \"\n",
    "2) Proceeding the subject ID. For example in 04-mwp1glanat_resampled.nii, this is \"-mwp1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.dataframe_utilities import extract_and_rename_subject_id\n",
    "\n",
    "def preprocess_names(df, string_preceding_id, string_proceeding_id, cols=True):\n",
    "    \"\"\"\n",
    "    Preprocess the given dataframe by extracting and renaming the subject ID, \n",
    "    then transposing the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The dataframe to preprocess.\n",
    "    - string_preceding_id: String preceding the subject ID.\n",
    "    - string_proceeding_id: String proceeding the subject ID.\n",
    "\n",
    "    Returns:\n",
    "    - The preprocessed dataframe.\n",
    "    \"\"\"\n",
    "    split_command_dict = {string_preceding_id: 1, string_proceeding_id: 0}\n",
    "    if cols:\n",
    "        df = extract_and_rename_subject_id(dataframe=df, split_command_dict=split_command_dict).transpose()\n",
    "    else:\n",
    "        df = extract_and_rename_subject_id(dataframe=df, split_command_dict=split_command_dict)\n",
    "    df.index.name = 'subject'\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_preceding_id = 'neuroimaging_sub-subh'\n",
    "string_proceeding_id = 'ugreyumatter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = preprocess_names(df_1, string_preceding_id, string_proceeding_id)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle NaNs if you'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_1[:] = np.nan_to_num(df_1.values, nan=0, posinf=30, neginf=-30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the path to the CSV which has your clinical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2 = '/Volumes/Expansion/datasets/adni/metadata/updated_master_list/adas_cog_1year.csv'\n",
    "excel_sheet_name = None #Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a CSV with the clinical data of interest\n",
    "import pandas as pd\n",
    "if os.path.basename(path_2).split('.')[1] == 'csv':\n",
    "    df_2 = pd.read_csv(path_2)\n",
    "else:\n",
    "    df_2 = pd.read_excel(path_2, sheet_name=excel_sheet_name)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Specific Columns to Keep in the Second List\n",
    "- Example: #df_2 = df_2.loc[:, ['Unnamed: 0', 'DBS response ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_cols_to_keep = ['subject', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6',\n",
    "       'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q14', 'Total', 'TotalMOD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.loc[:, list_of_cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix Subject Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_subject_column_to_subject(df, subject_column, string_preceding_id='', string_proceeding_id='',):\n",
    "    popped_column = df.pop(subject_column)\n",
    "    df['subject'] = popped_column\n",
    "    \n",
    "    if all(df[subject_column].apply(lambda x: type(x) is str)):\n",
    "        if string_proceeding_id != '':\n",
    "            df[subject_column] = [name.split(string_proceeding_id)[0] for name in df[subject_column]]\n",
    "        if string_preceding_id != '':\n",
    "            df[subject_column] = [name.split(string_preceding_id)[1] for name in df[subject_column]]\n",
    "        print('extracting subject ID')\n",
    "    else:\n",
    "        df[subject_column] = df[subject_column].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_column = 'subject'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_preceding_id = ''\n",
    "string_proceeding_id = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected there is a columnc called which has subject information in it. The information in this column must correspond in the dataframe above. If it does not exist, add it to your CSV before proceeding. \n",
    "\n",
    "Define the column below using:\n",
    "\n",
    "subject_colum = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calvin_utils.statistical_utils.voxelwise_statistical_testing import generate_r_map\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti\n",
    "\n",
    "def reset_index_if_subject_is_index(df, subject_column):\n",
    "    if subject_column in df.index.names:\n",
    "        df.reset_index(inplace=True)\n",
    "        # Reorder columns to make 'subject' the first column\n",
    "        cols = [subject_column] + [col for col in df.columns if col != subject_column]\n",
    "        df = df[cols]\n",
    "    return df\n",
    "\n",
    "def process_and_generate_maps(df_1, df_2, subject_column='subject', out_dir='', mask_path=None, method='pearson'):\n",
    "    \"\"\"\n",
    "    Process the given dataframes, and generate maps based on the columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df_1: First dataframe.\n",
    "    - df_2: Second dataframe.\n",
    "    - subject_column: The column name referring to the subject.\n",
    "    - out_dir: The output directory to save the generated maps.\n",
    "    \"\"\"\n",
    "    # Check if 'subject' is in the index or columns for df_1\n",
    "    if subject_column in df_1.index.names:\n",
    "        df_1 = reset_index_if_subject_is_index(df_1, subject_column)\n",
    "    if subject_column in df_2.index.names:\n",
    "        df_2 = reset_index_if_subject_is_index(df_2, subject_column)\n",
    "        \n",
    "    # Process subject column to string    \n",
    "    df_1[subject_column] = df_1[subject_column].astype(int)\n",
    "    df_2[subject_column] = df_2[subject_column].astype(int)\n",
    "    df_1.fillna(0)\n",
    "    \n",
    "    # Iterate over column, avoiding the one with subject id in it\n",
    "    for colname in [col for col in df_2.columns if col != subject_column]:\n",
    "        print(f'Working on {colname}')\n",
    "        merged_df = df_2[[colname, subject_column]].merge(df_1, on=subject_column, how='inner').set_index(subject_column)\n",
    "        \n",
    "        # Remove any rows with NaN values\n",
    "        copy_df = merged_df.copy()\n",
    "        try:\n",
    "            merged_df.dropna(inplace=True)\n",
    "            r_df, p_df, r_squared_df = generate_r_map(merged_df, mask_path=mask_path, method=method)\n",
    "\n",
    "            view_and_save_nifti(p_df, os.path.join(out_dir, 'p_map', colname))\n",
    "            view_and_save_nifti(r_df, os.path.join(out_dir, 'r_map', colname))\n",
    "            view_and_save_nifti(r_squared_df, os.path.join(out_dir, 'r_squared_map', colname))\n",
    "        except Exception as e:\n",
    "            if \"x and y must have length at least 2\" in str(e):\n",
    "                print('Caught exception: NaNs or Infs suspected in input data. Trying workaround.')\n",
    "                copy_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                copy_df.fillna(0, inplace=True)\n",
    "                r_df, p_df, r_squared_df = generate_r_map(copy_df, mask_path=mask_path)\n",
    "\n",
    "                view_and_save_nifti(p_df, os.path.join(out_dir, 'p_map', colname))\n",
    "                view_and_save_nifti(r_df, os.path.join(out_dir, 'r_map', colname))\n",
    "                view_and_save_nifti(r_squared_df, os.path.join(out_dir, 'r_squared_map', colname))\n",
    "            else:\n",
    "                print(f'Error: {e}')\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting subject ID\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '002uSu0295'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_2 \u001b[38;5;241m=\u001b[39m set_subject_column_to_subject(df_2, subject_column\u001b[38;5;241m=\u001b[39msubject_column, string_preceding_id\u001b[38;5;241m=\u001b[39mstring_preceding_id, string_proceeding_id\u001b[38;5;241m=\u001b[39mstring_proceeding_id)\n\u001b[0;32m----> 2\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_and_generate_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43msubject_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubject_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmask_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/cu135/hires_backdrops/MNI/MNI152_T1_2mm_brain_mask.nii\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspearman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 30\u001b[0m, in \u001b[0;36mprocess_and_generate_maps\u001b[0;34m(df_1, df_2, subject_column, out_dir, mask_path, method)\u001b[0m\n\u001b[1;32m     27\u001b[0m     df_2 \u001b[38;5;241m=\u001b[39m reset_index_if_subject_is_index(df_2, subject_column)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Process subject column to string    \u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m df_1[subject_column] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_1\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubject_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m df_2[subject_column] \u001b[38;5;241m=\u001b[39m df_2[subject_column]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     32\u001b[0m df_1\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/Python_3.7.7_nimlab/lib/python3.11/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/Python_3.7.7_nimlab/lib/python3.11/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/Python_3.7.7_nimlab/lib/python3.11/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/.virtualenvs/Python_3.7.7_nimlab/lib/python3.11/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/Python_3.7.7_nimlab/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.virtualenvs/Python_3.7.7_nimlab/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/.virtualenvs/Python_3.7.7_nimlab/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '002uSu0295'"
     ]
    }
   ],
   "source": [
    "df_2 = set_subject_column_to_subject(df_2, subject_column=subject_column, string_preceding_id=string_preceding_id, string_proceeding_id=string_proceeding_id)\n",
    "merged_df = process_and_generate_maps(df_1.copy(), df_2.copy(), \n",
    "                                      subject_column=subject_column, \n",
    "                                      out_dir=out_dir, \n",
    "                                      mask_path='/Users/cu135/hires_backdrops/MNI/MNI152_T1_2mm_brain_mask.nii', \n",
    "                                      method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your R-Maps have all been generated. Consider adding Calvin as a collaborator if this was useful!\n",
    "\n",
    "-- Calvin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Perform Delta R-Map and Permute it for Significance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the Observed Delta-R Map Between 2 Populations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.voxelwise_statistical_testing import generate_delta_r_map\n",
    "delta_matrix = merged_df.copy()\n",
    "observed_delta_r_map = generate_delta_r_map(delta_matrix, threshold_of_interest=65, column_of_interest='Age at DOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti\n",
    "view_and_save_nifti(observed_delta_r_map, (out_dir+'/over_vs_under_65_delta_r_map'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Empiric Delta-R Map Distribution \n",
    "### Note, this permutes the label of the population without permuting the neuroimaging data.\n",
    "### Therefore, we are testing if the separation of the r-maps is significantly due to the variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.voxelwise_statistical_testing import permuted_patient_label_delta_r_map\n",
    "from calvin_utils.file_utils.print_suppression import HiddenPrints\n",
    "n_permutations = 2\n",
    "column_of_interest = 'Age at DOS'\n",
    "threshold_of_interest = 65\n",
    "with HiddenPrints():\n",
    "    p_count_df = permuted_patient_label_delta_r_map(dataframe_to_permute=merged_df, \n",
    "                                                observed_delta_r_map=observed_delta_r_map, \n",
    "                                                column_of_interest=column_of_interest, \n",
    "                                                threshold_of_interest=threshold_of_interest, \n",
    "                                                n_permutations=n_permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti\n",
    "view_and_save_nifti(p_values_df, (out_dir+'/over_vs_under_65_delta_r_map_p_values_df'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Spatial Correlation of R-Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare a Second Set of Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Neuroimaging Path Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shared path to the folder/csv containing the nifti files/files paths for the neuroimaging files?\n",
    "path_1 = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/derivatives/third_level/vta_connectivity'\n",
    "\n",
    "#What is the shared file architecture of your neuroimaging files after the base path?\n",
    "file_pattern = '*fMRI_T.nii*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------DO NOT TOUCH--------------------------------------------------------\n",
    "import os\n",
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "df_1B = import_matrices_from_folder(path_1, file_pattern=file_pattern, subject_id_index=5)\n",
    "df_1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean subject names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_preceding_id = 'datasets_MDST'\n",
    "string_proceeding_id = '_seed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1B = preprocess_names(df_1B, string_preceding_id, string_proceeding_id)\n",
    "df_1B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Clinical CSV Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2 = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects.xlsx'\n",
    "excel_sheet_name = 'master_list_proper_subjects' #Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a CSV with the clinical data of interest\n",
    "import pandas as pd\n",
    "if os.path.basename(path_2).split('.')[1] == 'csv':\n",
    "    df_2B = pd.read_csv(path_2)\n",
    "else:\n",
    "    df_2B = pd.read_excel(path_2, sheet_name=excel_sheet_name)\n",
    "df_2B = df_2B[df_2B['City']=='Wurzburg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2B = df_2B.loc[:, ['subject', 'Z-Scored Percent Cognitive Improvement']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_column = 'subject'\n",
    "string_preceding_id = ''\n",
    "string_proceeding_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process df_2 subject names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2B = set_subject_column_to_subject(df_2B, subject_column=subject_column, string_preceding_id=string_preceding_id, string_proceeding_id=string_proceeding_id)\n",
    "df_2B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Spatial Correlatio Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from calvin_utils.statistical_utils.voxelwise_statistical_testing import generate_r_map\n",
    "\n",
    "# # Define a dummy tqdm function\n",
    "# def dummy_tqdm(*args, **kwargs):\n",
    "#     if 'iterable' in kwargs:\n",
    "#         return kwargs['iterable']\n",
    "#     return args[0] if args else range(0)\n",
    "\n",
    "# @contextmanager\n",
    "# def suppress_print():\n",
    "#     # Save the original tqdm and stdout\n",
    "#     original_tqdm = tqdm\n",
    "#     original_stdout = sys.stdout\n",
    "\n",
    "#     # Replace tqdm with the dummy function and stdout with a null device\n",
    "#     tqdm = dummy_tqdm\n",
    "#     sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "#     try:\n",
    "#         yield\n",
    "#     finally:\n",
    "#         # Restore the original tqdm and stdout\n",
    "#         tqdm = original_tqdm\n",
    "#         sys.stdout = original_stdout\n",
    "        \n",
    "@contextmanager\n",
    "def suppress_print():\n",
    "    # Open a null device\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        # Save the current stdout\n",
    "        old_stdout = sys.stdout\n",
    "        # Redirect the current stdout to the null device\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            # Yield back to the calling function\n",
    "            yield\n",
    "        finally:\n",
    "            # Restore the original stdout\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "class SpatialCorrelRMaps:\n",
    "    def __init__(self, df_1, df_2, df_1B, df_2B, subject_column='subject', out_dir=None, mask_path=None, method='pearson'):\n",
    "        self.df_1 = df_1\n",
    "        self.df_2 = df_2\n",
    "        self.df_1B = df_1B\n",
    "        self.df_2B = df_2B\n",
    "        self.subject_column = subject_column\n",
    "        self.out_dir = out_dir\n",
    "        self.mask_path = mask_path\n",
    "        self.method = method\n",
    "\n",
    "\n",
    "    def spatial_correlation(self, r_map1, r_map2):\n",
    "        r_map1 = np.nan_to_num(r_map1.to_numpy().flatten(), neginf=1, posinf=0, nan=0)\n",
    "        r_map2 = np.nan_to_num(r_map2.to_numpy().flatten(), neginf=1, posinf=0, nan=0)\n",
    "        correlation_coefficient, _ = scipy.stats.pearsonr(r_map1, r_map2)\n",
    "        return correlation_coefficient\n",
    "\n",
    "    def permute_subjects(self, df):\n",
    "        df_permuted = df.copy()\n",
    "        df_permuted[self.subject_column] = np.random.permutation(df[self.subject_column])\n",
    "        return df_permuted\n",
    "    \n",
    "    def r_map(self, df_1, df_2):\n",
    "        \"\"\"\n",
    "        Process the given dataframes, and generate maps based on the columns.\n",
    "\n",
    "        Parameters:\n",
    "        - df_1: First dataframe.\n",
    "        - df_2: Second dataframe.\n",
    "        - subject_column: The column name referring to the subject.\n",
    "        - out_dir: The output directory to save the generated maps.\n",
    "        \"\"\"\n",
    "        # Check if 'subject' is in the index or columns for df_1\n",
    "        if subject_column in df_1.index.names:\n",
    "            df_1 = reset_index_if_subject_is_index(df_1, subject_column)\n",
    "        if subject_column in df_2.index.names:\n",
    "            df_2 = reset_index_if_subject_is_index(df_2, subject_column)\n",
    "            \n",
    "        # Process subject column to string    \n",
    "        df_1[subject_column] = df_1[subject_column].astype(str)\n",
    "        df_2[subject_column] = df_2[subject_column].astype(str)\n",
    "        \n",
    "        # Iterate over column, avoiding the one with subject id in it\n",
    "        for colname in [col for col in df_2.columns if col != subject_column]:\n",
    "            merged_df = df_2[[colname, subject_column]].merge(df_1, on=subject_column, how='inner').set_index(subject_column)\n",
    "            \n",
    "            # Remove any rows with NaN values\n",
    "            copy_df = merged_df.copy()\n",
    "            try:\n",
    "                merged_df.dropna(inplace=True)\n",
    "                with suppress_print():\n",
    "                    r_df, _, _ = generate_r_map(merged_df, mask_path=self.mask_path, method=self.method, tqdm_on=False)\n",
    "            except Exception as e:\n",
    "                if \"x and y must have length at least 2\" in str(e):\n",
    "                    print('Caught exception: NaNs or Infs suspected in input data. Trying workaround.')\n",
    "                    copy_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                    copy_df.fillna(0, inplace=True)\n",
    "                    r_df, _, _ = generate_r_map(copy_df, mask_path=self.mask_path, tqdm_on=False)\n",
    "                else:\n",
    "                    print(f'Error {e}')\n",
    "        return r_df\n",
    "\n",
    "    def observed_distribution(self):\n",
    "        r_map1 = self.r_map(self.df_1, self.df_2)\n",
    "        r_map2 = self.r_map(self.df_1B, self.df_2B)\n",
    "        observed_corr = self.spatial_correlation(r_map1, r_map2)\n",
    "        return observed_corr\n",
    "\n",
    "    def empiric_distribution(self, n_permutations):\n",
    "        empiric_corrs = []\n",
    "        for _ in tqdm(range(n_permutations)):\n",
    "            permuted_df_2 = self.permute_subjects(self.df_2)\n",
    "            permuted_df_2B = self.permute_subjects(self.df_2B)\n",
    "            r_map1 = self.r_map(self.df_1, permuted_df_2)\n",
    "            r_map2 = self.r_map(self.df_1B, permuted_df_2B)\n",
    "            corr = self.spatial_correlation(r_map1, r_map2)\n",
    "            empiric_corrs.append(corr)\n",
    "        return empiric_corrs\n",
    "    \n",
    "    def p_value(self, observed_corr, empiric_corrs):\n",
    "        \"\"\"\n",
    "        Calculate the p-value for the observed spatial correlation.\n",
    "\n",
    "        Parameters:\n",
    "            observed_corr (float): The observed spatial correlation coefficient.\n",
    "            empiric_corrs (list): A list of spatial correlation coefficients from permuted data.\n",
    "\n",
    "        Returns:\n",
    "            float: The p-value representing the statistical significance of the observed correlation.\n",
    "        \"\"\"\n",
    "        # Count how many empiric correlations are greater than or equal to the observed correlation\n",
    "        count_greater = sum(emp_corr >= observed_corr for emp_corr in empiric_corrs)\n",
    "\n",
    "        # Calculate the p-value (proportion of empiric correlations greater than or equal to the observed)\n",
    "        p_val = count_greater / len(empiric_corrs)\n",
    "        return p_val\n",
    "\n",
    "    def run(self, n_permutations=100):\n",
    "        \"\"\"\n",
    "        Execute the entire process of calculating spatial correlations and p-value.\n",
    "\n",
    "        Parameters:\n",
    "            n_permutations (int): Number of permutations for the empirical distribution calculation.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the observed correlation, the empirical correlations, and the p-value.\n",
    "        \"\"\"\n",
    "        # Calculate the observed spatial correlation\n",
    "        observed_corr = self.observed_distribution()\n",
    "\n",
    "        # Calculate the empirical distribution of spatial correlations from permuted data\n",
    "        empiric_corrs = self.empiric_distribution(n_permutations)\n",
    "\n",
    "        # Calculate the p-value for the observed spatial correlation\n",
    "        p_val = self.p_value(observed_corr, empiric_corrs)\n",
    "        print(f\"Observed Correlation: {observed_corr}\")\n",
    "        print(f\"P-Value: {p_val}\")\n",
    "        return observed_corr, empiric_corrs, p_val\n",
    "\n",
    "\n",
    "    # Placeholder for any additional methods, such as p-value calculation or utility functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run It/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_1, df_2, df_1B, df_2B are predefined DataFrames\n",
    "# Initialize the SpatialCorrelRMaps instance\n",
    "spatial_correl = SpatialCorrelRMaps(df_1, df_2, df_1B, df_2B, \n",
    "                                    subject_column='subject', \n",
    "                                    out_dir=out_dir, \n",
    "                                    mask_path=None, \n",
    "                                    method='pearson')\n",
    "obsv, empir, pval = spatial_correl.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_correl.p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - FWE Corrected R Map "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = '/Users/cu135/Dropbox (Partners HealthCare)/studies/atrophy_seeds_2023/metadata/experiment_metadata/q4_regression.csv'\n",
    "out_dir = '/Users/cu135/Dropbox (Partners HealthCare)/studies/atrophy_seeds_2023/Figures/r_maps_to_praxis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(input_csv_path, index_col=0)\n",
    "data_df = data_df.dropna(axis=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the columns to keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Niftis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = '/Users/cu135/Dropbox (Partners HealthCare)/studies/atrophy_seeds_2023/shared_analysis/niftis_for_elmira/smoothed_atrophy_seeds'\n",
    "file_target = '*/*/unthresholded_tissue_segment_z_scores/*cerebrospinal_fluid_generated_nifti_no*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_functions import GiiNiiFileImport\n",
    "giinii = GiiNiiFileImport(import_path=import_path, file_column=None, file_pattern=file_target)\n",
    "nimg_df = giinii.run()\n",
    "nimg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = 'sub-'\n",
    "post = '_cerebro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimg_df = giinii.splice_colnames(nimg_df, pre, post)\n",
    "nimg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "from typing import Tuple\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti\n",
    "\n",
    "class CalvinFWEMap():\n",
    "    \"\"\"\n",
    "    This is a class to orchestrate a simple association between some Y variable of interest and voxelwise data (X variable)\n",
    "    It will run FWE correction via the Maximum Statistic Correction method. \n",
    "    \"\"\"\n",
    "    def __init__(self, neuroimaging_dataframe: pd.DataFrame, variable_dataframe: pd.DataFrame, method: str='spearman', mask_path=None, mask_threshold: int=0.0, out_dir=''):\n",
    "        \"\"\"\n",
    "        Need to provide the dataframe dictionaries and dataframes of importance. \n",
    "        \n",
    "        Args:\n",
    "        - neuroimaging_dataframe (df): DF with neuroimaging data (voxelwise dataframe) column represents represents a subject,\n",
    "                                        and each row represents a voxel.\n",
    "        - variable_dataframe (pd.DataFrame): DataFrame where each column represents represents a subject,\n",
    "                                        and each row represents the variable to regress upon. \n",
    "        - method (str): the association method to relate the voxelwise data to. Defaults to spearman correlation\n",
    "                                        options: spearman | pearson | regression\n",
    "        - mask_path (str): the path to the mask you want to use. \n",
    "                                        If None, will threshold the voxelwise image itself by mask_threshold.\n",
    "        - mask_threshold (int): The threshold to mask the neuroimaging data at.\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.mask_path = mask_path\n",
    "        self.mask_threshold = mask_threshold\n",
    "        neuroimaging_dataframe, self.variable_dataframe = self.sort_dataframes(covariate_df=variable_dataframe, voxel_df=neuroimaging_dataframe)\n",
    "        self.original_mask, self.nonzero_mask, self.neuroimaging_dataframe = self.mask_dataframe(neuroimaging_dataframe)\n",
    "        self.out_dir = out_dir\n",
    "\n",
    "        \n",
    "    def sort_dataframes(self, voxel_df: pd.DataFrame, covariate_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Will sort the rows of the voxelwise DF and the covariate DF to make sure they are identically organized.\n",
    "        Then will check that the columns are equivalent. \n",
    "        \"\"\"\n",
    "        # Force Columns to Match\n",
    "        voxel_cols = set(voxel_df.columns.astype(str).sort_values().values)\n",
    "        covariate_cols = set(covariate_df.columns.astype(str).sort_values().values)\n",
    "        shared_columns = list(voxel_cols.intersection(covariate_cols))\n",
    "        \n",
    "        # This will occur when columns have strange naming, such as subject 1 being 0001 verus 1. \n",
    "        if len(shared_columns) == 0:\n",
    "            voxel_cols = voxel_df.columns.astype(int).astype(str).sort_values().values\n",
    "            covariate_cols = covariate_df.columns.astype(int).astype(str).sort_values().values\n",
    "            \n",
    "            voxel_df.columns = voxel_cols\n",
    "            covariate_df.columns = covariate_cols\n",
    "            \n",
    "            shared_columns = list(set(voxel_cols).intersection(set(covariate_cols)))\n",
    "            \n",
    "        return voxel_df.loc[:, shared_columns], covariate_df.loc[:, shared_columns]\n",
    "    \n",
    "    def threshold_probabilities(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Apply a threshold to mask raw voxelwise data. \n",
    "        Finds all voxels which are nonzero across all rows and create a mask from them. \n",
    "        \n",
    "        Parameters:\n",
    "        df (pd.DataFrame): DataFrame with voxelwise data.\n",
    "        \n",
    "        Returns:\n",
    "        pd.Series: Mask of nonzero voxels.\n",
    "        \"\"\"\n",
    "        if self.mask_path is not None: \n",
    "            mask_data = nib.load(self.mask_path).get_fdata()\n",
    "            mask_data = pd.DataFrame(mask_data, index=df.index, columns=df.columns)\n",
    "            mask_data = mask_data.where(df > self.mask_threshold, 0)\n",
    "        else:\n",
    "            mask_data = df.where(df > self.mask_threshold, 0)\n",
    "\n",
    "        mask = mask_data.sum(axis=1) > 0\n",
    "        return mask\n",
    "    \n",
    "    def mask_dataframe(self, neuroimaging_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Apply a mask to the neuroimaging DataFrame based on nonzero voxels.\n",
    "        \n",
    "        Parameters:\n",
    "        neuroimaging_df (pd.DataFrame): DataFrame with neuroimaging data.\n",
    "        \n",
    "        Returns:\n",
    "        pd.Index: Index of the whole DataFrame.\n",
    "        pd.Series: Mask of nonzero voxels.\n",
    "        pd.DataFrame: Masked neuroimaging DataFrame.\n",
    "        \"\"\"\n",
    "        # Now you can use the function to apply a threshold to patient_df and control_df\n",
    "        mask = self.threshold_probabilities(neuroimaging_df)\n",
    "        \n",
    "        original_mask = neuroimaging_df.index\n",
    "        masked_neuroimaging_df = neuroimaging_df.loc[mask, :]\n",
    "        return original_mask, mask, masked_neuroimaging_df\n",
    "    \n",
    "    def unmask_dataframe(self, df:pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Simple unmasking function.\n",
    "        \"\"\"\n",
    "        # Initialize a new DF\n",
    "        empty_mask = pd.DataFrame(index=self.original_mask, columns=['voxels'], data=0)\n",
    "\n",
    "        # Insert data into the DF \n",
    "        empty_mask.loc[self.nonzero_mask, :] = df.values.reshape(-1, 1)\n",
    "        return empty_mask\n",
    "    \n",
    "    def mask_by_p_values(self, results_df:pd.DataFrame, p_values_df:pd.DataFrame):\n",
    "        \"\"\"Simple function to perform the thresholding by FWE corrected p-values\"\"\"\n",
    "        unmasked_df = results_df.copy()\n",
    "        \n",
    "        mask = p_values_df.where(p_values_df < 0.05, 0)\n",
    "        mask = mask.sum(axis=1) == 0\n",
    "        \n",
    "        unmasked_df.loc[mask, :] = 0\n",
    "        return unmasked_df\n",
    "    \n",
    "    def permute_covariates(self):\n",
    "        \"\"\"Permute the patient data by randomly assigning patient data (columnar data) to new patients (columns)\"\"\"\n",
    "        return self.variable_dataframe.sample(frac=1, axis=1, random_state=None)\n",
    "    \n",
    "    def linear_regression(self, permuted_variable_df: pd.DataFrame=None, use_intercept: bool=True, debug: bool=False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate voxelwise relationship to Y variable with linear regression.\n",
    "        It is STRONGLY advised to set mask=True when running this.\n",
    "\n",
    "        This function performs a linear regression using sklearn's LinearRegression\n",
    "        The regression is done once across all voxels simultaneously, \n",
    "        treating each voxel's values across subjects as independent responses. \n",
    "        This vectorized approach efficiently handles the calculations by leveraging matrix operations, \n",
    "        which are computationally optimized in libraries like numpy and sklearn.\n",
    "\n",
    "        Args:\n",
    "            use_intercept (bool): if true, will add intercept to the regression\n",
    "            debug (bool): if true, prints out summary metrics\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame:\n",
    "        \"\"\"\n",
    "        # Design matrix X for control group, outcomes Y for control group\n",
    "        if permuted_variable_df is not None:\n",
    "            X = permuted_variable_df.T\n",
    "        else:\n",
    "            X = self.variable_dataframe.T \n",
    "        Y = self.neuroimaging_dataframe.T.values\n",
    "        \n",
    "        # Fit model on control data across all voxels\n",
    "        model = LinearRegression(fit_intercept=use_intercept)\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        # Predict on experimental group and calculate R-squared\n",
    "        Y_HAT = model.predict(X)\n",
    "        Y_BAR = np.mean(Y, axis=0, keepdims=True)\n",
    "        SSE = np.sum( (Y_HAT - Y_BAR)**2, axis=0)\n",
    "        SST = np.sum( (Y     - Y_BAR)**2, axis=0)\n",
    "        R2 = SSE/SST\n",
    " \n",
    "        if debug:\n",
    "            print(X.shape, Y.shape, Y_HAT.shape, Y_BAR.shape, SSE.shape, SST.shape, R2.shape)\n",
    "            print('Observed R2 max: ', np.max(R2))\n",
    "            \n",
    "        # Reshape R2 to DataFrame format\n",
    "        R2_df = pd.DataFrame(R2.T, index=self.neuroimaging_dataframe.index, columns=['R2'])\n",
    "        return R2_df\n",
    "    \n",
    "    def maximum_stat_fwe(self, n_permutations=100, debug=False):\n",
    "        \"\"\"\n",
    "        Perform maximum statistic Family-Wise Error (FWE) correction using permutation testing.\n",
    "\n",
    "        This method calculates the maximum voxelwise R-squared values across multiple permutations\n",
    "        of the covariates. It then uses these maximum statistics to correct for multiple comparisons,\n",
    "        ensuring robust and conservative statistical inference.\n",
    "\n",
    "        Args:\n",
    "            n_permutations (int): Number of permutations to perform. Defaults to 100.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of maximum R-squared values from each permutation.\n",
    "        \"\"\"\n",
    "        max_stats = []\n",
    "        for i in tqdm(range(0, n_permutations), desc='Permuting'):\n",
    "            permuted_covariates = self.permute_covariates()\n",
    "            permuted_R2_df = self.linear_regression(permuted_covariates, debug=False)\n",
    "            max_stat = np.max(permuted_R2_df)\n",
    "            max_stats.append(max_stat)\n",
    "            if debug:\n",
    "                print('Permutation max stat: ', max_stat)\n",
    "        return max_stats\n",
    "            \n",
    "    def p_value_calculation(self, uncorrected_df, max_stat_dist, debug=False):\n",
    "        \"\"\"\n",
    "        Calculate p-values for the uncorrected statistic values using the distribution of maximum statistics.\n",
    "\n",
    "        Args:\n",
    "            uncorrected_df (pd.DataFrame): DataFrame of uncorrected statistic values.\n",
    "            max_stat_dist (list): Distribution of maximum statistic values from each permutation.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of p-values corresponding to the uncorrected statistic values.\n",
    "        \"\"\"\n",
    "        max_stat_dist = np.array(max_stat_dist)\n",
    "        max_stat_dist = max_stat_dist[:, np.newaxis]\n",
    "        p_values = np.mean(max_stat_dist >= uncorrected_df.values, axis=0)\n",
    "        p_values_df = uncorrected_df.copy()\n",
    "        p_values_df.loc[:,:] =p_values\n",
    "        if debug:\n",
    "            print(p_values_df.shape)\n",
    "        return p_values_df\n",
    "\n",
    "    def save_single_nifti(self, nifti_df, out_dir, name='generated_nifti', silent=True):\n",
    "        \"\"\"Saves NIFTI images to directory.\"\"\"\n",
    "        preview = view_and_save_nifti(matrix=nifti_df,\n",
    "                            out_dir=out_dir,\n",
    "                            output_name=name,\n",
    "                            silent=silent)\n",
    "        return preview\n",
    "        \n",
    "    def save_results(self, voxelwise_results, unmasked_p_values, voxelwise_results_fwe):\n",
    "        \"\"\"\n",
    "        Saves the generated files. \n",
    "        \"\"\"\n",
    "        self.uncorrected_img = self.save_single_nifti(nifti_df=voxelwise_results, out_dir=self.out_dir, name='uncorrected_results', silent=False)\n",
    "        self.p_img = self.save_single_nifti(nifti_df=unmasked_p_values, out_dir=self.out_dir, name='p_values', silent=False)\n",
    "        self.corrected_img = self.save_single_nifti(nifti_df=voxelwise_results_fwe, out_dir=self.out_dir, name='fwe_corrected_results', silent=False)\n",
    "\n",
    "    def run(self, n_permutations=100, debug=False):\n",
    "        \"\"\"\n",
    "        Orchestration method. \n",
    "        \"\"\"\n",
    "        #Can be abstracted to run the analysis of choice and return it and the p-values\n",
    "        voxelwise_results = self.linear_regression(debug=debug)\n",
    "        max_stat_dist = self.maximum_stat_fwe(n_permutations=n_permutations, debug=debug)\n",
    "        p_values = self.p_value_calculation(voxelwise_results, max_stat_dist, debug=debug)\n",
    "        # \n",
    "        voxelwise_results = self.unmask_dataframe(voxelwise_results)\n",
    "        unmasked_p_values = self.unmask_dataframe(p_values)\n",
    "        voxelwise_results_fwe = self.mask_by_p_values(results_df=voxelwise_results, p_values_df=unmasked_p_values)\n",
    "        self.save_results(voxelwise_results, unmasked_p_values, voxelwise_results_fwe)\n",
    "        if debug:\n",
    "            print(np.max(voxelwise_results), np.max(unmasked_p_values), np.max(voxelwise_results_fwe))\n",
    "            print(voxelwise_results.shape, unmasked_p_values.shape, voxelwise_results_fwe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calvin_fwe = CalvinFWEMap(neuroimaging_dataframe=nimg_df, variable_dataframe=data_df, mask_threshold=0, out_dir=out_dir)\n",
    "calvin_fwe.run(n_permutations=1000, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calvin_fwe.corrected_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calvin_fwe.save_results(out_dir=out_dir)\n",
    "calvin_fwe.uncorrected_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(calvin_fwe.voxelwise_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy\n",
    "\n",
    "--Calvin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.7.7_nimlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
