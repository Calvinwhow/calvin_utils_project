{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import glob as glob\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "#Calculate Correlation\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import platform\n",
    "\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory to Save To "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/derivatives/response_topology/voxelwise_glm/stim_by_age/interaction_coefficient'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_path = '/Users/cu135/Dropbox (Partners HealthCare)/studies/cognition_2023/metadata/master_list_proper_subjects.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------user input above----------------------------------------------------------------\n",
    "data_df = pd.read_csv(clin_path)\n",
    "    \n",
    "# #Remove subjects\n",
    "# outlier_index=[11, 47, 48, 49]\n",
    "# data_df = data_df.drop(index=outlier_index)\n",
    "# data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_nans = True\n",
    "column_to_drop_from = 'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_nans:\n",
    "    if column_to_drop_from == None:\n",
    "        data_df.dropna(inplace=True)\n",
    "    else:\n",
    "        data_df.dropna(subset=[column_to_drop_from], inplace=True)\n",
    "# data_df = data_df.fillna(method='pad')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Subset of dataframe by Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_split_by = 'Cohort'\n",
    "value_to_split_by = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[data_df[column_to_split_by] == value_to_split_by]\n",
    "dropped_df = data_df[data_df[column_to_split_by] != value_to_split_by]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Columns of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_column = 'subject'\n",
    "outcome_column = 'Z-Scored Percent Cognitive Improvement'\n",
    "#----------------------------------------------------------------DO NOT MODIFY--------------------------------------------------------\n",
    "outcomes_df = pd.DataFrame()\n",
    "outcomes_df['outcome'] = data_df.loc[:, [outcome_column]]\n",
    "\n",
    "#Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "outcomes_df.iloc[:,:] = scaler.fit_transform(outcomes_df.iloc[:,:])\n",
    "\n",
    "\n",
    "outcomes_df['subject_id'] = data_df.loc[:, [subject_column]]\n",
    "# outcomes_df['subject_id'] = [id.split('_')[0] for id in data_df[subject_column].to_list()] #<------------------------problem code. must generalize to extract subject id better\n",
    "outcomes_df.set_index('subject_id', inplace=True)\n",
    "\n",
    "# data_df = data_df.set_index('Patient # CDR, ADAS')\n",
    "# data_df['subject_id'] = data_df.index\n",
    "# data_df['outcome'] = data_df\n",
    "\n",
    "# Convert the 'subject_id' column to strings for each DataFrame\n",
    "outcomes_df.index = outcomes_df.index.astype(str)\n",
    "display(outcomes_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get More Clinical Data As Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_column = 'subject'\n",
    "clinical_information_column = 'Age'\n",
    "#----------------------------------------------------------------DO NOT MODIFY--------------------------------------------------------\n",
    "clinical_df_1 = pd.DataFrame()\n",
    "clinical_df_1['clinical_data'] = data_df.loc[:, [clinical_information_column]]\n",
    "\n",
    "clinical_df_1['subject_id'] = data_df.loc[:, [subject_column]]\n",
    "# clinical_df_1['subject_id'] = [id.split('_')[0] for id in data_df[subject_column].to_list()] \n",
    "clinical_df_1.set_index('subject_id', inplace=True)\n",
    "\n",
    "# Convert the 'subject_id' column to strings for each DataFrame\n",
    "clinical_df_1.index = clinical_df_1.index.astype(str)\n",
    "display(clinical_df_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Imaging Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to neuroimaging files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shared path to the folder/csv containing the nifti files/files paths for the neuroimaging files?\n",
    "path_1 = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/connectivity_data/vta_published_t_connectivity'\n",
    "\n",
    "#What is the shared file architecture of your neuroimaging files after the base path?\n",
    "file_pattern = '*.nii*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "neuroimaging_df_1 = import_matrices_from_folder(path_1, file_pattern=file_pattern)\n",
    "neuroimaging_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Subject IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.dataframe_utilities import extract_and_rename_subject_id\n",
    "\n",
    "def preprocess_names(df, string_preceding_id, string_proceeding_id, cols=True):\n",
    "    \"\"\"\n",
    "    Preprocess the given dataframe by extracting and renaming the subject ID, \n",
    "    then transposing the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The dataframe to preprocess.\n",
    "    - string_preceding_id: String preceding the subject ID.\n",
    "    - string_proceeding_id: String proceeding the subject ID.\n",
    "\n",
    "    Returns:\n",
    "    - The preprocessed dataframe.\n",
    "    \"\"\"\n",
    "    split_command_dict = {string_preceding_id: 1, string_proceeding_id: 0}\n",
    "    if cols:\n",
    "        df = extract_and_rename_subject_id(dataframe=df, split_command_dict=split_command_dict).transpose()\n",
    "    else:\n",
    "        df = extract_and_rename_subject_id(dataframe=df, split_command_dict=split_command_dict)\n",
    "    df.index.name = 'subject'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter String Patterns preceding and proceeding subject ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_preceding_id = ' '\n",
    "string_proceeding_id = '_vat_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroimaging_df_1 = preprocess_names(neuroimaging_df_1, string_preceding_id, string_proceeding_id)\n",
    "neuroimaging_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import More Neuroimaging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What is the shared path to the folder/csv containing the nifti files/files paths for the neuroimaging files?\n",
    "# path_2 = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/derivatives/third_level/vta_connectivity'\n",
    "\n",
    "# #What is the shared file architecture of your neuroimaging files after the base path?\n",
    "# file_pattern = '*_T*.nii*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "# neuroimaging_df_2 = import_matrices_from_folder(path_2, file_pattern=file_pattern)\n",
    "# neuroimaging_df_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Join Imaging Data to the Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #only operate within the brain mask\n",
    "# from nimlab import datasets as nimds\n",
    "# mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "# mask_data = mni_mask.get_fdata().flatten()\n",
    "# brain_indices = np.where(mask_data > 0)[0]\n",
    "# prepped_matrix = prepped_matrix.iloc[brain_indices, :].reset_index()\n",
    "# prepped_matrix = prepped_matrix.transpose()\n",
    "\n",
    "# #Now, join voxels to the clinical data\n",
    "# regression_df = data_df.merge(neuroimaging_df_1, on='Patient # CDR, ADAS', right_index=False)\n",
    "\n",
    "# display(regression_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.nifti_utils.matrix_utilities import unmask_matrix, mask_matrix\n",
    "neuroimaging_df_1 = mask_matrix(neuroimaging_df_1, mask_path=None, mask_by='columns')\n",
    "neuroimaging_df_1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Statistical Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moderated Mediation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "def calculate_confidence_intervals(ab_paths, mediators):\n",
    "    \"\"\"\n",
    "    Calculates the confidence intervals and p-value based on the bootstrapped samples.\n",
    "\n",
    "    Parameters:\n",
    "    - ab_paths: list of lists containing the bootstrapped ab paths for each mediator.\n",
    "    - total_indirect_effects: list of bootstrapped summed ab paths.\n",
    "    - mediators: list of mediator names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the mean indirect effect, confidence intervals, and p-values for each mediator and the total indirect effect.\n",
    "    \"\"\"\n",
    "    ab_path_values = np.array(ab_paths)\n",
    "\n",
    "    # Check if there's only one mediator\n",
    "    if isinstance(mediators, str):\n",
    "        mediators = [mediators]\n",
    "\n",
    "    # Calculate mean indirect effect and confidence intervals for each mediator\n",
    "    mean_ab_paths = np.mean(ab_path_values, axis=0)\n",
    "    lower_bounds = np.percentile(ab_path_values, 2.5, axis=0)\n",
    "    upper_bounds = np.percentile(ab_path_values, 97.5, axis=0)\n",
    "\n",
    "    # Calculate p-values for each mediator\n",
    "    ab_path_p_values = [np.mean(np.sign(mean_ab_paths) * ab_path_values <= 0)]\n",
    "\n",
    "    # Create DataFrame to store the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'Point Estimate': mean_ab_paths,\n",
    "        '2.5th Percentile': lower_bounds,\n",
    "        '97.5th Percentile': upper_bounds,\n",
    "        'P-value': ab_path_p_values\n",
    "    }, index=mediators)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def perform_mediated_moderation_analysis(dataframe, exposure, mediator, moderator, dependent_variable, bootstrap_samples=5000):\n",
    "    \"\"\"\n",
    "    Performs a mediated moderation analysis by estimating the joint indirect effects of an exposure variable\n",
    "    through a mediator on a dependent variable using bootstrapping, considering the moderating effect of another variable.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: DataFrame containing the data.\n",
    "    - exposure: str, column name of the exposure variable.\n",
    "    - mediator: str, column name of the mediator variable.\n",
    "    - moderator: str, column name of the moderator variable.\n",
    "    - dependent_variable: str, column name of the dependent variable.\n",
    "    - bootstrap_samples: int, optional, number of bootstrap samples to be used (default is 5000).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the mean indirect effect, confidence intervals, and p-values for the indirect effect.\n",
    "\n",
    "    Example Usage:\n",
    "    result_df = perform_mediated_moderation_analysis(data_df, exposure='Age',\n",
    "                                                     mediator='Brain_Lobe',\n",
    "                                                     moderator='Stimulation',\n",
    "                                                     dependent_variable='Outcome')\n",
    "    \"\"\"\n",
    "\n",
    "    ab_paths = []\n",
    "\n",
    "    # Loop over each bootstrap sample\n",
    "    for i in range(bootstrap_samples):\n",
    "        # Resample the data with replacement\n",
    "        sample = dataframe.sample(frac=1, replace=True)\n",
    "\n",
    "        # Fit the models and calculate the indirect effect for this bootstrap sample\n",
    "        model_M = ols(f\"{mediator} ~ {moderator}\", data=sample).fit()\n",
    "        model_Y = ols(f\"{dependent_variable} ~ {exposure} + {mediator} + {moderator} + {exposure}:{mediator} + {exposure}:{moderator}\", data=sample).fit()\n",
    "\n",
    "        indirect_effect = model_M.params[moderator] * model_Y.params[f'{exposure}:{mediator}']\n",
    "\n",
    "        # Append the indirect effect to the list\n",
    "        ab_paths.append(indirect_effect)\n",
    "    # Calculate confidence intervals and p-values\n",
    "    result_df = calculate_confidence_intervals(ab_paths, mediators=mediator)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def voxelwise_mediated_moderation_analysis(mediator_df, moderator_df, exposure_df, outcome_df, bootstrap_samples=5000):\n",
    "    \"\"\"\n",
    "    Perform voxelwise mediated moderation analysis between the corresponding voxels from\n",
    "    neuroimaging dataframes and clinical dataframes on a patient's outcome.\n",
    "    \n",
    "    Parameters:\n",
    "        mediator_df (pd.DataFrame): DataFrame containing the mediator variable with patients in rows and 'subject_id' as index.\n",
    "        moderator_df (pd.DataFrame): DataFrame containing the moderator variable with patients in rows and 'subject_id' as index.\n",
    "        exposure_df (pd.DataFrame): DataFrame containing the exposure variable with patients in rows and 'subject_id' as index.\n",
    "        outcome_df (pd.DataFrame): DataFrame containing the outcome variable in 'outcome' column with patients in rows and 'subject_id' as index.\n",
    "        bootstrap_samples (int): Number of bootstrap samples to use in mediated moderation analysis.\n",
    "    \n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): DataFrame containing mediated moderation analysis results for each voxel.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of voxels\n",
    "    n_voxels = exposure_df.shape[1]\n",
    "\n",
    "    # Initialize a list to store the results for each voxel\n",
    "    results = []\n",
    "\n",
    "    # Loop through each voxel and perform mediated moderation analysis\n",
    "    for i in tqdm(range(n_voxels)):\n",
    "        # Create temporary dataframe with outcome, mediator, moderator, and exposure data for the current voxel\n",
    "        temp_df = pd.DataFrame({\n",
    "            'exposure': exposure_df.iloc[:, i] if exposure_df.shape[1] > 1 else exposure_df.iloc[:, 0],\n",
    "            'mediator': mediator_df.iloc[:, i] if mediator_df.shape[1] > 1 else mediator_df.iloc[:, 0],\n",
    "            'moderator': moderator_df.iloc[:, i] if moderator_df.shape[1] > 1 else moderator_df.iloc[:, 0],\n",
    "            'outcome': outcome_df['outcome']\n",
    "        })\n",
    "\n",
    "        # Perform mediated moderation analysis on the temporary dataframe\n",
    "        result_df = perform_mediated_moderation_analysis(dataframe = temp_df,\n",
    "                                                         exposure = 'exposure', \n",
    "                                                         mediator = 'mediator', \n",
    "                                                         moderator = 'moderator', \n",
    "                                                         dependent_variable ='outcome', \n",
    "                                                         bootstrap_samples=bootstrap_samples)\n",
    "        # Append voxel index to result_df\n",
    "        result_df['voxel_index'] = i\n",
    "        results.append(result_df)\n",
    "\n",
    "    # Concatenate all result dataframes into one\n",
    "    results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = voxelwise_mediated_moderation_analysis(mediator_df=neuroimaging_df_2,\n",
    "                                                    moderator_df=clinical_df_1, \n",
    "                                                    exposure_df=neuroimaging_df_1, \n",
    "                                                    outcome_df=outcomes_df, \n",
    "                                                    bootstrap_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Multiprocssed Voxelwise Mediated Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "def calculate_confidence_intervals(ab_paths, mediators):\n",
    "    \"\"\"\n",
    "    Calculates the confidence intervals and p-value based on the bootstrapped samples.\n",
    "\n",
    "    Parameters:\n",
    "    - ab_paths: list of lists containing the bootstrapped ab paths for each mediator.\n",
    "    - total_indirect_effects: list of bootstrapped summed ab paths.\n",
    "    - mediators: list of mediator names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the mean indirect effect, confidence intervals, and p-values for each mediator and the total indirect effect.\n",
    "    \"\"\"\n",
    "    ab_path_values = np.array(ab_paths)\n",
    "\n",
    "    # Check if there's only one mediator\n",
    "    if isinstance(mediators, str):\n",
    "        mediators = [mediators]\n",
    "\n",
    "    # Calculate mean indirect effect and confidence intervals for each mediator\n",
    "    mean_ab_paths = np.mean(ab_path_values, axis=0)\n",
    "    lower_bounds = np.percentile(ab_path_values, 2.5, axis=0)\n",
    "    upper_bounds = np.percentile(ab_path_values, 97.5, axis=0)\n",
    "\n",
    "    # Calculate p-values for each mediator\n",
    "    ab_path_p_values = [np.mean(np.sign(mean_ab_paths) * ab_path_values <= 0)]\n",
    "\n",
    "    # Create DataFrame to store the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'Point Estimate': mean_ab_paths,\n",
    "        '2.5th Percentile': lower_bounds,\n",
    "        '97.5th Percentile': upper_bounds,\n",
    "        'P-value': ab_path_p_values\n",
    "    }, index=mediators)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def perform_mediated_moderation_analysis(dataframe, exposure, mediator, moderator, dependent_variable, bootstrap_samples=5000):\n",
    "    \"\"\"\n",
    "    Performs a mediated moderation analysis by estimating the joint indirect effects of an exposure variable\n",
    "    through a mediator on a dependent variable using bootstrapping, considering the moderating effect of another variable.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: DataFrame containing the data.\n",
    "    - exposure: str, column name of the exposure variable.\n",
    "    - mediator: str, column name of the mediator variable.\n",
    "    - moderator: str, column name of the moderator variable.\n",
    "    - dependent_variable: str, column name of the dependent variable.\n",
    "    - bootstrap_samples: int, optional, number of bootstrap samples to be used (default is 5000).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the mean indirect effect, confidence intervals, and p-values for the indirect effect.\n",
    "\n",
    "    Example Usage:\n",
    "    result_df = perform_mediated_moderation_analysis(data_df, exposure='Age',\n",
    "                                                     mediator='Brain_Lobe',\n",
    "                                                     moderator='Stimulation',\n",
    "                                                     dependent_variable='Outcome')\n",
    "    \"\"\"\n",
    "\n",
    "    ab_paths = []\n",
    "\n",
    "    # Loop over each bootstrap sample\n",
    "    for i in range(bootstrap_samples):\n",
    "        # Resample the data with replacement\n",
    "        sample = dataframe.sample(frac=1, replace=True)\n",
    "\n",
    "        # Fit the models and calculate the indirect effect for this bootstrap sample\n",
    "        model_M = ols(f\"{mediator} ~ {moderator}\", data=sample).fit()\n",
    "        model_Y = ols(f\"{dependent_variable} ~ {exposure} + {mediator} + {moderator} + {exposure}:{mediator} + {exposure}:{moderator}\", data=sample).fit()\n",
    "\n",
    "        indirect_effect = model_M.params[moderator] * model_Y.params[f'{exposure}:{mediator}']\n",
    "        ab_paths.append(indirect_effect)\n",
    "    # Calculate confidence intervals and p-values\n",
    "    result_df = calculate_confidence_intervals(ab_paths, mediators=mediator)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def process_voxel(i, exposure_df, mediator_df, moderator_df, outcome_df, bootstrap_samples):\n",
    "    # This function will be applied to each voxel\n",
    "\n",
    "    # Compose the voxelwise data dictionary\n",
    "    temp_dict = {\n",
    "        'exposure': exposure_df.iloc[:, i] if exposure_df.shape[1] > 1 else exposure_df.iloc[:, 0],\n",
    "        'mediator': mediator_df.iloc[:, i] if mediator_df.shape[1] > 1 else mediator_df.iloc[:, 0],\n",
    "        'moderator': moderator_df.iloc[:, i] if moderator_df.shape[1] > 1 else moderator_df.iloc[:, 0],\n",
    "        'outcome': outcome_df['outcome']\n",
    "    }\n",
    "    \n",
    "    # Perform the mediated moderation analysis for the voxel\n",
    "    voxel_result = perform_mediated_moderation_analysis(dataframe = pd.DataFrame(temp_dict),\n",
    "                                                         exposure = 'exposure', \n",
    "                                                         mediator = 'mediator', \n",
    "                                                         moderator = 'moderator', \n",
    "                                                         dependent_variable ='outcome', \n",
    "                                                         bootstrap_samples=10)\n",
    "\n",
    "    return voxel_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def voxelwise_mediated_moderation_analysis(exposure_df, mediator_df, moderator_df, outcome_df, bootstrap_samples):\n",
    "    n_voxels = exposure_df.shape[1]\n",
    "\n",
    "    # Define the iterable inputs\n",
    "    iterable = range(n_voxels)\n",
    "\n",
    "    # Initialize a parallel joblib instance with verbose=10 to get progress updates\n",
    "    results = Parallel(n_jobs=-1, verbose=10)(\n",
    "        delayed(process_voxel)(i, exposure_df, mediator_df, moderator_df, outcome_df, bootstrap_samples) for i in iterable\n",
    "    )\n",
    "\n",
    "    # Combine the results and return\n",
    "    combined_df = pd.concat([result.reset_index(drop=True) for result in results], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# Run the voxel-wise analysis\n",
    "results_df = voxelwise_mediated_moderation_analysis(mediator_df=neuroimaging_df_2.iloc[:,1:10],\n",
    "                                                    moderator_df=clinical_df_1, \n",
    "                                                    exposure_df=neuroimaging_df_1.iloc[:,1:10], \n",
    "                                                    outcome_df=outcomes_df, \n",
    "                                                    bootstrap_samples=2)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxelwise T-Statistic of Interaction Effect\n",
    "\n",
    "    Parameters:\n",
    "        outcome_df (pd.DataFrame): DataFrame containing the outcome variable in 'outcome' column with patients in rows and 'subject_id' as index.\n",
    "        predictor_neuroimaging_dfs (list of pd.DataFrame): List of DataFrames containing voxelwise neuroimaging data with patients in rows and 'subject_id' as index.\n",
    "        predictor_clinical_dfs (list of pd.DataFrame): List of DataFrames containing clinical data with patients in rows and 'subject_id' as index.\n",
    "        model_type (str): Specifies the type of regression model to use ('linear' or 'logistic').\n",
    "    \n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): DataFrame containing t-statistics and p-values for each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.statistical_utils.voxelwise_statistical_testing import voxelwise_interaction_t_stat\n",
    "stat_df, all_df, _ = voxelwise_interaction_t_stat(outcome_df=outcomes_df,\n",
    "                                       predictor_neuroimaging_dfs=[neuroimaging_df_1], \n",
    "                                       predictor_clinical_dfs=[clinical_df_1],\n",
    "                                       model_type='linear', \n",
    "                                       manual_t_stat=False, \n",
    "                                       permutation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.nifti_utils.matrix_utilities import unmask_matrix\n",
    "def extract_and_save_results(df, mask_path=None):\n",
    "    # Iterate over column, avoiding the one with subject id in it\n",
    "    for colname in df.columns:\n",
    "        print(f'Working on {colname}')\n",
    "        try:\n",
    "            print('masking')\n",
    "            series = unmask_matrix(df[colname], mask_path=mask_path)\n",
    "            print('saving')\n",
    "            viewer = view_and_save_nifti(series, os.path.join(out_dir, 'regression_map', colname))\n",
    "        except Exception as e:\n",
    "            print(f'Error {e}')\n",
    "    return viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_and_save_results(all_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxelwise F-statistic Comparison of Model with Interaction Versus Without Interactions\n",
    "- this is for interaction of an arbritary number of neuroimaging datasets and clinical data.\n",
    "- Note - the F-statistic is not a pivotal statistic under heteroscedastic conditions, thus it is not ideal for permutation analysis unless the groups sizes are equivalent. \n",
    "    -  Given this analysis performs comparison of reduced versus full models between a single group, the f-statistic is acceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from statsmodels.stats.api import anova_lm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import chi2, t, f\n",
    "\n",
    "\n",
    "def calculate_g_statistic(full_model, reduced_model):\n",
    "    #### WORK IN PROGRESS #####\n",
    "    \"\"\"\n",
    "    Calculates the G-statistic for a given model.\n",
    "    Please note, a G-statistic is comparable to various other statistics under various conditions, \n",
    "    but it is not meant to derive p-values analytically. It is meant to derive p-values using permutation testing. \n",
    "\n",
    "    Parameters:\n",
    "        full_model: A fitted model object from the full model.\n",
    "        reduced_model: A fitted model object from the reduced model.\n",
    "        heteroscedastic: Boolean. If True, the errors are assumed to be heteroscedastic, \n",
    "                         and a chi-square distribution is used to calculate the p-value. \n",
    "                         If False, the errors are assumed to be homoscedastic and a Student's \n",
    "                         t-distribution is used to calculate the p-value.\n",
    "\n",
    "    Returns:\n",
    "        G_statistic: The calculated G-statistic.\n",
    "        p_value: The p-value associated with the G-statistic.\n",
    "    \"\"\"\n",
    "    # ψ^: The estimated parameters from the full model, minus those from the reduced model\n",
    "    psi_hat = full_model.params - reduced_model.params\n",
    "    # C: The contrast matrix. This depends on your specific hypotheses and model.\n",
    "    # Assuming that full_model and reduced_model are statsmodels regression result objects\n",
    "    # Get the parameter names from both models\n",
    "    full_model_params = full_model.params.index\n",
    "    reduced_model_params = reduced_model.params.index\n",
    "\n",
    "    # Initialize the contrast matrix as a zero matrix with the length of the full model params\n",
    "    C = np.zeros(len(full_model_params))\n",
    "\n",
    "    # For each parameter in the full model, if it is not in the reduced model, set the corresponding\n",
    "    # element in the contrast matrix to 1\n",
    "    for i, param in enumerate(full_model_params):\n",
    "        if param not in reduced_model_params:\n",
    "            C[i] = 1\n",
    "\n",
    "    # Ensure that C remains a 2D array (i.e., a matrix), which is expected for matrix operations\n",
    "    C = C.reshape(-1, len(C))\n",
    "\n",
    "    # M: The design matrix from the full model\n",
    "    M = full_model.model.exog\n",
    "\n",
    "    # W: Diagonal weighting matrix.\n",
    "    # Compute the residuals from your full model\n",
    "    residuals = full_model.resid.to_numpy()\n",
    "\n",
    "    # Compute the variance of residuals\n",
    "    variances = np.reshape(np.var(residuals), -1)\n",
    "\n",
    "    # Assume gn contains the variance group assignments for each observation\n",
    "    # Assume R is the residual forming matrix\n",
    "    # Assume epsilon_hat contains the vector of residuals\n",
    "\n",
    "    # Initialize W as a zero matrix with the same shape as R\n",
    "    W = np.zeros_like(residuals)\n",
    "\n",
    "    # Iterate over each observation\n",
    "    for n in range(len(gn)):\n",
    "        # Get the variance group assignment for the n-th observation\n",
    "        variance_group = gn[n]\n",
    "\n",
    "        # Find the indices of observations belonging to the same variance group\n",
    "        group_indices = np.where(gn == variance_group)[0]\n",
    "\n",
    "        # Compute the sum of diagonal elements of R for the variance group\n",
    "        sum_R = np.sum(R[group_indices, group_indices])\n",
    "\n",
    "        # Compute the product of epsilon_hat for the variance group\n",
    "        product_epsilon_hat = np.prod(epsilon_hat[group_indices])\n",
    "\n",
    "        # Compute the diagonal element of W for the n-th observation\n",
    "        W_nn = sum_R / product_epsilon_hat\n",
    "\n",
    "        # Set the diagonal element of W for the n-th observation\n",
    "        W[n, n] = W_nn\n",
    "\n",
    "\n",
    "    # Calculate Λ and the inverse of it.\n",
    "    Lambda_inv = np.linalg.inv(C @ M.T @ W @ M @ C.T)\n",
    "\n",
    "    # Calculate the G-statistic\n",
    "    G_statistic = psi_hat.T @ C.T @ Lambda_inv @ C @ psi_hat\n",
    "\n",
    "    # Degrees of freedom is the rank of C\n",
    "    df = np.linalg.matrix_rank(C)\n",
    "\n",
    "    # Compute 1-tailed p-value to assess if full model is significant better than reduced model\n",
    "    # Assess heteroscedasticity with the Breusch-Pagan test, p-value <0.05 indicates the linear model is heteroscedastic\n",
    "    _, p_value, _, _ = het_breuschpagan(residuals, full_model.model.exog)\n",
    "    print('6')\n",
    "    if p_value < 0.05:\n",
    "        if df == 1:\n",
    "            p_value = np.NaN\n",
    "            #This is equivalent to Welch's v^2, which does not have an analytical distribution\n",
    "        else:\n",
    "            p_value = np.NaN\n",
    "            #This is equivalent to Aspen-Welch v, which does not have an analytical distribution\n",
    "        print('7')\n",
    "    else:\n",
    "        print('8')\n",
    "        if df == 1:\n",
    "            print('9')\n",
    "            p_value = 2 * (1 - t.cdf(np.sqrt(G_statistic), df))\n",
    "            #This is equivalent to student's T, which does have an analytical distribution\n",
    "        else:\n",
    "            print('10')\n",
    "            p_value = 1 - f.cdf(G_statistic, df, full_model.df_resid - df)\n",
    "            #This is equivalent to F-ratio, which does have an analytical distribution\n",
    "\n",
    "    return G_statistic, p_value\n",
    "\n",
    "\n",
    "\n",
    "def handle_nan_p_values(p_value_series):\n",
    "    \"\"\"\n",
    "    Function to handle NANs in p-values by backward filling.\n",
    "    \n",
    "    Parameters:\n",
    "        p_value_series (pd.Series): Series containing the p-values.\n",
    "    \n",
    "    Returns:\n",
    "        p_value_series (pd.Series): Series with NANs handled.\n",
    "    \"\"\"\n",
    "    \n",
    "    if p_value_series.isna().any():\n",
    "        p_value_series.fillna(method='bfill', inplace=True)\n",
    "        print('WARNING: p-values containing NAN values')\n",
    "        if p_value_series.isna().any():\n",
    "            p_value_series.fillna(1.0, inplace=True)\n",
    "    \n",
    "    return p_value_series\n",
    "\n",
    "\n",
    "def fdr_correct_p_values_and_threshold_r_squared(results_df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs FDR correction on the p-values in the results DataFrame and thresholds R-squared values based on the corrected p-values.\n",
    "    \n",
    "    Parameters:\n",
    "        results_df (pd.DataFrame): DataFrame containing the p-values and R-squared values.\n",
    "        alpha (float): Significance level for FDR correction.\n",
    "        \n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): DataFrame with FDR corrected p-values and thresholded R-squared values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Correct for multiple comparisons using FDR\n",
    "    p_values = results_df['one_way_p_value'].values\n",
    "    _, pvals_fdr, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
    "    \n",
    "    # Add FDR corrected p-values to the DataFrame\n",
    "    results_df['fdr_corrected_p_value'] = pvals_fdr\n",
    "    \n",
    "    # Threshold R-squared values based on FDR corrected p-values\n",
    "    try:\n",
    "        results_df['adj_r_squared_thresholded'] = results_df['adj_r_squared']\n",
    "        results_df.loc[results_df['fdr_corrected_p_value'] > alpha, 'adj_r_squared_thresholded'] = 0.0\n",
    "    except:\n",
    "        print(' results_df[\"adj_r_squared_thresholded\"]  not found')\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def voxelwise_interaction_f_stat(outcome_df, predictor_neuroimaging_dfs, predictor_clinical_dfs, model_type='linear', manual_f_stat=False, manual_g_stat=True):\n",
    "    \"\"\"\n",
    "    Perform voxelwise regression with interactions between the corresponding voxels from\n",
    "    neuroimaging dataframes and clinical dataframes on a patient's outcome and use F-test to compare models with and without interactions.\n",
    "    The F-test is the proportion of mean squared errors. When comparing two different models, F-statistic is the proprtion of the change in\n",
    "    the mean squared error between the two models compared to the full model. \n",
    "    However, this means that if the second model is much worse than the first model (Larger MSE), the F-statistic can be negative. Thus, \n",
    "    negative F-statistic is considered to be equivalent to and F-statistic of zero. \n",
    "    \n",
    "    degrees_of_freedom is equivalent to sample size. \n",
    "    F-statistic = ((sum_squared_residuals_1 - sum_squared_residuals_2)/(df_residuals_1 - df_residuals_2)) / (sum_squared_residuals_2/(df_residuals_2)\n",
    "    MSE = sum_squared_residuals/degrees_of_freedom \n",
    "    F-statistic = delta_MSE/full_model_MSE\n",
    "    \n",
    "    Parameters:\n",
    "        outcome_df (pd.DataFrame): DataFrame containing the outcome variable in 'outcome' column with patients in rows and 'subject_id' as index.\n",
    "        predictor_neuroimaging_dfs (list of pd.DataFrame): List of DataFrames containing voxelwise neuroimaging data with patients in rows and 'subject_id' as index.\n",
    "        predictor_clinical_dfs (list of pd.DataFrame): List of DataFrames containing clinical data with patients in rows and 'subject_id' as index.\n",
    "        model_type (str): Specifies the type of regression model to use ('linear' or 'logistic').\n",
    "        manual_f_stat (bool): If True, use the manual calculation for F-statistic and p-value. Otherwise, use anova_lm function.\n",
    "    \n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): DataFrame containing F-statistics and p-values for each voxel.\n",
    "        \n",
    "    Cite: \n",
    "    chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://sites.duke.edu/bossbackup/files/2013/02/FTestTutorial.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of voxels in the first neuroimaging dataframe\n",
    "    n_voxels = predictor_neuroimaging_dfs[0].shape[1]\n",
    "\n",
    "    # Initialize a list to store the results for each voxel\n",
    "    results = []\n",
    "\n",
    "    # Define lambda functions for standardization\n",
    "    standardize_within_patient = lambda df: df.apply(lambda x: (x - x.mean()) / x.std(), axis=1)\n",
    "    standardize_across_patients = lambda df: df.apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "    \n",
    "    # Standardize outcome data across patients\n",
    "    outcome_df = standardize_across_patients(outcome_df)\n",
    "    if predictor_neuroimaging_dfs[0] is not None:\n",
    "        # Standardize neuroimaging data within each patient\n",
    "        predictor_neuroimaging_dfs = [standardize_within_patient(df) for df in predictor_neuroimaging_dfs]\n",
    "    if predictor_clinical_dfs[0] is not None:\n",
    "        # Standardize clinical data across patients\n",
    "        predictor_clinical_dfs = [standardize_across_patients(df) for df in predictor_clinical_dfs]\n",
    "\n",
    "    # Loop through each voxel and perform regression\n",
    "    for i in tqdm(range(0, n_voxels)):\n",
    "        # Create temporary dataframe with outcome and corresponding voxel from all neuroimaging dataframes and clinical data\n",
    "        temp_df = outcome_df[['outcome']].copy()\n",
    "        variable_names = []\n",
    "        \n",
    "        if predictor_neuroimaging_dfs[0] is not None:\n",
    "            for j, neuroimaging_df in enumerate(predictor_neuroimaging_dfs):\n",
    "                temp_df[f'dataframe_{j}_voxel_i'] = neuroimaging_df.iloc[:, i]\n",
    "                variable_names.append(f'dataframe_{j}_voxel_i')\n",
    "        \n",
    "        if predictor_clinical_dfs[0] is not None:\n",
    "            for j, clinical_df in enumerate(predictor_clinical_dfs):\n",
    "                temp_df = temp_df.merge(clinical_df, left_index=True, right_index=True)\n",
    "                variable_names.extend(clinical_df.columns.tolist())\n",
    "            \n",
    "        # Construct the regression formulas dynamically\n",
    "        variables_combined = \" + \".join(variable_names)\n",
    "        interaction_terms = \" + \".join([f\"{var1}:{var2}\" for idx, var1 in enumerate(variable_names) for var2 in variable_names[idx+1:]])\n",
    "        \n",
    "        formula_no_interaction = f'outcome ~ {variables_combined}'\n",
    "        formula_interaction = f'outcome ~ {variables_combined} + {interaction_terms}'\n",
    "                \n",
    "        # Fit the models\n",
    "        if model_type == 'linear':\n",
    "            model_no_interaction = smf.ols(formula_no_interaction, data=temp_df).fit()\n",
    "            model_interaction = smf.ols(formula_interaction, data=temp_df).fit()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "\n",
    "        if manual_f_stat:\n",
    "            # Calculate the F-statistic manually\n",
    "            sum_square_1 = model_no_interaction.ssr\n",
    "            sum_square_2 = model_interaction.ssr\n",
    "            degrees_freedom_1 = model_no_interaction.df_resid\n",
    "            degrees_freedom_2 = model_interaction.df_resid\n",
    "\n",
    "            F_statistic = ((sum_square_1 - sum_square_2) / (degrees_freedom_1 - degrees_freedom_2)) / (sum_square_2 / degrees_freedom_2)\n",
    "            if F_statistic < 0:\n",
    "                F_statistic = 0 \n",
    "            # Calculate the p-value using the cumulative distribution function of the F-distribution\n",
    "            if np.isinf(F_statistic):\n",
    "                print(\"Warning: Infinite F-statistic detected. Setting p-value to zero.\")\n",
    "                P_value = 0\n",
    "            else:\n",
    "                P_value = 1 - f.cdf(F_statistic, degrees_freedom_1 - degrees_freedom_2, degrees_freedom_2)\n",
    "            statistic = 'manual_f_statistic'\n",
    "        elif manual_g_stat:\n",
    "            # This assesses heteroscedasticity at every voxel and decides what distribution the G-statistic should use\n",
    "            calculate_g_statistic(model_interaction, model_no_interaction)\n",
    "            statistic = 'manual_g_statistic'\n",
    "        else:\n",
    "            # Calculate the F-statistic and p-value using anova_lm function\n",
    "            table = anova_lm(model_no_interaction, model_interaction)\n",
    "            F_statistic = table['F'][1]\n",
    "            if np.isinf(F_statistic):\n",
    "                print(\"Warning: Infinite F-statistic detected. Setting p-value to zero.\")\n",
    "                P_value = 0.0\n",
    "            else:\n",
    "                P_value = table['Pr(>F)'][1]\n",
    "            statistic = 'statsmodels_f_statistic'\n",
    "        # Store the results for the current voxel\n",
    "        # voxel_results = {\n",
    "        #     'statistic_value': F_statistic,\n",
    "        # }\n",
    "        voxel_results = {\n",
    "            'voxel_index': i,\n",
    "            'statistic_value': F_statistic,\n",
    "            'statistic_type': statistic,\n",
    "            'one_way_p_value': P_value,\n",
    "            'unc_r_squared': model_interaction.rsquared,\n",
    "            'adj_r_squared': model_interaction.rsquared_adj\n",
    "        }\n",
    "        # Append the voxel_results dictionary to the results list\n",
    "        results.append(voxel_results)\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print('\\n Example formula without interaction: ', formula_no_interaction)\n",
    "    print('Example formula with interaction: ', formula_interaction)\n",
    "    return results_df, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = outcomes_df\n",
    "neuroimaging_dfs = [neuroimaging_df_1]  # List of neuroimaging DataFrames\n",
    "clinical_dfs = [clinical_df_1]  # List of clinical DataFrames\n",
    "# Note that this will only perform up to 2 way interactions. \n",
    "#----------------------------------------------------------------DO NOT MODIFY!----------------------------------------------------------------\n",
    "# Function calls\n",
    "# Perform voxelwise regression and F-test\n",
    "results_df, temp_df = voxelwise_interaction_f_stat(outcome_df, \n",
    "                                                   neuroimaging_dfs, \n",
    "                                                   clinical_dfs,\n",
    "                                                   manual_f_stat=False, \n",
    "                                                   manual_g_stat=False)\n",
    "\n",
    "# Handle NaN in p-values\n",
    "results_df['one_way_p_value'] = handle_nan_p_values(results_df['one_way_p_value'])\n",
    "\n",
    "# FDR correction and thresholding R-squared values\n",
    "results_df = fdr_correct_p_values_and_threshold_r_squared(\n",
    "    results_df=results_df,\n",
    ")\n",
    "\n",
    "# Output the results DataFrame\n",
    "results_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['statistic_value'].to_csv(os.path.join(out_dir, 'observed_f_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max statistic: ', results_df['statistic_value'].max())\n",
    "print('Min uncorrected p-value: ', results_df['one_way_p_value'].min())\n",
    "print('Min corrected p-value: ', results_df['fdr_corrected_p_value'].min())\n",
    "print('Max uncorrected r-squared: ', results_df['unc_r_squared'].max())\n",
    "print('Max adjusted r-squared: ', results_df['adj_r_squared'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Resulting Data as niftis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_path = None #'/Users/cu135/Dropbox (Partners HealthCare)/resources/mni_spaces/6th_gen/mni_152_gm_mask_resampled.nii'\n",
    "# masking_df = None\n",
    "#----------------------------------------------------------------DO NOT TOUCH\n",
    "#Prepare Indices to Unmask\n",
    "from calvin_utils.matrix_utilities import unmask_matrix\n",
    "from nimlab import datasets as nimds\n",
    "from calvin_utils.generate_nifti import view_and_save_nifti\n",
    "from nilearn import image\n",
    "\n",
    "# unmask_matrix\n",
    "# from calvin_utils.matrix_utilities import unmask_matrix\n",
    "# results_df = unmask_matrix(masking_df, \n",
    "#                            mask_path=None, \n",
    "#                            mask_threshold=0.2, \n",
    "#                            unmask_by='columns', \n",
    "#                            dataframe_to_unmask_by=masking_df)\n",
    "# results_df\n",
    "\n",
    "#Perform Unmasking\n",
    "if mask_path is not None:\n",
    "    mni_mask = image.load_img(mask_path).get_fdata().flatten()\n",
    "    brain_indices = np.where(mni_mask > 0.2)[0]\n",
    "elif masking_df is not None:\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\").get_fdata().flatten()\n",
    "    mask = masking_df.transpose().reset_index(drop=True).copy()\n",
    "    mask['mask_index'] = mask.sum(axis=1)\n",
    "    brain_indices = np.where(mask['mask_index'] != 0)[0]\n",
    "else:\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\").get_fdata().flatten()\n",
    "    brain_indices = np.where(mni_mask > 0)[0]\n",
    "\n",
    "# Create a boolean mask for brain_indices\n",
    "bool_mask = np.zeros_like(mni_mask, dtype=bool)\n",
    "bool_mask[brain_indices] = True\n",
    "\n",
    "for statistic in results_df.columns:\n",
    "    try:\n",
    "        print(statistic)\n",
    "        \n",
    "        # Initialize the output mask with NaN values\n",
    "        output_mask = np.full_like(mni_mask, np.nan)\n",
    "        \n",
    "        # Reinstate the values at brain_indices\n",
    "        output_mask[bool_mask] = results_df[statistic]\n",
    "        \n",
    "        # View and save\n",
    "        view_and_save_nifti(output_mask, out_dir=out_dir, output_name=statistic)\n",
    "    except:\n",
    "        print(f\"Couldn't convert {statistic} to nifti\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxelwise Regression Using 1 Neuroimaging File and 1 Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "def voxelwise_regression(data_df, outcome_col, clinical_vars, voxel_start_index, model_type='linear', interaction_effects=None):\n",
    "    # Replace column names with their index\n",
    "    data_df.columns = list(range(len(data_df.columns)))\n",
    "    \n",
    "    n_voxels = len(data_df.columns[voxel_start_index:])\n",
    "    print(n_voxels)\n",
    "    \n",
    "    p_values = [[] for _ in range(n_voxels)]\n",
    "    coeff_values = [[] for _ in range(n_voxels)]\n",
    "    t_values = [[] for _ in range(n_voxels)]\n",
    "\n",
    "    indep_vars_str = \" + \".join(f\"var{x}\" for x in clinical_vars)\n",
    "\n",
    "    if interaction_effects:\n",
    "        interaction_terms_str = \" + \".join([f\"var{data_df.columns[a]}_interact_connectivity\" for a in interaction_effects])\n",
    "        indep_vars_str = f\"{indep_vars_str} + connectivity + {interaction_terms_str}\"\n",
    "\n",
    "    if model_type == 'linear':\n",
    "        formula = f'outcome ~ {indep_vars_str}'\n",
    "        print(\"Regression formula (linear):\", formula)\n",
    "    elif model_type == 'logistic':\n",
    "        formula = f'outcome ~ {indep_vars_str}'\n",
    "        print(\"Regression formula (logistic):\", formula)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "\n",
    "    for i in tqdm(range(0, n_voxels)):\n",
    "        temp_df = data_df.iloc[:, [outcome_col] + clinical_vars + [voxel_start_index + i]]\n",
    "        temp_df.columns = ['outcome'] + [f\"var{x}\" for x in clinical_vars] + ['connectivity']\n",
    "\n",
    "        # Add interaction columns to the temp_df\n",
    "        if interaction_effects:\n",
    "            for idx in interaction_effects:\n",
    "                interaction_col_name = f'var{data_df.columns[idx]}_interact_connectivity'\n",
    "                temp_df[interaction_col_name] = temp_df[f\"var{data_df.columns[idx]}\"] * temp_df['connectivity']\n",
    "\n",
    "        if model_type == 'linear':\n",
    "            model = smf.ols(formula, data=temp_df).fit()\n",
    "        elif model_type == 'logistic':\n",
    "            temp_df['outcome'] = (temp_df['outcome'] > 0.5).astype(int)\n",
    "            model = smf.logit(formula, data=temp_df).fit(disp=0)\n",
    "        elif model_type == 'ridge':\n",
    "            ridge = Ridge(alpha=1.0)\n",
    "            X = sm.add_constant(temp_df.drop(columns='outcome'))\n",
    "            y = temp_df['outcome']\n",
    "            ridge.fit(X, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "\n",
    "        if model_type =='ridge':\n",
    "            # Perform p and t value extraction for the ridge regressions\n",
    "            n = len(temp_df)\n",
    "            k = len(temp_df.columns)\n",
    "            dof = n - k\n",
    "            y_pred = ridge.predict(X)\n",
    "            residuals = y - y_pred\n",
    "            mse = np.sum(residuals**2) / dof\n",
    "            se = np.sqrt(np.diagonal(mse * np.linalg.inv(np.dot(X.T, X))))\n",
    "            t_values_ridge = ridge.coef_ / se\n",
    "            p_values_ridge = [2 * (1 - stats.t.cdf(abs(t_value), dof)) for t_value in t_values_ridge]\n",
    "\n",
    "            for j, (p_value, coef, t_value) in enumerate(zip(p_values_ridge, ridge.coef_, t_values_ridge)):\n",
    "                p_values[i].append(p_value)\n",
    "                coeff_values[i].append(coef)\n",
    "                t_values[i].append(t_value)\n",
    "        else:\n",
    "            for j, (p_value, coef, t_value) in enumerate(zip(model.pvalues, model.params, model.tvalues)):\n",
    "                p_values[i].append(p_value)\n",
    "                coeff_values[i].append(coef)\n",
    "                t_values[i].append(t_value)\n",
    "\n",
    "    columns = ['Intercept'] + [f\"var{x}\" for x in clinical_vars] + ['connectivity'] + [f'var{var}_interact_connectivity' for var in interaction_effects]\n",
    "    p_values_df = pd.DataFrame(p_values, columns=columns)\n",
    "    coeff_values_df = pd.DataFrame(coeff_values, columns=columns)\n",
    "    t_values_df = pd.DataFrame(t_values, columns=columns)\n",
    "\n",
    "    return p_values_df, coeff_values_df, t_values_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check input for NANs and Constant values\n",
    "regression_df.isnull().values.any()\n",
    "test_df = regression_df.loc[:, regression_df.apply(pd.Series.nunique) != 1]\n",
    "if test_df.shape != regression_df.shape:\n",
    "    print('Note, there are constant columns in the regression dataframe. This may cause unstable regression results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrix input should be oriented such that voxels and other clinical information are in columns, with patients in rows\n",
    "p_values_df, coeff_values_df, t_values_df, model = voxelwise_regression(data_df=regression_df, outcome_col=0, clinical_vars=[1], voxel_start_index=2, model_type='linear', interaction_effects=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of p_values_df:\", p_values_df.shape)\n",
    "print(\"Shape of coeff_values_df:\", coeff_values_df.shape)\n",
    "print(\"Shape of t_values_df:\", t_values_df.shape)\n",
    "print(\"Shape of regression_df:\", regression_df.shape)\n",
    "print(model.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nilearn.glm import fdr_threshold\n",
    "from calvin_utils.z_score_matrix import z_score_matrix\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.stats import multitest\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "\n",
    "def insert_outcomes_to_matrix(p_values_df, coeff_values_df, t_values_df, brain_indices, mask_data, view_and_save_nifti, out_dir, adjustment_method='nilearn', strict=False):\n",
    "    '''\n",
    "    This function takes the outcomes of the voxelwise regression and generates NIfTI images from them.\n",
    "    It also calculates FDR-adjusted p-values and masks the coefficients based on the adjusted p-values.\n",
    "    \n",
    "    Parameters:\n",
    "    p_values_df (pd.DataFrame): DataFrame containing p-values.\n",
    "    coeff_values_df (pd.DataFrame): DataFrame containing coefficients.\n",
    "    t_values_df (pd.DataFrame): DataFrame containing t-values.\n",
    "    brain_indices (array): Indices of the brain voxels.\n",
    "    mask_data (array): Mask data for the brain.\n",
    "    view_and_save_nifti (function): Function to view and save NIfTI images.\n",
    "    out_dir (str): Output directory for saving the NIfTI images.\n",
    "    adjustment_method (str, optional): Method to use for FDR adjustment. Defaults to 'nilearn'.\n",
    "                                      Accepted values are 'nilearn' and 'statsmodels'.\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If adjustment_method is not 'nilearn' or 'statsmodels'.\n",
    "    '''\n",
    "    \n",
    "    # Convert DataFrames to NumPy arrays\n",
    "    p_values = p_values_df.to_numpy()\n",
    "    coeff_values = coeff_values_df.to_numpy()\n",
    "    t_values = t_values_df.to_numpy()\n",
    "\n",
    "    # Iterate over statistical values\n",
    "    for stat_name, stat_values in zip([\"p_values\", \"coeff_values\", \"t_values\"], [p_values, coeff_values, t_values]):\n",
    "        for col_idx, col_name in enumerate(p_values_df.columns):\n",
    "            matrix = mask_data.copy()\n",
    "            matrix[brain_indices] = stat_values[:, col_idx]\n",
    "\n",
    "            # Generate NIfTI image for the current statistical value and term\n",
    "            out_file = f\"{out_dir}/{col_name}/{stat_name}.nii\"\n",
    "            view_and_save_nifti(matrix=matrix, out_dir=out_file)\n",
    "\n",
    "            if stat_name == 't_values':\n",
    "                if adjustment_method == 'nilearn':\n",
    "                    # Calculate FDR-adjusted p-values based on t-values\n",
    "                    z_scored_t_values = zscore(np.nan_to_num(t_values[:, col_idx]))\n",
    "                    z_scored_t_values = np.nan_to_num(z_scored_t_values, nan=0, posinf=0, neginf=0)\n",
    "                    print(np.max(z_scored_t_values), np.min(z_scored_t_values))\n",
    "                    threshold = fdr_threshold(z_scored_t_values, 0.05)\n",
    "                    print('FDR Threshold at : ', threshold)\n",
    "                    \n",
    "                    # Mask the coefficients based on FDR-adjusted z_scored_t_values\n",
    "                    fdr_masked_coeff = np.where(z_scored_t_values < threshold, coeff_values[:, col_idx], 0)\n",
    "                                        \n",
    "                elif adjustment_method == 'statsmodels':\n",
    "                    # Calculate FDR-adjusted p-values using statsmodels\n",
    "                    if strict:\n",
    "                        method = 'poscor'\n",
    "                    else:\n",
    "                        method = 'indep'\n",
    "                    _, fdr_adj_p_values = fdrcorrection(p_values[:, col_idx], method='indep')\n",
    "                    print('unadjusted p-values min and max at : ', np.min(p_values[:, col_idx]), np.max(p_values[:, col_idx]))\n",
    "                \n",
    "                    print('FDR adjusted p-values min and max at : ', np.min(fdr_adj_p_values), np.max(fdr_adj_p_values))\n",
    "                    # Mask the coefficients based on FDR-adjusted p-values\n",
    "                    fdr_masked_coeff = np.where(fdr_adj_p_values < 0.05, coeff_values[:, col_idx], 0)\n",
    "                \n",
    "                else:\n",
    "                    raise ValueError(f\"adjustment_method model_type: {adjustment_method}, nilearn or 'statsmodels only currently supported\")\n",
    "                \n",
    "                # Generate NIfTI image for FDR-masked coefficients\n",
    "                matrix = mask_data.copy()\n",
    "                matrix[brain_indices] = fdr_masked_coeff\n",
    "                out_file = f\"{out_dir}/{col_name}/fdr_masked_coeff_{stat_name}.nii\"\n",
    "                view_and_save_nifti(matrix=matrix, out_dir=out_file)\n",
    "                \n",
    "                # Generate NIfTI image for FDR-madjusted p-values\n",
    "                new_matrix = mask_data.copy()\n",
    "                new_matrix[brain_indices] = fdr_adj_p_values\n",
    "                out_file = f\"{out_dir}/{col_name}/fdr_adjusted_p_values.nii\"\n",
    "                view_and_save_nifti(matrix=new_matrix, out_dir=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming p_values_df, coeff_values_df, and t_values_df are obtained from voxelwise_regression function\n",
    "# Assuming brain_indices and mask_data are predefined\n",
    "# Assuming view_and_save_nifti function is predefined\n",
    "from calvin_utils.generate_nifti import view_and_save_nifti\n",
    "out_dir = out_dir\n",
    "\n",
    "#only operate within the brain mask\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_data = mni_mask.get_fdata().flatten()\n",
    "brain_indices = np.where(mask_data > 0)[0]\n",
    "\n",
    "insert_outcomes_to_matrix(p_values_df, coeff_values_df, t_values_df, brain_indices, mask_data, view_and_save_nifti, out_dir, adjustment_method='statsmodels', strict=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Coefficient by PALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coeff_values_df.iloc[:, -1] = np.where(palm_p_values.iloc[:, -1] < 0.05, coeff_values_df.iloc[:, -1], 0)\n",
    "mask_data[brain_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palm_p_values = pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/memory/final_analyses/interaction_analysis/whole_brain_analysis/permutation_results/10185_summed_coefficient_pp_values (2).csv')\n",
    "# coeff_values_df.iloc[:, -1] = np.where(palm_p_values.iloc[:, -1] < 0.05, coeff_values_df.iloc[:, -1], 0)\n",
    "mask_data[brain_indices] = coeff_values_df.iloc[:, -1]\n",
    "#Generate a nifti for coefficients thresholded to uncorrected p-values\n",
    "view_and_save_nifti(mask_data, (out_dir+'/coefficient_thresholded_to_palm_p_values'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Permutation Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is generally best done with submission to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.multiprocessing import permute_brain\n",
    "#Permute the data, returning a matrix organized such that shape = [voxels, patient, permutation]\n",
    "#Convert dataframe to numpy for time savings\n",
    "#Is actually receiving the float values, not the indices. \n",
    "permuted_array = permute_brain(prepped_matrix.to_numpy(), n_permutations=1)\n",
    "\n",
    "#---Visualize a given permutation\n",
    "# retrive brain mask\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_data = mni_mask.get_fdata().flatten()\n",
    "brain_indices = np.where(mask_data > 0)[0]\n",
    "\n",
    "#Retrieve patient data\n",
    "test_matrix = prepped_matrix.iloc[:,0].to_numpy()\n",
    "\n",
    "#reindex patient data by permutation\n",
    "permutation = permuted_array[:,0,0]\n",
    "test_matrix[brain_indices] = permutation\n",
    "view_and_save_nifti(test_matrix, (out_dir+'/verification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def calculate_empirical_p(observed_t_value, sim_t_values):\n",
    "    \"\"\"\n",
    "    Calculate empirical p-value from a distribution of empiric t-values and an observed t-value.\n",
    "    \"\"\"\n",
    "    # count the number of simulated t-values that are as extreme or more extreme than the critical t-value\n",
    "    extreme_count = np.sum(np.abs(sim_t_values) >= observed_t_value)\n",
    "\n",
    "    # divide the number of extreme values by the total number of simulated values to get the empirical p-value\n",
    "    empirical_p = extreme_count / len(sim_t_values)\n",
    "\n",
    "    return empirical_p\n",
    "\n",
    "def process_voxel(i):\n",
    "    #Do not calculate on zero values as intercept will be applied in the regressoin\n",
    "    if np.sum(prepped_matrix.iloc[i,:]) != 0:\n",
    "        #Assign a temporary dataframe with values that the statsmodels model is expecting\n",
    "        ##the plan is to use patient ID to intersect the voxelwise data to the associated clinical data\n",
    "        temp_df = data_df.copy()\n",
    "        #Names of the columns are as per data_df. The voxel coumn's name is the integer of the voxel\n",
    "        temp_df = temp_df.merge(prepped_matrix.iloc[i,:].transpose(), left_index=True, right_index=True)\n",
    "        #Rename the column of the voxel to 'connectivity' to prepare it for the statsmodels input requirements\n",
    "        temp_df = temp_df.rename(columns={i: \"connectivity\"}, errors=\"raise\")\n",
    "        \n",
    "        #Fit model. Outcomes are always first\n",
    "        results = smf.ols(f'{temp_df.columns[0]} ~ {temp_df.columns[1]}*{temp_df.columns[2]}', data=temp_df).fit()\n",
    "        #Extract the t statistic of relevance from the model at this voxel. Interaction effect is len(indep_vars)+1 (intercept, main1, main2, interaction)\n",
    "        voxel_t_value = results.tvalues[3]\n",
    "        \n",
    "        #Assess the t value against the permutation's distribution \n",
    "        t_value_list = []       \n",
    "        for j in range(permuted_array.shape[2]):\n",
    "            # Assign a temporary dataframe with values that the statsmodels model is expecting\n",
    "            temp_df_permuted = data_df.copy()\n",
    "            #Permute the associated data\n",
    "            temp_df_permuted[temp_df.columns[1]] = np.random.permutation(temp_df[temp_df.columns[1]])\n",
    "            #Retrive the already permuted voxel for all patients at this permutation\n",
    "            temp_df_permuted[temp_df.columns[2]] = permuted_array[i,:,j]\n",
    "            \n",
    "            # Fit model. Outcomes are always first\n",
    "            results_permuted = smf.ols(f'{temp_df_permuted.columns[0]} ~ {temp_df_permuted.columns[1]}*{temp_df_permuted.columns[2]}', data=temp_df_permuted).fit()\n",
    "            # Extract the t statistic of relevance from the model at this voxel. Interaction effect is len(indep_vars)+1 (intercept, main1, main2, interaction)\n",
    "            voxel_t_value_permuted = results_permuted.tvalues[3]\n",
    "            t_value_list.append(voxel_t_value_permuted)\n",
    "            \n",
    "        #Calculate the p-value from the observed t-value copmared to the empiric permuted distribution of t-values\n",
    "        voxelwise_p_value = calculate_empirical_p(voxel_t_value_permuted, t_value_list)\n",
    "    else:\n",
    "        #If voxel is zero-connectivity, assign zero so as to avoid application of intercept\n",
    "        voxelwise_p_value = 0\n",
    "    return voxelwise_p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def calculate_empirical_p(observed_t_value, sim_t_values):\n",
    "    \"\"\"\n",
    "    Calculate empirical p-value from a distribution of empiric t-values and an observed t-value.\n",
    "    \"\"\"\n",
    "    # count the number of simulated t-values that are as extreme or more extreme than the critical t-value\n",
    "    extreme_count = np.sum(np.abs(sim_t_values) >= observed_t_value)\n",
    "\n",
    "    # divide the number of extreme values by the total number of simulated values to get the empirical p-value\n",
    "    empirical_p = extreme_count / len(sim_t_values)\n",
    "\n",
    "    return empirical_p\n",
    "\n",
    "def permutation_testing(matrix_to_test, t_matrix):\n",
    "    '''\n",
    "    The plan will be to create one dataframe which can be used to evaluate. \n",
    "    Permuted age will go in the first column\n",
    "    Connectivity will be transposed and attached to the second column\n",
    "    the 'connectivity' column name will move for every voxel and the t value will be calculated\n",
    "    \n",
    "    args: \n",
    "    matrix_to_test is the isolated prepared dataframe to utilize.\n",
    "    t_matrix is the pre-calculated matrix of t values from the non-permuted brains\n",
    "    \n",
    "    performs:\n",
    "    Will calculate t-value for each voxel. Then, it will assess if the t-value is less than the t-value from the non-permuted brain\n",
    "    If the permuted t-value is less than the t-value from the non-permuted brain, then that voxels is set to 0. \n",
    "    \n",
    "    returns:\n",
    "    simply a binary matrix of 1s and 0s\n",
    "    '''\n",
    "    print('in function')\n",
    "    #Permute outcomes\n",
    "    permuted_outcomes = np.apply_along_axis(np.random.permutation, 0, matrix_to_test[:,0])\n",
    "    \n",
    "    #Permute the age\n",
    "    permuted_age = np.apply_along_axis(np.random.permutation, 0, matrix_to_test[:,1])    \n",
    "    \n",
    "    #Permute the Voxels across and within patients (PALM default)\n",
    "    permuted_across = np.apply_along_axis(np.random.permutation, 0, matrix_to_test[:,2:])\n",
    "    permuted_within = np.apply_along_axis(np.random.permutation, 1, permuted_across)\n",
    "    print('permutations complete')\n",
    "    #Reconstruct a dataframe\n",
    "    permuted_dataframe = pd.DataFrame(np.concatenate((permuted_outcomes.reshape(-1, 1), permuted_age.reshape(-1, 1), permuted_within), axis=1))\n",
    "    print('dataframe completed')\n",
    "    #Perform calculation of t values for each voxel\n",
    "    permuted_t_values = []\n",
    "    for i in tqdm(range(len(t_matrix)), desc='Subprocess Progress'):\n",
    "        print('on loop ', i)\n",
    "        #The dataframe has the structure: outcome, age, voxels\n",
    "        #Fit the model. Voxel columns must be strings. \n",
    "        results_permuted = smf.ols(f'{permuted_dataframe.columns[0]} ~ {permuted_dataframe.columns[1]}*{permuted_dataframe.columns[i+2]}', data=permuted_dataframe).fit()\n",
    "        permuted_t_values.append(results_permuted.tvalues[3])\n",
    "    \n",
    "    #Binarize permuted t-values by comparison to non-permuted t-values\n",
    "    #Any t-values that are higher are not significant, so that are = 1.\n",
    "    return np.where(permuted_t_values>t_matrix, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prepare data for analysis\n",
    "\n",
    "# Retrieve brain mask\n",
    "mni_mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_data = mni_mask.get_fdata().flatten()\n",
    "brain_indices = np.where(mask_data > 0)[0]\n",
    "t_matrix = t_df.to_numpy()[brain_indices, -1]\n",
    "\n",
    "# Isolate only the values of interest from the associated data\n",
    "matrix_to_test = prepped_matrix.iloc[brain_indices, :]\n",
    "\n",
    "if 'response_df' in locals():\n",
    "    t_df = response_df['t_values_topology'].to_numpy()[brain_indices]\n",
    "else:\n",
    "    print('not found in locals, must import')\n",
    "    t_df = pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/memory/functional_networks/response_topology/voxelwise_glm/age_interaction_rios_vtas/t_values_topology/t_values.csv', index_col=None)\n",
    "#index t matrix at last column to make sure no index included \n",
    "t_matrix = t_df.to_numpy()[brain_indices, -1]\n",
    "\n",
    "#Prepare the data for assessment\n",
    "matrix_to_test = data_df.merge(matrix_to_test.transpose(), left_index=True, right_index=True)\n",
    "#Reduce size of data to ease computations\n",
    "matrix_to_test = matrix_to_test.astype(np.float16)\n",
    "\n",
    "n_permutations = 100\n",
    "display(matrix_to_test)\n",
    "# matrix_to_test.to_csv(out_dir+'/t_values_topology/matrix_to_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t_matrix))\n",
    "print(np.shape(matrix_to_test))\n",
    "# for i in range(len(t_matrix)):\n",
    "#     print(matrix_to_test.columns[i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform analysis\n",
    "from tqdm import tqdm\n",
    "import concurrent\n",
    "from calvin_utils.multiprocessing import whole_brain_permutation_test\n",
    "#----------------------------------------------------------------Do not Touch Below\n",
    "# Multiprocess Results\n",
    "p_matrix = np.ones_like(t_matrix)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "    #Begin submitting the masked data to the permutor\n",
    "    results = []\n",
    "    for i in tqdm(range(n_permutations), desc=\"Jobs Launched\"):\n",
    "        #Assign the permuted data to a worker. return the result\n",
    "        result = executor.submit(whole_brain_permutation_test, matrix_to_test, t_matrix)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Limit number of workers at given time to prevent memory pressure issues\n",
    "        # jobs[result] = i #Add job to dict of ongoing jobs\n",
    "        # if len(jobs) > max_jobs-1: #Check number of ongoing jobs\n",
    "        #     completed_jobs, _ = concurrent.futures.wait(jobs.keys(), return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "        #     # Remove the completed job from the dict of ongoing jobs\n",
    "        #     del jobs[completed_jobs.pop()]\n",
    "\n",
    "    progress_bar = tqdm(total=n_permutations, desc=\"Jobs Finalized\")\n",
    "    for i, result in enumerate(concurrent.futures.as_completed(results)):\n",
    "        #Input the permuted data into the array\n",
    "        extracted_p_values = result.result()\n",
    "        p_matrix = p_matrix + extracted_p_values\n",
    "        \n",
    "        #Update visualization\n",
    "        progress_bar.update()\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = pd.DataFrame(p_matrix)\n",
    "p_df.to_csv(out_dir+'/t_values_topology/p_vals_2.csv',index=False, header=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize PALM Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows recomposition of the PALM results from the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.generate_nifti import view_and_save_nifti\n",
    "out_dir = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/voxelwise_glm/stim_by_age/palm_results'\n",
    "#data path\n",
    "p_values_dir = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/voxelwise_glm/stim_by_age/10000_summed_f_statp_values.csv'\n",
    "t_values_dir = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/voxelwise_glm/stim_by_age/observed_f_stats.csv'\n",
    "num_perms = 5\n",
    "\n",
    "#import data\n",
    "if 'response_df' in locals():\n",
    "    t_df = response_df['t_values_topology'].to_numpy()[brain_indices]\n",
    "else:\n",
    "    print('not found in locals, must import')\n",
    "    t_df = pd.read_csv(t_values_dir, index_col=None)\n",
    "p_df = pd.read_csv(p_values_dir, header=0, index_col=None)\n",
    "\n",
    "#Scale the p_values\n",
    "print('---Information on p-Values---')\n",
    "print('Nonzero p_values found: ', np.count_nonzero(p_df[p_df.iloc[:,-1] < 0.05]))\n",
    "print('Lowest p_values: ', np.min(p_df.iloc[:,-1]))\n",
    "display(p_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle NANs or Unclean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df.dropna(inplace=True)\n",
    "p_df.dropna(inplace=True)\n",
    "t_df.pop('Unnamed: 0')\n",
    "t_df.max()\n",
    "p_df.max()\n",
    "# t_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unmask the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.matrix_utilities import unmask_matrix\n",
    "from nilearn import masking, image\n",
    "from nimlab import datasets as nimds\n",
    "from nilearn import image\n",
    "\n",
    "mask = nimds.get_img(\"mni_icbm152\")\n",
    "mask_data = mask.get_fdata().flatten()\n",
    "brain_indices = np.where(mask_data > 0)[0]\n",
    "\n",
    "#Insert \n",
    "p_unmasked = np.copy(mask_data)\n",
    "p_unmasked[brain_indices] = p_df.values.flatten()\n",
    "print(np.max(p_unmasked))\n",
    "\n",
    "other_unmasked = np.copy(mask_data)\n",
    "other_unmasked[brain_indices] = t_df.values.flatten()\n",
    "print(np.max(other_unmasked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask statistic of interest by p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold by p_values\n",
    "other_unmasked = np.where(p_unmasked < 0.05, other_unmasked, 0)\n",
    "print(np.max(p_unmasked))\n",
    "print(np.max(other_unmasked))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Statistic of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_unmasked_img = view_and_save_nifti(other_unmasked, out_dir=(os.path.join(out_dir, f'palm_statistic_topology')))\n",
    "other_unmasked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Base Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_img = view_and_save_nifti(other_unmasked, out_dir=(os.path.join(out_dir, f'unmasked_statistic_topo')))\n",
    "base_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image\n",
    "r_path = ''\n",
    "r2_img = image.load_img(r_path)\n",
    "r2_img_data = r2_img.get_fdata().flatten()\n",
    "r2_masked = np.where(p_unmasked < 0.05, r2_img_data, 0)\n",
    "\n",
    "masked_r2_img = view_and_save_nifti(r2_masked, out_dir=(os.path.join(out_dir, f'masked_r2')))\n",
    "masked_r2_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Adjusted R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image\n",
    "adjr_path = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/voxelwise_glm/stim_by_age/adj_r_squared_generated_nifti.nii'\n",
    "adjr2_img = image.load_img(adjr_path)\n",
    "adjr2_img_data = adjr2_img.get_fdata().flatten()\n",
    "adjr2_masked = np.where(p_unmasked < 0.05, adjr2_img_data, 0)\n",
    "\n",
    "masked_adjr2_img = view_and_save_nifti(adjr2_masked, out_dir=(os.path.join(out_dir, f'masked_adjr2')))\n",
    "masked_adjr2_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
