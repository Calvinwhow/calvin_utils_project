{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import sys\n",
    "import os\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from calvin_utils.file_utils.dataframe_utilities import preprocess_colnames_for_regression\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_path = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/study_metadata/derivative_metadata/qualitative_atrophy/ad_dataset_n50_atrophy_qualitative.csv'\n",
    "out_dir = r'/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_PD_DBS_STN_WURZBURG/submissions_to_server'\n",
    "save = True\n",
    "if os.path.exists(out_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Data to Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------user input above----------------------------------------------------------------\n",
    "data_df = pd.read_csv(clin_path)\n",
    "    \n",
    "# #Remove subjects\n",
    "# outlier_index=[11, 47, 48, 49]\n",
    "# data_df = data_df.drop(index=outlier_index)\n",
    "# data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_column = 'subject_id'\n",
    "outcome_column = '% Change from baseline (ADAS-Cog11)'\n",
    "#----------------------------------------------------------------DO NOT MODIFY--------------------------------------------------------\n",
    "outcomes_df = pd.DataFrame()\n",
    "outcomes_df['outcome'] = data_df.loc[:, [outcome_column]]\n",
    "\n",
    "#Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "outcomes_df.iloc[:,:] = scaler.fit_transform(outcomes_df.iloc[:,:])\n",
    "\n",
    "\n",
    "outcomes_df['subject_id'] = data_df.loc[:, [subject_column]]\n",
    "# outcomes_df['subject_id'] = [id.split('_')[0] for id in data_df[subject_column].to_list()] #<------------------------problem code. must generalize to extract subject id better\n",
    "outcomes_df.set_index('subject_id', inplace=True)\n",
    "\n",
    "# data_df = data_df.set_index('Patient # CDR, ADAS')\n",
    "# data_df['subject_id'] = data_df.index\n",
    "# data_df['outcome'] = data_df\n",
    "\n",
    "# Convert the 'subject_id' column to strings for each DataFrame\n",
    "outcomes_df.index = outcomes_df.index.astype(str)\n",
    "\n",
    "outcomes_df = preprocess_colnames_for_regression(outcomes_df)\n",
    "outcomes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clinical Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_column = 'subject_id'\n",
    "clinical_information_column = 'Age'\n",
    "#----------------------------------------------------------------DO NOT MODIFY--------------------------------------------------------\n",
    "clinical_df_1 = pd.DataFrame()\n",
    "clinical_df_1['clinical_data'] = data_df.loc[:, [clinical_information_column]]\n",
    "\n",
    "clinical_df_1['subject_id'] = data_df.loc[:, [subject_column]]\n",
    "# clinical_df_1['subject_id'] = [id.split('_')[0] for id in data_df[subject_column].to_list()] \n",
    "clinical_df_1.set_index('subject_id', inplace=True)\n",
    "\n",
    "# Convert the 'subject_id' column to strings for each DataFrame\n",
    "clinical_df_1.index = clinical_df_1.index.astype(str)\n",
    "clinical_df_1 = preprocess_colnames_for_regression(clinical_df_1)\n",
    "clinical_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxelwise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "from calvin_utils.nifti_utils.generate_nifti import nifti_from_matrix\n",
    "from nimlab import datasets as nimds\n",
    "import numpy as np\n",
    "from nilearn import image, plotting, maskers\n",
    "\n",
    "#get conectivity values of interest\n",
    "matrix_path = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/connectivity_data/vta_published_t_connectivity'\n",
    "matrix = import_matrices_from_folder(matrix_path, file_pattern='/*_T*.nii*')\n",
    "\n",
    "# prepped_matrix = pd.DataFrame()\n",
    "# for column in matrix.columns:\n",
    "#     print(column)\n",
    "#     new_name = int(column.split('_')[0]) #.split('T')[1]) #<----------------------------------THIS IS A BUG-CREATOR. MUST BE TAILORED TO PATIENT DATA\n",
    "\n",
    "# #Set patients to those in the clinical data dataframe\n",
    "matrix = matrix.transpose()\n",
    "# matrix['subject_id'] = [str(col).split('_')[0].split('MDST')[1] for col in matrix.index]\n",
    "matrix['subject_id'] = [str(col).split('_')[0] for col in matrix.index]\n",
    "matrix.set_index('subject_id', inplace=True)\n",
    "matrix.index = matrix.index.astype(str)\n",
    "neuroimaging_df_1 = preprocess_colnames_for_regression(matrix)\n",
    "neuroimaging_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxelwise Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Another Dataframe if desired\n",
    "from calvin_utils.file_utils.import_matrices import import_matrices_from_folder\n",
    "from calvin_utils.nifti_utils.generate_nifti import nifti_from_matrix\n",
    "from nimlab import datasets as nimds\n",
    "import numpy as np\n",
    "from nilearn import image, plotting, maskers\n",
    "\n",
    "#get conectivity values of interest\n",
    "matrix_path = '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/cat12/cat12_ultrafine-reg/CAT12.8.2_2170'\n",
    "neuroimaging_df_2 = import_matrices_from_folder(matrix_path, file_pattern='/*/*/*/*mwp1*resamp*.nii*')\n",
    "performed_z_score = False \n",
    "#----------------------------------------------------------------DO NOT MODIFY!----------------------------------------------------------------\n",
    "\n",
    "#Set patients to those in the clinical data dataframe\n",
    "# prepped_matrix = prepped_matrix.loc[:, data_df.index]\n",
    "\n",
    "neuroimaging_df_2 = neuroimaging_df_2.transpose()\n",
    "neuroimaging_df_2['subject_id'] = [col.split('_')[0] for col in neuroimaging_df_2.index]\n",
    "neuroimaging_df_2.set_index('subject_id', inplace=True)\n",
    "\n",
    "neuroimaging_df_2.index = neuroimaging_df_2.index.astype(str)\n",
    "\n",
    "\n",
    "#Display results\n",
    "neuroimaging_df_2 = preprocess_colnames_for_regression(neuroimaging_df_2)\n",
    "display(neuroimaging_df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask Dataframes\n",
    "mask_path = None # '/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/voxelwise_glm/stim_by_age/palm_results/palm_statistic_topology/generated_nifti.nii' #None #\n",
    "masking_df = None # # \n",
    "#----------------------------------------------------------------DO NOT MODIFY\n",
    "from calvin_utils.nifti_utils.matrix_utilities import mask_matrix\n",
    "# def mask_matrix(df_1, mask_path=None, mask_threshold=0.2, mask_by='rows', dataframe_to_mask_by=None):\n",
    "\n",
    "neuroimaging_df_1 = mask_matrix(neuroimaging_df_1, mask_path=mask_path, mask_threshold=0, mask_by='columns', dataframe_to_mask_by=masking_df)\n",
    "# neuroimaging_df_2 = mask_matrix(neuroimaging_df_2, mask_path=mask_path, mask_threshold=0, mask_by='columns', dataframe_to_mask_by=masking_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - Ensure Aligned Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose df1, df2, df3 are your dataframes\n",
    "common_index = outcomes_df.index.intersection(clinical_df_1.index).intersection(neuroimaging_df_1.index)\n",
    "\n",
    "outcomes_df = outcomes_df.loc[common_index]\n",
    "clinical_df_1 = clinical_df_1.loc[common_index]\n",
    "neuroimaging_df_1 = neuroimaging_df_1.loc[common_index]\n",
    "outcomes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframes for Access by Script\n",
    "    \"\"\"\n",
    "    Saves DataFrames to CSV files and returns the paths.\n",
    "\n",
    "    Parameters:\n",
    "    - outcome_dfs (list): A list of outcome DataFrames.\n",
    "    - covariate_dfs (list): A list of covariate DataFrames.\n",
    "    - voxelwise_dfs (list): A list of voxelwise DataFrames.\n",
    "    - path_to_dataframes (str): The directory where the DataFrames will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing lists of paths for each DataFrame type.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.dataframe_utilities import save_dataframes_to_csv\n",
    "\n",
    "where_to_save = f\"{out_dir}/server_prep\"\n",
    "#----------------------------------------------------------------\n",
    "df_paths_dict = save_dataframes_to_csv(outcome_dfs = [outcomes_df], \n",
    "                                       covariate_dfs = [clinical_df_1],\n",
    "                                       voxelwise_dfs = [neuroimaging_df_1], \n",
    "                                       path_to_dataframes = where_to_save)\n",
    "print(df_paths_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Statistical Methods You Can Use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.file_utils.script_printer import ScriptInfo\n",
    "from calvin_utils.permutation_analysis_utils.scripts_for_submission.script_descriptions import script_dict\n",
    "#----------------------------------------------------------------\n",
    "info = ScriptInfo(script_dict)\n",
    "info.print_high_level_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose Method**\n",
    "\n",
    "Copy the method you want to run fromt the output above into the cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_to_use = 'F-Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transfer Files To Server**\n",
    "\n",
    "This is going to upload the data and the script you will use.\n",
    "___\n",
    "\n",
    "    ScpTransfer class provides a convenient way to transfer files to a remote server\n",
    "    using SCP (Secure Copy).\n",
    "    Will request your password for transfer if no ssh_key provided.\n",
    "\n",
    "    Example:\n",
    "        remote_base_path = \"/path/on/remote/server\"\n",
    "        remote_hostname = \"example.com\"\n",
    "        remote_username = \"your_username\"\n",
    "        ssh_key = \"path/to/ssh_key/\"\n",
    "\n",
    "        scp_transfer = ScpTransfer(remote_hostname, remote_username, remote_password)\n",
    "        scp_transfer.transfer_files_in_dict(dict_files_to_transfer, remote_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from calvin_utils.server_utils.file_transfer_helper import ScpTransfer\n",
    "remote_path_to_save_to = '/PHShome/cu135/permutation_tests/f_test/age_by_stim_ad_dbs_final/inputs'\n",
    "server = \"erisone.partners.org\"\n",
    "username=\"cu135\"\n",
    "#----------------------------------------------------------------\n",
    "df_paths_dict['python_script'] = [importlib.import_module(info.get_module_by_method(method_to_use)).__file__]\n",
    "scp_transfer = ScpTransfer(hostname=server,\n",
    "                           username=username, \n",
    "                           ssh_key=None\n",
    "                        )\n",
    "remote_df_paths_dict = scp_transfer.transfer_files_in_dict(dict_files=df_paths_dict, \n",
    "                                    base_remote_path=remote_path_to_save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print Input Arguments and Dictionary for Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "inputs = info.get_inputs_by_method(method_to_use)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enter Arguments for a Script**\n",
    "\n",
    "Copy the dictionary printed above into the cell below and fill it out. \n",
    "\n",
    "For example\n",
    " \n",
    " script_inputs_dict =\n",
    " \n",
    "  {\n",
    "    \n",
    " 'n_cores': 16,\n",
    "\n",
    " 'out_dir': \"/PHShome/cu135/permutation_tests/f_test/age_by_stim_ad_dbs_redone/results/tmp\",\n",
    "\n",
    " 'job_name': 'ftest_bm',\n",
    "\n",
    " 'outcome_data_path': remote_df_paths_dict['outcomes'],\n",
    "\n",
    " 'clinical_covariate_paths': remote_df_paths_dict['covariates'],\n",
    "\n",
    " 'voxelwise_data_paths': remote_df_paths_dict['voxelwise']\n",
    " \n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_inputs_dict = {\n",
    " 'n_cores': 5,\n",
    " 'out_dir': f\"{remote_path_to_save_to}/results/raw_results\",\n",
    " 'job_name': 'f_test_ad',\n",
    " 'memory_per_job': 8,\n",
    " 'outcome_data_path': remote_df_paths_dict['outcomes'],\n",
    " 'clinical_covariate_paths': remote_df_paths_dict['covariates'],\n",
    " 'neuroimaging_df_paths': remote_df_paths_dict['voxelwise']\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submit Jobs**\n",
    "\n",
    "    Initialize server\n",
    "    server = LSFServer(\"erisone.partners.org\", \"cu135\")\n",
    "\n",
    "    Initialize job\n",
    "    job = LSFJob(\n",
    "    job_name=\"f_test_bm\",\n",
    "    user_email=\"choward12@bwh.harvard.edu\",\n",
    "    cpus=1,\n",
    "    output_dir=\"/PHShome/cu135/terminal_outputs\",\n",
    "    error_dir=\"/PHShome/cu135/error_outputs\",\n",
    "    queue=\"big-multi\",\n",
    "    script_path=\"/PHShome/cu135/python_scripts/launch_f_test_palm.py\",\n",
    "    options=\"-o /PHShome/cu135/permutation_tests/f_test/age_by_stim_pd_dbs/results\",\n",
    "    Gb_requested=4,\n",
    "    wait_time=0\n",
    "    )\n",
    "\n",
    "    Initialize job submitter\n",
    "    job_submitter = JobSubmitter(server, job)\n",
    "\n",
    "    Submit jobs\n",
    "    job_submitter.submit_jobs(10)\n",
    "\n",
    "This example will log into the server 'erisone.partners.org' as user 'cu135'.\n",
    " \n",
    "Will submit 10 jobs named 'f_test_bm' to the queue 'big-multi'. \n",
    "\n",
    "Each job will run the Python script at '/PHShome/cu135/python_scripts/launch_f_test_palm.py' with the options specified.\n",
    "\n",
    "Putputting to '/PHShome/cu135/terminal_outputs' and '/PHShome/cu135/error_outputs' for terminal and error logs respectively. \n",
    "\n",
    "The job requests 1 CPU and 4GB of memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a guide to job submission:\n",
    "\n",
    "https://rc.partners.org/kb/article/1462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.server_utils.job_submission_helper import LSFServer, LSFJob, JobSubmitter\n",
    "# Create an instance of the LSF job.\n",
    "# Assuming a resource requirement of 500GB memory and 6 cores for each job.\n",
    "# And a job script with some options.\n",
    "server_name = \"erisone.partners.org\"\n",
    "user_email = \"choward12@bwh.harvard.edu\"\n",
    "user_name = \"cu135\"\n",
    "num_permutations = 10000\n",
    "queue_name = \"big-multi\"\n",
    "server_env_activation_string = \"conda activate nimlab\"\n",
    "#----------------------------------------------------------------\n",
    "lsf_job = LSFJob(job_name=script_inputs_dict['job_name'],\n",
    "                 user_email=user_email,\n",
    "                 output_dir=\"~/terminal_outputs\",\n",
    "                 error_dir=\"~/terminal_outputs\",\n",
    "                 queue=queue_name,\n",
    "                 n_jobs=int(np.round(num_permutations/script_inputs_dict['n_cores'])),\n",
    "                 cpus=script_inputs_dict['n_cores'],\n",
    "                 gb_requested=script_inputs_dict['memory_per_job'],\n",
    "                 wait_time=None,\n",
    "                 script_path=remote_df_paths_dict['python_script'][0],\n",
    "                 environment_activation_string=server_env_activation_string,\n",
    "                 options=script_inputs_dict\n",
    "                 )\n",
    "\n",
    "lsf_server = LSFServer(server_name=server_name, \n",
    "                       username=user_name)\n",
    "\n",
    "job_submitter = JobSubmitter(lsf_server, lsf_job)\n",
    "job_command = job_submitter.submit_jobs(print_job=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recompose Data\n",
    "This code uses a file-staging approach to large-scale computation. The resultant files have been saved to your output directory. You must now recompose them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csvs(directory, output_filename):\n",
    "    \"\"\"\n",
    "    Combine all CSV files in a directory into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The path to the directory containing the CSV files.\n",
    "    - output_filename (str): The path to the output CSV file.\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Get a list of all CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "\n",
    "    # Loop through the CSV files and append each one to the combined DataFrame\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(directory, csv_file))\n",
    "        combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "    # Save the combined DataFrame as a new CSV file\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "    return output_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_csv_path = combine_csvs(\"/path/to/your/directory\", \"combined.csv\")\n",
    "recomposed_csv_df = pd.read_csv(recomposed_csv_path)\n",
    "recomposed_csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import confidence_intervals\n",
    "# run confidence intervals on the csv\n",
    "\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "def calculate_confidence_intervals(ab_paths, mediators):\n",
    "    \"\"\"\n",
    "    Calculates the confidence intervals and p-value based on the bootstrapped samples.\n",
    "\n",
    "    Parameters:\n",
    "    - ab_paths: list of lists containing the bootstrapped ab paths for each mediator.\n",
    "    - total_indirect_effects: list of bootstrapped summed ab paths.\n",
    "    - mediators: list of mediator names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the mean indirect effect, confidence intervals, and p-values for each mediator and the total indirect effect.\n",
    "    \"\"\"\n",
    "    ab_path_values = np.array(ab_paths)\n",
    "\n",
    "    # Check if there's only one mediator\n",
    "    if isinstance(mediators, str):\n",
    "        mediators = [mediators]\n",
    "\n",
    "    # Calculate mean indirect effect and confidence intervals for each mediator\n",
    "    mean_ab_paths = np.mean(ab_path_values, axis=0)\n",
    "    lower_bounds = np.percentile(ab_path_values, 2.5, axis=0)\n",
    "    upper_bounds = np.percentile(ab_path_values, 97.5, axis=0)\n",
    "\n",
    "    # Calculate p-values for each mediator\n",
    "    ab_path_p_values = [np.mean(np.sign(mean_ab_paths) * ab_path_values <= 0)]\n",
    "\n",
    "    # Create DataFrame to store the results\n",
    "    result_df = pd.DataFrame({\n",
    "        'Point Estimate': mean_ab_paths,\n",
    "        '2.5th Percentile': lower_bounds,\n",
    "        '97.5th Percentile': upper_bounds,\n",
    "        'P-value': ab_path_p_values\n",
    "    }, index=mediators)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = list(tqdm(pool.imap(calculate_confidence_intervals, [row for _, row in recomposed_csv_df.iterrows()]), total=recomposed_csv_df.shape[0]))\n",
    "pool.close() \n",
    "\n",
    "# Create the results DataFrame\n",
    "results_df = pd.concat(results, axis=1).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate FWE P-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_utils.permutation_analysis_utils.statistical_utils.p_value_statistics import PermutationPValueCalculator\n",
    "\n",
    "first_stage_dir = 'permutation_tests/f_test/age_by_stim_pd_dbs_redone/inputs/results/raw_results'\n",
    "job_name = 'f_test_pd'\n",
    "observed_nifti_path = '/PHShome/cu135/permutation_tests/f_test/age_by_stim_pd_dbs_redone/inputs/observed/f_statistic_generated_nifti.nii'\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "calculator = PermutationPValueCalculator(None, None)\n",
    "fwe_p_values = calculator.fwe_calculate(directory=first_stage_dir, basename=job_name, nifti_path=None, use_nifti=True, multiprocess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Nifti Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv('/Users/cu135/Dropbox (Partners HealthCare)/resources/datasets/BIDS_AD_DBS_FORNIX/response_topology/mediated_moderation/age_mediated_by_grey_matter/P-value_mediated_moderation_analysis_results.csv')\n",
    "\n",
    "mask_threshold=0\n",
    "#----------------------------------------------------------------DO NOT TOUCH\n",
    "#Prepare Indices to Unmask\n",
    "from calvin_utils.nifti_utils.matrix_utilities import unmask_matrix\n",
    "from nimlab import datasets as nimds\n",
    "from calvin_utils.nifti_utils.generate_nifti import view_and_save_nifti\n",
    "from nilearn import image\n",
    "\n",
    "#Perform Unmasking\n",
    "if mask_path is not None:\n",
    "    mni_mask = image.load_img(mask_path).get_fdata().flatten()\n",
    "    brain_indices = np.where(mni_mask > mask_threshold)[0]\n",
    "elif masking_df is not None:\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\").get_fdata().flatten()\n",
    "    mask = masking_df.transpose().reset_index(drop=True).copy()\n",
    "    mask['mask_index'] = mask.sum(axis=1)\n",
    "    brain_indices = np.where(mask['mask_index'] != 0)[0]\n",
    "else:\n",
    "    mni_mask = nimds.get_img(\"mni_icbm152\").get_fdata().flatten()\n",
    "    brain_indices = np.where(mni_mask > 0)[0]\n",
    "\n",
    "# Create a boolean mask for brain_indices\n",
    "bool_mask = np.zeros_like(mni_mask, dtype=bool)\n",
    "bool_mask[brain_indices] = True\n",
    "\n",
    "for statistic in results_df.columns:\n",
    "    try:\n",
    "        print(statistic)\n",
    "        \n",
    "        # Initialize the output mask with NaN values\n",
    "        output_mask = np.full_like(mni_mask, np.nan)\n",
    "        \n",
    "        # Reinstate the values at brain_indices\n",
    "        output_mask[bool_mask] = results_df[statistic]\n",
    "        \n",
    "        # View and save\n",
    "        view_and_save_nifti(output_mask, out_dir=out_dir, output_name=statistic)\n",
    "    except:\n",
    "        print(f\"Couldn't convert {statistic} to nifti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimlab_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
